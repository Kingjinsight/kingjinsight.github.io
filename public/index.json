[{"content":"Bitcoin The legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\nThe Blockchain: A Digital Chain of Trust ‚õìÔ∏è The blockchain is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\nIt\u0026rsquo;s a Chain of Blocks: Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block. It\u0026rsquo;s Immutable: Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending. The Genesis Block: Where It All Began üìú The very first block, known as the Genesis Block, was mined on 2009.01.04, by Bitcoin\u0026rsquo;s mysterious creator, Satoshi Nakamoto. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\nMining: Securing the Network and Creating Coins ‚õèÔ∏è Mining is the process of creating new blocks. It\u0026rsquo;s a competitive race that serves two critical purposes:\nValidating Transactions: Miners group pending transactions into a new block. Creating New Bitcoin: The winner of the race is rewarded with new bitcoin. Here‚Äôs how a miner wins the race and proves their block is valid:\nThe Hashing Puzzle: Miners take the data in their block and use their computers to find a specific number called a nonce. When the block data and the nonce are combined and put through a cryptographic function (SHA-256), they produce a unique digital fingerprint called a hash. The \u0026ldquo;Lower Than\u0026rdquo; Rule: To win, a miner must find a hash that is lower than the current network \u0026ldquo;target\u0026rdquo;. This target is a very large number that the entire network agrees on. Finding a hash below this target is incredibly difficult and requires immense computational power‚Äîit\u0026rsquo;s like trying to win a global lottery every 10 minutes. The Reward and The Halving: The first miner to find a valid hash wins the block reward. Initially, the reward was 50 BTC. However, this reward is programmed to cut in half roughly every four years (or 210,000 blocks) in an event called the halving. As of the April 2024 halving, the reward is now 3.125 BTC. This mechanism controls the supply of new bitcoin, making the currency scarce and ensuring its total amount will never exceed 21 million coins. When a winning block is found, its hash is broadcast across the P2P network. All other participants quickly verify that the hash is valid. Once confirmed, they add the new block to their copy of the blockchain and immediately start competing to solve the next block.\nHow to Mine: Choosing Your Method There are a few ways to participate in Bitcoin mining, each with its own pros and cons.\nSolo Mining: This is you, on your own, trying to solve a block. If you succeed, you get the entire block reward (3.125 BTC + transaction fees). However, the odds of a single person solving a block today are astronomically low due to the immense competition. It\u0026rsquo;s like buying a single lottery ticket and hoping to win the grand prize. Mining Pool: This is the most common method. You join a \u0026ldquo;pool\u0026rdquo; with thousands of other miners, combining your computing power. The pool works together to find blocks much more frequently. When the pool wins, the reward is split among all participants based on how much computing power they contributed. This provides smaller, but much more consistent and predictable, payouts. Block Forks: When the Chain Splits üç¥ A fork happens when the blockchain temporarily or permanently splits into two different paths.\nAccidental Fork: Sometimes, two miners find a valid block at almost the exact same time. The network briefly splits as some nodes follow one miner and some follow the other. This is usually resolved within a few minutes when the next block is found and added to one of the chains, making it the \u0026ldquo;longest\u0026rdquo; and therefore the official one. The shorter chain is then abandoned. Hard Fork: This is an intentional split that happens when the network\u0026rsquo;s software rules are changed in a way that is not backward-compatible. All participants must upgrade to the new rules to continue. If a significant portion of the community refuses to upgrade, the split becomes permanent, resulting in the creation of a new, separate cryptocurrency (e.g., Bitcoin Cash was created from a hard fork of Bitcoin). Ethics Mining bitcoin always consume immense energy, which critics view as a wasteful environmental cost for a seemingly meaningless computation. Proponents argue this mechanism decentralized financial system that offers freedom from the control of banks and governments.\nTools Calculate computing performance: https://www.nicehash.com/profitability-calculator\nThis is the computating power of my personal game laptop(one dollar per day hhh) Resources: Youtube channels\nhttps://www.youtube.com/watch?v=5hgdekVZb3A\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=3 https://www.youtube.com/watch?v=a41DMDfJjsU\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=2 https://Gemini.google.com Trivias Sherrington, coined the word \u0026ldquo;synapse\u0026rdquo; to define the connection between two neurons Two different roads of AI: connectionism and Symbolism. Hidden layer was firstly been implemented in Boltzmann machine, although Rosenblatt had some idea about multilayer perceptrons, but he didn\u0026rsquo;t find any useful training algorithm. Restricted Boltzmann machine - each layer is only allowed to be fully connectted to the next layer, current layer nodes are not connectted to each other. The advantages of this machine is it allows to update bodes within the same layer in parallel The invention of hidden layer allows model to understand abstract features. It also becomes to one of the most significant component in deep learning. The main difference between the Hopfield Network and the Boltzmann nachine is the presence of hidden layers. Other differences include the fact that the Hopfield network is deterministic, whereas the Boltzmann machine is stochastic, and the defintions of their energy function also differ. The fovea has many photoreceptors, with a high density of cones(for colors) and nearly no rods(for dark). This structure allows us to see the world clearly. If you develop myopia, the image formed after light is reflected by the eye may not be focused directly on the fovea. The idea of CNN was mainly inspired by the HMAX model(hierarchical, pooling, convolution), and the HMAX model was proposed by Tomaso Poggio, to simulate primate visual system, specifically ventral stream. Resource Documentary of AlphaGo: https://www.youtube.com/watch?v=WXuK6gekU1Y This is the most comprehensive guide for AI beginner I had ever seen: Guide link ","permalink":"https://example.org/posts/techweekly/techweek4/","summary":"\u003ch2 id=\"bitcoin\"\u003eBitcoin\u003c/h2\u003e\n\u003cp\u003eThe legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-blockchain-a-digital-chain-of-trust-\"\u003eThe Blockchain: A Digital Chain of Trust ‚õìÔ∏è\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eblockchain\u003c/strong\u003e is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s a Chain of Blocks:\u003c/strong\u003e Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s Immutable:\u003c/strong\u003e Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-genesis-block-where-it-all-began-\"\u003eThe Genesis Block: Where It All Began üìú\u003c/h4\u003e\n\u003cp\u003eThe very first block, known as the \u003cstrong\u003eGenesis Block\u003c/strong\u003e, was mined on \u003cstrong\u003e2009.01.04\u003c/strong\u003e, by Bitcoin\u0026rsquo;s mysterious creator, \u003cstrong\u003eSatoshi Nakamoto\u003c/strong\u003e. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\u003c/p\u003e","title":"A Deep Dive into the Bitcoin Technology | King Weekly"},{"content":"2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning. However, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\nRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually. And before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation. So today I will record this moment.\nHopfield Network was invented at 1982. Processor John Hopfield designed it based on ideas from statistical mechanics.\nThe graph above shows two states of a ball From the left part, we can see the energy system of the ball is at the highest, which means the ball is very unstable From the right part, we can see the ball had already fall into the bottom, which means the ball is very stable. Although it is a classical physics model, but it definitly explain the main idea of hopfield network.\nSo now we can say hopfield network is just make a system move from an unstable state to stable state.\nIf you interested how exactly hopfield network work. See the video below, it\u0026rsquo;s pretty nice.\nThis idea is also useful in today. In LLM, we can say the user\u0026rsquo;s prompt and question is the most unstable states while the answer is the stable state.\nBased on the concept of Hopfield networks, many different architectures had been invented, which makes Connectionism and Deep Learning great again.\nAfter all, I catch the reason why nobel prize gives to physics.\nhow hopfield work why we have hopfield how it work in ai how the concept can be used in llm why nobel physics prize gives to ai in 2024 ","permalink":"https://example.org/posts/hopfieldnetwork/","summary":"\u003cp\u003e2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning.\nHowever, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\u003c/p\u003e\n\u003cp\u003eRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually.\nAnd before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation.\nSo today I will record this moment.\u003c/p\u003e","title":"Hopfield Network"},{"content":"Book - Unix: A History and a Memoir Recently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world. During the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\nAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals. The system developed before Unix was called Multics. Since ‚ÄúMultics‚Äù already used ‚Äúmulti,‚Äù the early name of Unix was ‚ÄúUnics.‚Äù Unix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasn‚Äôt written for the PDP-10. Tools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs. Unix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems. GNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source. MacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX. In the early days, operating systems were not portable. This changed with the invention of the C language and its compiler. MINIX was widely used because it was embedded in Intel chips. The working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation. Microsoft once had its 3own Unix-like system. Another completely different path from Unix was MS-DOS, which eventually evolved into today‚Äôs Windows. You can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan. ‚ÄúEverything is a file‚Äù is one of the core principles of Unix. The KISS principle (‚ÄúKeep It Simple, Stupid‚Äù) is a fundamental part of Unix philosophy. The UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\nKeep it simple stupid Do one thing, and do it well Everything is a file Make each program a filter Fail loudly Modularity Prototyping early In today, many barriers had already been removed.\nAnd I realise.\nThe revolution of AI is just like the reenactment of Unix\u0026rsquo;s development.\nSo.\nKISS.\nTrivias tty - TeleTYpewriter, terminal in the old time, before lcd screen been invented. UNIX was developed on the PDP-7, a computer with no screen, no mouse and only 8KB of RAM. It weighted nearly 500kg. UNIX and UNIX-like system use abbreviated commands because typing on TTY terminals in the 1960s was slow and insufficient. Second-system effect: It believes that after completing a small, elegant, and successful system, people tend to have overly high expectations for the next projects, which may lead to the creation of a huge, feature-rich but monstrous system.The \u0026ldquo;second-system effect\u0026rdquo; can result in software project plans being overdesigned, with too many variables and excessive complexity, ultimately falling short of expectations and leading to failure. such as PL/I in Multics Fortran(formular translation): The purpose of this lagnauge is to proceed mathematics formular and float number in an efficient way, like integration, linear algebra. That\u0026rsquo;s why fortran is still popular in some supercomputer and scientific calculation. B lagnauge is designed in a bit-unit computer PCP-7, where C langauge is designed in a byte-unit computer. Therefore the main difference between B and C is B langauge doesn\u0026rsquo;t support types and C does. Development of Clang: PL/I -\u0026gt; BCPL -\u0026gt; B -\u0026gt; New B(C) If computers using the same cpu architecture, they will using the same assembly language. The grep command is used to find lines that match a specific pattern in a file while the sed command is used to insert, replace and delete text from a file. Finally, the awk command supports programming logic and is often used for advanced data processing tasks. The development of UNIX from 1970 until now. Talks \u0026ldquo;A new technological discovery is often discredited by older generations of professionals - especially those with high authority and prestige in the existing field - in order to protect their own status\u0026rdquo; ","permalink":"https://example.org/posts/techweekly/techweek3/","summary":"\u003ch2 id=\"book---unix-a-history-and-a-memoir\"\u003eBook - Unix: A History and a Memoir\u003c/h2\u003e\n\u003cp\u003eRecently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world.\nDuring the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals.\u003c/li\u003e\n\u003cli\u003eThe system developed before Unix was called Multics.\u003c/li\u003e\n\u003cli\u003eSince ‚ÄúMultics‚Äù already used ‚Äúmulti,‚Äù the early name of Unix was ‚ÄúUnics.‚Äù\u003c/li\u003e\n\u003cli\u003eUnix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasn‚Äôt written for the PDP-10.\u003c/li\u003e\n\u003cli\u003eTools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs.\u003c/li\u003e\n\u003cli\u003eUnix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems.\u003c/li\u003e\n\u003cli\u003eGNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source.\u003c/li\u003e\n\u003cli\u003eMacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX.\u003c/li\u003e\n\u003cli\u003eIn the early days, operating systems were not portable. This changed with the invention of the C language and its compiler.\u003c/li\u003e\n\u003cli\u003eMINIX was widely used because it was embedded in Intel chips.\u003c/li\u003e\n\u003cli\u003eThe working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation.\u003c/li\u003e\n\u003cli\u003eMicrosoft once had its 3own Unix-like system.\u003c/li\u003e\n\u003cli\u003eAnother completely different path from Unix was MS-DOS, which eventually evolved into today‚Äôs Windows.\u003c/li\u003e\n\u003cli\u003eYou can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan.\u003c/li\u003e\n\u003cli\u003e‚ÄúEverything is a file‚Äù is one of the core principles of Unix.\u003c/li\u003e\n\u003cli\u003eThe KISS principle (‚ÄúKeep It Simple, Stupid‚Äù) is a fundamental part of Unix philosophy.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\u003c/p\u003e","title":"For UNIX Week | King Weekly"},{"content":"Model Context Protocol overview MCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\nWhy is MCP needed? In the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications √ó M tools = N√óM interfaces).\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces). The current components of MCP servers include: How does MCP work? Example: Suppose you\u0026rsquo;re using an AI assistant to manage your work, and it wants to help you schedule today\u0026rsquo;s meeting:\nWithout MCP, developers would need to write separate integration code for Outlook, Google Calendar, and Apple Calendar.\nWith MCP, the AI only needs to call the MCP server, which will automatically interface with your calendar system. No matter which calendar service you use, the AI can work seamlessly.\nCore Functions of MCP: Reducing development costs (no need to develop separate integrations for each tool).\nEnhancing AI\u0026rsquo;s ability to access external data (allowing AI to easily query and manipulate external data).\nStandardizing communication (making communication between different AI applications and tools smoother).\nYou can think of MCP as the \u0026ldquo;USB interface for AI\u0026rdquo;‚Äîany AI device can plug into different tools without needing to individually adapt to each one!\nTrivias: Graph is a great data structure, it can be used to find the shortest path, solve a magic cube. Rely on this data structure, Human find the shortest steps to solve the worst case configured 3\\*3\\*3 magic cube in 20 steps (which also called god\u0026rsquo;s number) and 11 steps for 2\\*2\\*2 one. What about n*n*n, Here is an interesing paper about time compleixity of solving a n*n*n magic cube: Algorithms for Solving Rubik‚Äôs Cubes Topological sort is an algorithm based on DFS and DAG, It\u0026rsquo;s not a traditional sorting algorithm, like comparing the size of each number and sort them, but sorting based on dependencies. Dynamic programming is just like recursion + memorization + guessing P vs NP problems: P is a problem that can be solved easily by a computer, NP is a problem that you can check for correctness very easily once solved. However, P != NP for example wec can\u0026rsquo;t engineer luck. Also, NP hard means it is at least as hard as any problem in NP, and NP-complete is lke if you can solve one NP-complete question, you can solve all NP question. Reduction is like to prove a known NP-complete question, and transfer it into a NP question X, then X is also a NP-complete question HTTP vs REST API HTTP is a protocol, and its core task is to define how to request and transfer data between your broswer and server\nIt focuses on the data exchange level, regardless of whether the data is an image, text, video, or API response. For example, you can use an HTTP request to access a webpage (HTML page) or use an HTTP request to retrieve API data (JSON data). It doesn‚Äôt care about what data you\u0026rsquo;re transmitting. REST API is a design style, based on the HTTP protocol, with rules and best practices:\nIt views content on the server as \u0026ldquo;resources\u0026rdquo; and specifies how to operate on them using HTTP methods (GET, POST, PUT, DELETE). REST API design considers resource orientation: Resources (like users, articles, comments, etc.) have unique identifiers (URIs), and clients interact with these resources through HTTP requests. For example, in a REST API, you can:\nGET request: Retrieve a resource (e.g., get movie information). POST request: Create a new resource (e.g., submit a new comment). PUT request: Update a resource (e.g., modify user information). DELETE request: Delete a resource (e.g., remove an article). Resource: A tool to build a personal streaming music player: navidrome A great blog that introduce human visualization: human visualization Five basic algorithms explanation: Dynamic programming Greedy Algorithm Backtracking Branch and Bounding This week, Nvidia GTC 2025 brings a lot of new tech, PC in AI age, B300, new architecture, robots. After watched the coverage keynotes, I was just feel like our mankind is in an special stage with unpredictable pace. In science fiction films, at this point , it often leads to the arrival of an alien civilization. A dijkstra algorithm visualiser that helps me understand it: Dijkstra shortest path MCP explain: A blog that explain MCP crystal clear ","permalink":"https://example.org/posts/techweekly/techweek2/","summary":"\u003ch2 id=\"model-context-protocol-overview\"\u003eModel Context Protocol overview\u003c/h2\u003e\n\u003cp\u003eMCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\u003c/p\u003e\n\u003ch4 id=\"why-is-mcp-needed\"\u003eWhy is MCP needed?\u003c/h4\u003e\n\u003cp\u003eIn the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications √ó M tools = N√óM interfaces).\u003cbr\u003e\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces).\n\u003cimg alt=\"components mcp include\" loading=\"lazy\" src=\"/TechStuff/mcp.png\"\u003e\u003c/p\u003e","title":"Jarvis Will Coming Soon | King Weekly"},{"content":"Email sending and receiving system The main system is build based on three protocols: SMTP, POP3 and IMAP. SMTP is used for sending emails to the recipient‚Äôs email server, but it does not handle receiving emails. User1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\nPOP3 downloads emails from the email server to the email client. By default, it removes emails from the server after downloading, but some email clients allow users to keep copies on the server. IMAP keeps emails on the server and synchronizes them across multiple devices. The email client initially loads only the headers, and the full email content is fetched from the server when the user opens it.\nIf you want to customize an email domain. You need to have your own SMTP and IMAP/POP3 server and a domain, the other steps are the same as above.\nPKGBUILD in Arch Linux PKGBUILD is a bash script contain the build information required by archlinux package we use makepkg script to build the package, it will search PKGBUILD first in the current folder. Benefits:\nusing pacman to manage, user can update and uninstall easily some pkgbuild file include the commands to generate a binary file and store it in /user/bin Drawbacks:\nNot friendly to starter Although we can use yay to help us do all these stuff.\nTrivias Newton\u0026rsquo;s method is quadratic convergence when we want to calculate the root of a number Catalan number is a group of sequence that appear widely in combinatorics. e.g. ways to arrange n brackets, number of triangles in an n+2 convex polygon. The common property of these applications is that they are recursive and have a constrained structure. Toom cook: It divide a d-digit number into n parts and doing arithmatic calculations. Schonhage-strassen scheme: It multiplies two integers of length ùëõ in O (ùëõ logùëõ log logùëõ) steps on a multitape Turing machine A Naive algorithm is usually the most obvious solution when one is asked a problem. It may not be a smart algorithm but will probably get the job done The taste of red wine is determined by acidity, sweetness, alcohol content, tannins, and body. Wines are categorized into New World and Old World. New World wines (from countries like the USA, Chile, Argentina, and China) are named after the grape variety, while Old World wines (mainly from Europe) are named after their place of origin.\nRed wine is made by fermenting red grapes with their skins. White wine is made from either white grapes or red grapes without their skins. Ros√© wine is made by soaking the grape skins briefly but fermenting without them. Sparkling wine undergoes a second fermentation to produce bubbles. Gabriel\u0026rsquo;s horn is a type of geometric figure that has infinite surface area but finite volume. Resources Code question(leetcode), system design question(crack the code interview), teamwork, communication are all important in the interview. An old guideline to learn ai: https://www.captainai.net/itcoke/ A guideline to learn CS: https://csdiy.wiki/ÂêéËÆ∞/ Useful tips to integrate by parts, ÂèçÂØπÂπÇÊåá‰∏â, to choose u. Customize your zsh: oh my zsh Xiaomi releases a concept modular camera, it looks pretty awesome and innovative. 3b1b\u0026rsquo;s taylor series explaination:https://www.youtube.com/watch?v=3d6DsjIBzJ4 3b1b\u0026rsquo;s explaination of why we have exponential e:https://www.youtube.com/watch?v=m2MIpDrF7Es This website is all about competitive writing of source code that is as short as possible: Codewolf Explaination of greedy algorithm: greedy algorithm Explaination of dynamic programming: dynamic programming Deploy perosonal VPN tools: tailscale Abstract If no one is reading blogs anymore, why should we write them? Let‚Äôs make it simple: you write a blog, but nobody cares, nobody reads it. At least, the number of readers is not as many as you thought. You put your personal ideas and thoughts into the article, carefully structuring each sentence, and choose a great image‚Äîthen, no response, no likes, no shares, no activity. So, what is the meaning of writing a blog? First, there are two misconceptions about blogging. One is that if I write a good article, readers will come naturally. No, they won‚Äôt come. There are billions of blogs on the internet, like a massive hurricane, and yours is just a single leaf in the wind. Who would notice? Another misconception is that if nobody reads it, writing is a waste of time. Blogs have their own hidden value. You write blogs not for the applause of others, but for your own needs. Blogs help clear your mind. They help you organize your thoughts and sharpen your perspective. When you think better, you will achieve better results. The target audience of a blog is actually not the people on the internet, but your future self. Your article will help you see the evolution of your own thoughts. Additionally, one day in the future, someone who truly needs your article will find it. A deep, thoughtful article has a longer-lasting impact than a viral article. Writing a blog is quite like street photography. You take your camera and walk through the city. You see a scene‚Äîa moment filled with light, shadow, and humanity‚Äîand then you capture it. Nobody cares about what you actually captured. But that‚Äôs not the reason you photograph; you photograph because you see something interesting. Writing a blog is the same. You write a blog because you are thinking, observing new things, and hope to store them somewhere. If someone reads it, that\u0026rsquo;s great. If not, you‚Äôve still completed your work\n","permalink":"https://example.org/posts/techweekly/techweek1/","summary":"\u003ch2 id=\"email-sending-and-receiving-system\"\u003eEmail sending and receiving system\u003c/h2\u003e\n\u003cp\u003eThe main system is build based on three protocols: SMTP, POP3 and IMAP.\n\u003cimg alt=\"process of email system\" loading=\"lazy\" src=\"/emailsys.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSMTP is used for sending emails to the recipient‚Äôs email server, but it does not handle receiving emails.\nUser1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\u003c/p\u003e","title":"Start | King Weekly"},{"content":"In this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O O(n) notation before, but for Big theta Œ∏(n) and reccurence relations T(n), I never heard them before. Today, I hope I can finally figure them out.\nWhat T(n) represents the actual running time of an algorithm\nO(n) represents the asymptotic upper bound of the running time of an algorithm\nŒ∏(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\nHow to convert the three of them Normally we can directly transfer T(n) to O(n) or Œ∏(n). Int sum = 0 for (i = 1; i \u0026lt;= n, i ++) { sum = sum + i } This is a classical example, First of all, we initialize variable sum requires one unit of running time. There are three statements inside the for loop, statement 1 i = 1 requires one unit of the running time, statement 2 i \u0026lt;= n requires n+1 units of the running time, statement 3 i ++ requires n units of the runningn time, and sum = sum + i requires 2n units of the running time, n for addition and n for assignment. Therefore T(n) = 1+1+(n+1)+n+2n = 4n + 3. In O(n), we ignore the constant and the lower-order terms, therefore the time complexity is O(n) / Œ∏(n).\nWhen the algorithm is a recursion, such as karatsuba multiplication and high precision multiplication. There are two methods to convert T(n) into O(n)\nRecursion tree method According to Recursion tree method, we derive master theorem The time complexity of multiplication is equal to the time complexity of division\n","permalink":"https://example.org/posts/the-difference-between-tn--on-and-%CE%B8n/","summary":"\u003cp\u003eIn this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O \u003ccode\u003eO(n)\u003c/code\u003e notation before, but for Big theta \u003ccode\u003eŒ∏(n)\u003c/code\u003e and reccurence relations \u003ccode\u003eT(n)\u003c/code\u003e, I never heard them before. Today, I hope I can finally figure them out.\u003c/p\u003e\n\u003ch1 id=\"what\"\u003eWhat\u003c/h1\u003e\n\u003cp\u003eT(n) represents the actual running time of an algorithm\u003cbr\u003e\nO(n) represents the asymptotic upper bound of the running time of an algorithm\u003cbr\u003e\nŒ∏(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\u003c/p\u003e","title":"Time Complexity Notations"},{"content":"This week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\nOverview The machine model is HP-Elitedesk-800-G4-SFF. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is ¬£150.\nIf you want to learn more here is the machine datasheet:server_datasheet\nHardware Motherboard: Q370 viewer CPU: i5-8500 GPU: intel UHD 630 RAM: 16G SSD: 256G HDD: 500G * 1 The motherboard provides a high flexibility to expand more internal storage, also it has 4 PCie expansion slots which can used to expand more storage space or other components you want.\nSoftware nextcloud\nemail domain\ngitlab\nminecraft server\ndocker\njellyin\nsynthing\nproxy?router?gateway?\nvirtual machine\nI host my server with ubuntu server distro. The reason I didn\u0026rsquo;t choose proxmox is because I want to learn server step by step, proxmox is great in visualization, maybe in the future, I will try it.\nDurign the process of configuring storage, I learned LVM, which is a wonderful tool for those users that has multiple drives. User can create a storage pool called volume group. Firstly, user add their physical volumes into volumn group, and we create logical volumn based on the storage area had in volumn groups, and then we mount those LVs with the actural dirctory. It seems like Windows is not able to achieve this function. For RAID, we seperate it into 4 different categories, radi0, raid1, raid5 and raid10. This tools shows how to save files in different number of drives or in LVM.\nFor external access, I plan to use cloudflare tunnel. They provide such service, I need to buy a domain name and combine it with the cloudflare tunnel, and when I access the server, I firstly type the domain name in my browser to ask cloudflare, and they will guide me to the tunnel to my server, also in server end, I need to install cloudflared docker image as an end, then it works! It\u0026rsquo;s so convenient for those people who live in school accommodation. And it\u0026rsquo;s totally free!\nIn my network configuration. I didn\u0026rsquo;t install a router, but the best choice is to use a router for all devices in my home, and assign each of them an static IP address. To access the server, I bought a portable monitor, since the IP address of my server using DHCP, which required to check the IP address manually when the machine reboot or close. This issue will be solved easily when I have a router. Nowadays, I need to change the tunnels configuration to enable external access.\nsince I have 3 different operating systems in my three daily devices, phone for android, laptop for arachlinux and ipad for ipados, I installed nextcloud docker to try to integrate them into one ecosystem, that pretty awesome.\n","permalink":"https://example.org/posts/homeserver/","summary":"\u003cp\u003eThis week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThe machine model is \u003ccode\u003eHP-Elitedesk-800-G4-SFF\u003c/code\u003e. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is ¬£150.\u003cbr\u003e\n\u003cimg loading=\"lazy\" src=\"/TechStuff/serverbill.png\"\u003e\u003c/p\u003e","title":"My Homelab"},{"content":"This semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\nIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\nHow the JVM works The JVM executes er programs in several stages:\nCompilation: Java source code (.java) is compiled into bytecode (.class) by the Java Compiler (javac).\nClass Loading: The JVM loads compiled bytecode when required.\nBytecode Verification: Ensures security and correctness before execution.\nExecution: The JVM interprets or compiles bytecode using Just-In-Time (JIT) compilation.\nGarbage Collection: The JVM automatically manages memory, reclaiming unused objects.\nClass loader One of the organizational units of JVM byte code is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.\nLet\u0026rsquo;s explore this concept with an example: Imagine you\u0026rsquo;re watching a movie on a streaming service.\nLoading:\nThe service first finds and imports the movie data you want to watch. Similarly, the class loader locates the binary data for a Java class. Linking: Verification: Before you watch the movie, the service checks that the file isn\u0026rsquo;t corrupted. In Java, the class loader verifies the correctness of the class. Preparation: The service sets up the necessary space in memory to buffer the movie. Java allocates memory for variables and sets default values. Resolution: The service ensures all necessary subtitles or audio tracks are ready to play. Java resolves references to make them direct. Initialization:\nAs you start watching, the service begins playing the movie. Similarly, Java runs code to set up variables with their starting values. Class Loader Types:\nBootstrap Class Loader: Like the service\u0026rsquo;s core library of well-known movies, it loads fundamental, trusted classes. Extension Class Loader: Similar to special add-on features, it loads additional classes outside the core library. System/Application Class Loader: Like searching for new releases or user-uploaded content, it loads classes specific to the application you‚Äôre using. Virtual machine architecture Cross-Platform Compatibility and Limitations The JVM abstracts away the underlying hardware and operating system specifics, allowing Java bytecode to run on any device equipped with a compatible JVM. This cross-platform capability greatly simplified software distribution and development, as developers could write code once and deploy it across various platforms without modification.\nDespite its strong cross-platform capabilities, the JVM faced significant challenges due to competitive corporate strategies, particularly the \u0026ldquo;Embrace, Extend, and Extinguish\u0026rdquo; (EEE) approach adopted by some companies like Microsoft in the late 1990s. This strategy involved embracing a technology, extending it with proprietary features, and eventually using those extensions to undermine the original technology.\nMicrosoft initially embraced Java by integrating it into their Internet Explorer browser and Windows platforms. However, they extended Java with proprietary features that were specific to Windows, creating a version of Java that was incompatible with the standard JVM specifications set by Sun Microsystems. This move fragmented the Java platform and undermined the \u0026ldquo;Write Once, Run Anywhere\u0026rdquo; philosophy.\nWith the development and rise of programming languages like Swift, Kotlin, and JavaScript, the JVM faced significant challenges in maintaining its performance edge. Swift, designed by Apple for iOS and macOS platforms, offers high performance and safety due to its compiled nature and modern language features. Kotlin, although initially running on the JVM, introduced concise syntax and advanced features that surpassed Java in many ways, leading it to become the preferred language for Android development. JavaScript\u0026rsquo;s performance greatly improved with engines like V8, and its versatility expanded through technologies like Node.js for server-side development. These languages not only matched but often exceeded the JVM\u0026rsquo;s performance and adaptability in their respective domains, leading to a shift in developer preferences and a relative decline in the JVM\u0026rsquo;s dominance.\nOverall Nowadays, Java has become to a normal programming lauguage. And the question is obvious solved. Python has its own interpreter to transfer the source code into machine code. Haskell has its own compiler, C++/C are compiled directly into machine code. However,they can\u0026rsquo;t generate a compiled file that enable to run in every operating system. If there is no EEE strategy, Linux may have a stronger effects in today\u0026rsquo;s world.\n","permalink":"https://example.org/posts/jvm/","summary":"\u003cp\u003eThis semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\u003c/p\u003e","title":"Ambitious Cross-Platform Dream: JVM's Achievements and Limitations"},{"content":"","permalink":"https://example.org/aboutme/gallery/","summary":"","title":""},{"content":" Photo \u0026 Video\nAbove: A collection of memorable moments in my life - fragments of lights and time, captured with my vivo X100 Ultra. Each photo holds a memory, a feeling, a story worth remembering\n","permalink":"https://example.org/aboutme/","summary":"aboutme","title":"About me"}]