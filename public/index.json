[{"content":"I really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\nColor Theory. Additive Color: RGB for Red, Green and Blue. Red + Green + Blue = White Subtractive Color: These are created by mixing two additive colors. Red + Green = Yellow Green + Blue = Cyan Red + Blue = Magenta Yellow + Cyan + Magenta = Black Basics Histogram: a graph that displays the brightness and color distribution of an image From left to right, each column represents the number of the white-black pixels in different level It has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites. There are two triangles at the top left and top right. the top left one is shadow clipping which will show you whether your image has region that was too dark the top right one is highlight clipping which show you whether your image has region that was too light the reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0. There are several colors in the triangle gray - all details are well preserved red - red channel overexposure/underexposure green - green channel overexposure/underexposure blue - blue channel overexposure/underexposure yellow - yellow channel overexposure/underexposure cyan - cyan channel overexposure/underexposure magenta - magenta channel overexposure/underexposure solid white - RGB channel channel overexposure/underexposure in the same time White balance Temperature Tint Tone Exposure: change the overall brightness of the photo Contrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa. Highlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc. Shadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc. Whites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white. Blacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black. Presence Texture: Focus on the surface texture, such as skin pores, surface of rocks, etc. Clarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;. Dehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation. Vibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated. Saturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree. Tone Curve The square graph looks similar with histogram, but contain a line segment from bottom left to top right. We can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve. we can also pull RGB channel separately. S curve to increase contrast Tips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more. This post will update frequently.\n","permalink":"http://localhost:1313/posts/lightroom/","summary":"\u003cp\u003eI really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\u003c/p\u003e\n\u003ch2 id=\"color-theory\"\u003eColor Theory.\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAdditive Color: RGB for Red, Green and Blue.\n\u003cul\u003e\n\u003cli\u003eRed + Green + Blue = White\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSubtractive Color: These are created by mixing two additive colors.\n\u003cul\u003e\n\u003cli\u003eRed + Green = Yellow\u003c/li\u003e\n\u003cli\u003eGreen + Blue = Cyan\u003c/li\u003e\n\u003cli\u003eRed + Blue = Magenta\u003c/li\u003e\n\u003cli\u003eYellow + Cyan + Magenta = Black\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"basics\"\u003eBasics\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHistogram: a graph that displays the brightness and color distribution of an image\n\u003col\u003e\n\u003cli\u003eFrom left to right, each column represents the number of the white-black pixels in different level\u003c/li\u003e\n\u003cli\u003eIt has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites.\u003c/li\u003e\n\u003cli\u003eThere are two triangles at the top left and top right.\n\u003col\u003e\n\u003cli\u003ethe top left one is shadow clipping which will show you whether your image has region that was too dark\u003c/li\u003e\n\u003cli\u003ethe top right one is highlight clipping which show you whether your image has region that was too light\u003c/li\u003e\n\u003cli\u003ethe reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eThere are several colors in the triangle\n\u003col\u003e\n\u003cli\u003egray - all details are well preserved\u003c/li\u003e\n\u003cli\u003ered - red channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003egreen - green channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eblue - blue channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eyellow - yellow channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003ecyan - cyan channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003emagenta - magenta channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003esolid white - RGB channel channel overexposure/underexposure in the same time\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eWhite balance\n\u003col\u003e\n\u003cli\u003eTemperature\u003c/li\u003e\n\u003cli\u003eTint\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eTone\n\u003col\u003e\n\u003cli\u003eExposure: change the overall brightness of the photo\u003c/li\u003e\n\u003cli\u003eContrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa.\u003c/li\u003e\n\u003cli\u003eHighlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc.\u003c/li\u003e\n\u003cli\u003eShadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc.\u003c/li\u003e\n\u003cli\u003eWhites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white.\u003c/li\u003e\n\u003cli\u003eBlacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePresence\n\u003col\u003e\n\u003cli\u003eTexture: Focus on the surface texture, such as skin pores, surface of rocks, etc.\u003c/li\u003e\n\u003cli\u003eClarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eDehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation.\u003c/li\u003e\n\u003cli\u003eVibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated.\u003c/li\u003e\n\u003cli\u003eSaturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"tone-curve\"\u003eTone Curve\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThe square graph looks similar with histogram, but contain a line segment from bottom left to top right.\u003c/li\u003e\n\u003cli\u003eWe can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve.\u003c/li\u003e\n\u003cli\u003ewe can also pull RGB channel separately.\u003c/li\u003e\n\u003cli\u003eS curve to increase contrast\u003c/li\u003e\n\u003cli\u003eTips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis post will update frequently.\u003c/p\u003e","title":"Lightroom Notes"},{"content":"Recently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them. Virtualization How virtualization work Virtualization is the technology that allows a single physical machine, known as the host, to run multiple virtual machines or guests.\nThe key of virtualization is hypervisor. It\u0026rsquo;s the software that create and manages the virtual machines.\nType 1 (Bare-Metal): This hypervisor is installed directly onto the host\u0026rsquo;s hardware, acting as the operating system itself. Examples include VMware vSphere, Hyper-V. This type is common in data centers due to its high performance and efficiency Type 2 (Hosted): This hypervisor runs as an application on top of a conventional operating system(Like Windows, Linux). Examples include VMware Workstaion and Oracle VirtualBox. This approach is often used for desktop virtualization and development purposes. Once the hypervisor is in place, you can create one or more VMs. This involves\nAllocating resources(CPU cores, RAM, storage) Configuring virtual hardware(virtual network adapter, virtual storage controllers, virtual BIOS) Installing a guest OS, the guest os is unaware that it\u0026rsquo;s running in a virtualized environment. How WSL work: WSL 1: It did not run a real Linux kernel. Instead, it functioned as a real-time translation layer. It tricked Linux binaries into thinking they were communicating with a Linux kernel, when in reality, they were talking to a clever interpreter connected to the Windows kernel. WSL 2: Due to the limitations of translating every single Linux syscall, WSL 2 uses a lightweight, highly optimized type 1 virtual machine. Use case Server consolidation and optimization Development and testing envionments Application isolation and legacy application support Pros and Cons Pros:\nStrong isolation Total compatibility Cons:\nHeavyweight and slow Inefficient Containerization Containerization works by virtualizing the operating system, allowing an application to run in an isolated user space with all its dependencies, code and libraries. It all runs on a single host operating system and shared host OS\u0026rsquo;s kernel, making containers incredibly lightweight and fast.\nHow containers work Containers work by creating isolated environments for applications using two key technologies built into the host OS\u0026rsquo;s kernel:\nNamespaces: This feature provides isolation. Each container gets its own isolated view of resources like the network stack, process IDs, and filesystem mounts. This prevents conbtainers from seeing or interacting with each other or the host system. Control Groups(cgroups): This feature manages resource allocation. It limits and monitors how much of the host\u0026rsquo;s physical resources, such as CPU, RAM, I/O, each container can consume. This ensures no single container can monopolize the host\u0026rsquo;s resources. How container engine work Rule: A tool to create, run and manage container. It\u0026rsquo;s the translator/project manager between user and host\u0026rsquo;s OS kernel. Workflow: Creating a Dockerfile: A developer creates a text file defining all the steps required to build the application environment. Building an Image: The container engine reads the Dockerfile and packages it into a single, read-only, standardized image. This image serves as a static template for the container. Running a Container: The container engine uses this image to launch one or more container instances. During runtime, it creates an isolated namespace cgroups, while also adding a writable layer on top of the image to make the applications runnable. Docker vs Kubernetes vs Podman These are container enginers, but they serve very different purposes.\nDocker: The industry standard, it provides an all-in-one platform that includes a daemon, a client and an image registry(Docker hub). It\u0026rsquo;s easy to get started with and has a mature ecosystem Podman: A more security, daemonless alternative. Its command line is highly compatible with Docker\u0026rsquo;s, its run as a non-root user by default, and its architecture is more streamlined. Kubernetes: When applications scale up and need to run across hundreds or thousands of containers and multiple servers, Kubernetes is needed. It is a container orchestrator and servers as the brain of a container cluster. Kubernetes is not responsible for the actual running of containers. Instead, it handles higher-level management tasks such as automated deployment, elastic scaling, service discovery, load balancing and self healing. How distrobox works Distrobox is a clever tool that uses a container negine like Podman to create tightly integrated development environments. Its main purpose is to let you run any Linux distribution inside a container on your host OS but make it feel completely native.\nUse case Breaking down large, monolithic applications into smaller, independently deployable services Creating consistent and reproducible environments for building, testing and deploying software. Pros and Cons Pros\nLightweight and fast Highly portable Cons\nWeaker isolation You can only run containers that are compatible with the host OS kernel. Sandbox A sandbox is a secure, isolated environment on a computer where you can run untrusted program without risking harm to your host system.\nHow to create a sandbox Container Virtual machine Using dedicated sandbox software(like Sandboxie-Plus) Use case To be listed on the App Store or Google play store, and application must run in a sandbox. That\u0026rsquo;s why when we open a new app, the user have t approve a list of resources that enable the app to access. Education Browser plugin Pros and Cons Pros\nExtremely lightweight and fast: A sandbox applies rules to an already running process, adding minimal overhead. It\u0026rsquo;s instantaneous Targeted security: It\u0026rsquo;s perfect for its narrow purpose: running a single untrusted application and preventing it from accessing your personal files, network or hardware. Cons\nWeakest Isolation: It shares the host OS and kernel. Limited scape: It\u0026rsquo;s purely a security feature, not a deployment or development tool. ","permalink":"http://localhost:1313/posts/container_sandbox_vm/","summary":"\u003cp\u003eRecently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them.\n\u003cimg loading=\"lazy\" src=\"/Interesting_thing/distrobox.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"virtualization\"\u003eVirtualization\u003c/h2\u003e\n\u003ch4 id=\"how-virtualization-work\"\u003eHow virtualization work\u003c/h4\u003e\n\u003cp\u003eVirtualization is the technology that allows a single physical machine, known as the \u003cstrong\u003ehost\u003c/strong\u003e, to run multiple virtual machines or \u003cstrong\u003eguests\u003c/strong\u003e.\u003c/p\u003e","title":"Virtual machine vs Container vs Sandbox"},{"content":"The Global Interpreter Lock (GIL) At its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code. It is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment. The primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\nPython\u0026rsquo;s Memory Management Python uses a system called automatic memory management. Each object in Python has a reference count, which is a number that keeps track of how many variables or other object refer to it. When you assign a variable to an object, its reference count increases by one. When a reference is removed (for instance, when a variable goes out of scope), the count decreases. Once an object\u0026rsquo;s reference count drops to zero, it means nothing is using it.\nThis is where GIL becomes important. In a multi-threaded program, multiple threads could try to increase or decrease the same object\u0026rsquo;s reference count simultaneously, which may cause memory leaks.\nPython 3.13 GIL has become a bottleneck for CPU program performance, by reconstruct python, the development team has made a groundbreaking change in Python 3.13: the ability to disable the GIL.\nWhy python slow GIL Dynamic Datatype. It\u0026rsquo;s an interpreter language Easy to read makes python very abstract Trivias Generating functions are a powerful mathematical tool that transform discrete sequences into algebraic functions, enabling us to solve complex combinatorial counting problems through function operations. GPOS vs RTOS General purpose operating systems like Windows, macOS, and Linux are designed to provide a versatile, user-friendly computing environment. They prioritize overall system efficiency, with more flexible restrictions on task response times. Real time operating systems such as VxWorks, focus on strict time limitations for individual tasks, ensuring predictable and deterministic responses. These systems are typically used in environments demanding high reliability and precise time control, featuring a smaller, more streamlined kernel that guarantees real-time performance. In python, Boolean type is essentially a subclass of the integer type, True == 1 and False == 0. This design is mainly due to historical reasons and pragmatic considerations. In early version of Python(before 2.3), there was no dedicated bool type, and people used integers 1 and 0 to represent true and false. When the bool type was introuced, in order to allow old code to continue to run seamlessly, it was the best choice to continue to design it as a subclass of int. True + 1 = 2, False * 1 = 0 Huffman coding, an algorithm to compress data losslessly. Huffman coding assigns variable-length binary codes to input symbols. More frequent symbols -\u0026gt; shorter codes Less frequent symbols -\u0026gt; longer codes Windows vs Linux Aspect Linux Windows System Usage Lightweight, minimal background processes Heavy, many preloaded services and features Bloatware No pre-installed junk, user chooses all Comes with many default apps and features Transparency Fully open, config files are plain text Many hidden processes, registry-based config Customizability Highly customizable, from kernel to GUI Limited customization without hacking Privacy \u0026amp; Security User-controlled, minimal telemetry Sends telemetry, often needs antivirus Software Support Great for dev tools, less for gaming Excellent app/game compatibility System Control Full control over system and services Some restrictions, frequent auto-updates Hardware Support Good but sometimes manual setup required Plug-and-play for most consumer hardware Resouces Why the formular of normal distribution has a pi:explain from 3b1b Talks æ¼«å£«ï¼\n","permalink":"http://localhost:1313/posts/techweekly/techweek5/","summary":"\u003ch2 id=\"the-global-interpreter-lock-gil\"\u003eThe Global Interpreter Lock (GIL)\u003c/h2\u003e\n\u003cp\u003eAt its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code.\nIt is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment.\nThe primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\u003c/p\u003e","title":"Some Python Notes | King Weekly"},{"content":"Bitcoin The legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\nThe Blockchainâ›“ï¸ The blockchain is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\nIt\u0026rsquo;s a Chain of Blocks: Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block. It\u0026rsquo;s Immutable: Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending. The Genesis BlockğŸ“œ The very first block, known as the Genesis Block, was mined on 2009.01.04, by Bitcoin\u0026rsquo;s mysterious creator, Satoshi Nakamoto. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\nMiningâ›ï¸ Mining is the process of creating new blocks. It\u0026rsquo;s a competitive race that serves two critical purposes:\nValidating Transactions: Miners group pending transactions into a new block. Creating New Bitcoin: The winner of the race is rewarded with new bitcoin. Hereâ€™s how a miner wins the race and proves their block is valid:\nThe Hashing Puzzle: Miners take the data in their block and use their computers to find a specific number called a nonce. When the block data and the nonce are combined and put through a cryptographic function (SHA-256), they produce a unique digital fingerprint called a hash. The \u0026ldquo;Lower Than\u0026rdquo; Rule: To win, a miner must find a hash that is lower than the current network \u0026ldquo;target\u0026rdquo;. This target is a very large number that the entire network agrees on. Finding a hash below this target is incredibly difficult and requires immense computational powerâ€”it\u0026rsquo;s like trying to win a global lottery every 10 minutes. The Reward and The Halving: The first miner to find a valid hash wins the block reward. Initially, the reward was 50 BTC. However, this reward is programmed to cut in half roughly every four years (or 210,000 blocks) in an event called the halving. As of the April 2024 halving, the reward is now 3.125 BTC. This mechanism controls the supply of new bitcoin, making the currency scarce and ensuring its total amount will never exceed 21 million coins. When a winning block is found, its hash is broadcast across the P2P network. All other participants quickly verify that the hash is valid. Once confirmed, they add the new block to their copy of the blockchain and immediately start competing to solve the next block.\nHow to Mine: There are a few ways to participate in Bitcoin mining, each with its own pros and cons.\nSolo Mining: This is you, on your own, trying to solve a block. If you succeed, you get the entire block reward (3.125 BTC + transaction fees). However, the odds of a single person solving a block today are astronomically low due to the immense competition. It\u0026rsquo;s like buying a single lottery ticket and hoping to win the grand prize. Mining Pool: This is the most common method. You join a \u0026ldquo;pool\u0026rdquo; with thousands of other miners, combining your computing power. The pool works together to find blocks much more frequently. When the pool wins, the reward is split among all participants based on how much computing power they contributed. This provides smaller, but much more consistent and predictable, payouts. Block ForksğŸ´ A fork happens when the blockchain temporarily or permanently splits into two different paths.\nAccidental Fork: Sometimes, two miners find a valid block at almost the exact same time. The network briefly splits as some nodes follow one miner and some follow the other. This is usually resolved within a few minutes when the next block is found and added to one of the chains, making it the \u0026ldquo;longest\u0026rdquo; and therefore the official one. The shorter chain is then abandoned. Hard Fork: This is an intentional split that happens when the network\u0026rsquo;s software rules are changed in a way that is not backward-compatible. All participants must upgrade to the new rules to continue. If a significant portion of the community refuses to upgrade, the split becomes permanent, resulting in the creation of a new, separate cryptocurrency (e.g., Bitcoin Cash was created from a hard fork of Bitcoin). Ethics Mining bitcoin always consume immense energy, which critics view as a wasteful environmental cost for a seemingly meaningless computation. Proponents argue this mechanism decentralized financial system that offers freedom from the control of banks and governments.\nTools Calculate computing performance: https://www.nicehash.com/profitability-calculator\nThis is the computating power of my personal game laptop(one dollar per day hhh) Resources: Youtube channels\nhttps://www.youtube.com/watch?v=5hgdekVZb3A\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=3 https://www.youtube.com/watch?v=a41DMDfJjsU\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=2 https://Gemini.google.com Trivias Sherrington, coined the word \u0026ldquo;synapse\u0026rdquo; to define the connection between two neurons Two different roads of AI: connectionism and Symbolism. Hidden layer was firstly been implemented in Boltzmann machine, although Rosenblatt had some idea about multilayer perceptrons, but he didn\u0026rsquo;t find any useful training algorithm. Restricted Boltzmann machine - each layer is only allowed to be fully connectted to the next layer, current layer nodes are not connectted to each other. The advantages of this machine is it allows to update bodes within the same layer in parallel The invention of hidden layer allows model to understand abstract features. It also becomes to one of the most significant component in deep learning. The main difference between the Hopfield Network and the Boltzmann nachine is the presence of hidden layers. Other differences include the fact that the Hopfield network is deterministic, whereas the Boltzmann machine is stochastic, and the defintions of their energy function also differ. The fovea has many photoreceptors, with a high density of cones(for colors) and nearly no rods(for dark). This structure allows us to see the world clearly. If you develop myopia, the image formed after light is reflected by the eye may not be focused directly on the fovea. The idea of CNN was mainly inspired by the HMAX model(hierarchical, pooling, convolution), and the HMAX model was proposed by Tomaso Poggio, to simulate primate visual system, specifically ventral stream. Pytorch for research area, Tensorflow for industry and JAX for high level usage. Ethereum and ether is not the same as bitcoin. Ethereum has a longer vision, and the number of ether is unlimited. Resource Documentary of AlphaGo: https://www.youtube.com/watch?v=WXuK6gekU1Y How to use hugging face: https://www.youtube.com/watch?v=3kRB2TXewus This is the most comprehensive guide for AI beginner I had ever seen: Guide link ","permalink":"http://localhost:1313/posts/techweekly/techweek4/","summary":"\u003ch2 id=\"bitcoin\"\u003eBitcoin\u003c/h2\u003e\n\u003cp\u003eThe legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-blockchain\"\u003eThe Blockchainâ›“ï¸\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eblockchain\u003c/strong\u003e is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s a Chain of Blocks:\u003c/strong\u003e Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s Immutable:\u003c/strong\u003e Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-genesis-block\"\u003eThe Genesis BlockğŸ“œ\u003c/h4\u003e\n\u003cp\u003eThe very first block, known as the \u003cstrong\u003eGenesis Block\u003c/strong\u003e, was mined on \u003cstrong\u003e2009.01.04\u003c/strong\u003e, by Bitcoin\u0026rsquo;s mysterious creator, \u003cstrong\u003eSatoshi Nakamoto\u003c/strong\u003e. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\u003c/p\u003e","title":"Bitcoin Basic | King Weekly"},{"content":"2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning. However, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\nRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually. And before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation. So today I will record this moment.\nHopfield Network was invented at 1982. Processor John Hopfield designed it based on ideas from statistical mechanics.\nThe graph above shows two states of a ball From the left part, we can see the energy system of the ball is at the highest, which means the ball is very unstable From the right part, we can see the ball had already fall into the bottom, which means the ball is very stable. Although it is a classical physics model, but it definitly explain the main idea of hopfield network.\nSo now we can say hopfield network is just make a system move from an unstable state to stable state.\nIf you interested how exactly hopfield network work. See the video below, it\u0026rsquo;s pretty nice.\nThis idea is also useful in today. In LLM, we can say the user\u0026rsquo;s prompt and question is the most unstable states while the answer is the stable state.\nBased on the concept of Hopfield networks, many different architectures had been invented, which makes Connectionism and Deep Learning great again.\nAfter all, I catch the reason why nobel prize gives to physics.\n","permalink":"http://localhost:1313/posts/hopfieldnetwork/","summary":"\u003cp\u003e2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning.\nHowever, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\u003c/p\u003e\n\u003cp\u003eRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually.\nAnd before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation.\nSo today I will record this moment.\u003c/p\u003e","title":"Hopfield Network"},{"content":"Book - Unix: A History and a Memoir Recently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world. During the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\nAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals. The system developed before Unix was called Multics. Since â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€ Unix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10. Tools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs. Unix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems. GNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source. MacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX. In the early days, operating systems were not portable. This changed with the invention of the C language and its compiler. MINIX was widely used because it was embedded in Intel chips. The working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation. Microsoft once had its 3own Unix-like system. Another completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows. You can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan. â€œEverything is a fileâ€ is one of the core principles of Unix. The KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy. The UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\nKeep it simple stupid Do one thing, and do it well Everything is a file Make each program a filter Fail loudly Modularity Prototyping early In today, many barriers had already been removed.\nAnd I realise.\nThe revolution of AI is just like the reenactment of Unix\u0026rsquo;s development.\nSo.\nKISS.\nTrivias tty - TeleTYpewriter, terminal in the old time, before lcd screen been invented. UNIX was developed on the PDP-7, a computer with no screen, no mouse and only 8KB of RAM. It weighted nearly 500kg. UNIX and UNIX-like system use abbreviated commands because typing on TTY terminals in the 1960s was slow and insufficient. Second-system effect: It believes that after completing a small, elegant, and successful system, people tend to have overly high expectations for the next projects, which may lead to the creation of a huge, feature-rich but monstrous system.The \u0026ldquo;second-system effect\u0026rdquo; can result in software project plans being overdesigned, with too many variables and excessive complexity, ultimately falling short of expectations and leading to failure. such as PL/I in Multics Fortran(formular translation): The purpose of this lagnauge is to proceed mathematics formular and float number in an efficient way, like integration, linear algebra. That\u0026rsquo;s why fortran is still popular in some supercomputer and scientific calculation. B lagnauge is designed in a bit-unit computer PCP-7, where C langauge is designed in a byte-unit computer. Therefore the main difference between B and C is B langauge doesn\u0026rsquo;t support types and C does. Development of Clang: PL/I -\u0026gt; BCPL -\u0026gt; B -\u0026gt; New B(C) If computers using the same cpu architecture, they will using the same assembly language. The grep command is used to find lines that match a specific pattern in a file while the sed command is used to insert, replace and delete text from a file. Finally, the awk command supports programming logic and is often used for advanced data processing tasks. The development of UNIX from 1970 until now. Talks \u0026ldquo;A new technological discovery is often discredited by older generations of professionals - especially those with high authority and prestige in the existing field - in order to protect their own status\u0026rdquo; ","permalink":"http://localhost:1313/posts/techweekly/techweek3/","summary":"\u003ch2 id=\"book---unix-a-history-and-a-memoir\"\u003eBook - Unix: A History and a Memoir\u003c/h2\u003e\n\u003cp\u003eRecently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world.\nDuring the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals.\u003c/li\u003e\n\u003cli\u003eThe system developed before Unix was called Multics.\u003c/li\u003e\n\u003cli\u003eSince â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€\u003c/li\u003e\n\u003cli\u003eUnix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10.\u003c/li\u003e\n\u003cli\u003eTools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs.\u003c/li\u003e\n\u003cli\u003eUnix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems.\u003c/li\u003e\n\u003cli\u003eGNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source.\u003c/li\u003e\n\u003cli\u003eMacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX.\u003c/li\u003e\n\u003cli\u003eIn the early days, operating systems were not portable. This changed with the invention of the C language and its compiler.\u003c/li\u003e\n\u003cli\u003eMINIX was widely used because it was embedded in Intel chips.\u003c/li\u003e\n\u003cli\u003eThe working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation.\u003c/li\u003e\n\u003cli\u003eMicrosoft once had its 3own Unix-like system.\u003c/li\u003e\n\u003cli\u003eAnother completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows.\u003c/li\u003e\n\u003cli\u003eYou can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan.\u003c/li\u003e\n\u003cli\u003eâ€œEverything is a fileâ€ is one of the core principles of Unix.\u003c/li\u003e\n\u003cli\u003eThe KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\u003c/p\u003e","title":"For UNIX Week | King Weekly"},{"content":"Model Context Protocol overview MCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\nWhy is MCP needed? In the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces). The current components of MCP servers include: How does MCP work? Example: Suppose you\u0026rsquo;re using an AI assistant to manage your work, and it wants to help you schedule today\u0026rsquo;s meeting:\nWithout MCP, developers would need to write separate integration code for Outlook, Google Calendar, and Apple Calendar.\nWith MCP, the AI only needs to call the MCP server, which will automatically interface with your calendar system. No matter which calendar service you use, the AI can work seamlessly.\nCore Functions of MCP: Reducing development costs (no need to develop separate integrations for each tool).\nEnhancing AI\u0026rsquo;s ability to access external data (allowing AI to easily query and manipulate external data).\nStandardizing communication (making communication between different AI applications and tools smoother).\nYou can think of MCP as the \u0026ldquo;USB interface for AI\u0026rdquo;â€”any AI device can plug into different tools without needing to individually adapt to each one!\nTrivias: Graph is a great data structure, it can be used to find the shortest path, solve a magic cube. Rely on this data structure, Human find the shortest steps to solve the worst case configured 3\\*3\\*3 magic cube in 20 steps (which also called god\u0026rsquo;s number) and 11 steps for 2\\*2\\*2 one. What about n*n*n, Here is an interesing paper about time compleixity of solving a n*n*n magic cube: Algorithms for Solving Rubikâ€™s Cubes Topological sort is an algorithm based on DFS and DAG, It\u0026rsquo;s not a traditional sorting algorithm, like comparing the size of each number and sort them, but sorting based on dependencies. Dynamic programming is just like recursion + memorization + guessing P vs NP problems: P is a problem that can be solved easily by a computer, NP is a problem that you can check for correctness very easily once solved. However, P != NP for example wec can\u0026rsquo;t engineer luck. Also, NP hard means it is at least as hard as any problem in NP, and NP-complete is lke if you can solve one NP-complete question, you can solve all NP question. Reduction is like to prove a known NP-complete question, and transfer it into a NP question X, then X is also a NP-complete question HTTP vs REST API HTTP is a protocol, and its core task is to define how to request and transfer data between your broswer and server\nIt focuses on the data exchange level, regardless of whether the data is an image, text, video, or API response. For example, you can use an HTTP request to access a webpage (HTML page) or use an HTTP request to retrieve API data (JSON data). It doesnâ€™t care about what data you\u0026rsquo;re transmitting. REST API is a design style, based on the HTTP protocol, with rules and best practices:\nIt views content on the server as \u0026ldquo;resources\u0026rdquo; and specifies how to operate on them using HTTP methods (GET, POST, PUT, DELETE). REST API design considers resource orientation: Resources (like users, articles, comments, etc.) have unique identifiers (URIs), and clients interact with these resources through HTTP requests. For example, in a REST API, you can:\nGET request: Retrieve a resource (e.g., get movie information). POST request: Create a new resource (e.g., submit a new comment). PUT request: Update a resource (e.g., modify user information). DELETE request: Delete a resource (e.g., remove an article). Resource: A tool to build a personal streaming music player: navidrome A great blog that introduce human visualization: human visualization Five basic algorithms explanation: Dynamic programming Greedy Algorithm Backtracking Branch and Bounding This week, Nvidia GTC 2025 brings a lot of new tech, PC in AI age, B300, new architecture, robots. After watched the coverage keynotes, I was just feel like our mankind is in an special stage with unpredictable pace. In science fiction films, at this point , it often leads to the arrival of an alien civilization. A dijkstra algorithm visualiser that helps me understand it: Dijkstra shortest path MCP explain: A blog that explain MCP crystal clear ","permalink":"http://localhost:1313/posts/techweekly/techweek2/","summary":"\u003ch2 id=\"model-context-protocol-overview\"\u003eModel Context Protocol overview\u003c/h2\u003e\n\u003cp\u003eMCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\u003c/p\u003e\n\u003ch4 id=\"why-is-mcp-needed\"\u003eWhy is MCP needed?\u003c/h4\u003e\n\u003cp\u003eIn the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\u003cbr\u003e\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces).\n\u003cimg alt=\"components mcp include\" loading=\"lazy\" src=\"/TechStuff/mcp.png\"\u003e\u003c/p\u003e","title":"Jarvis Will Coming Soon | King Weekly"},{"content":"Email sending and receiving system The main system is build based on three protocols: SMTP, POP3 and IMAP. SMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails. User1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\nPOP3 downloads emails from the email server to the email client. By default, it removes emails from the server after downloading, but some email clients allow users to keep copies on the server. IMAP keeps emails on the server and synchronizes them across multiple devices. The email client initially loads only the headers, and the full email content is fetched from the server when the user opens it.\nIf you want to customize an email domain. You need to have your own SMTP and IMAP/POP3 server and a domain, the other steps are the same as above.\nPKGBUILD in Arch Linux PKGBUILD is a bash script contain the build information required by archlinux package we use makepkg script to build the package, it will search PKGBUILD first in the current folder. Benefits:\nusing pacman to manage, user can update and uninstall easily some pkgbuild file include the commands to generate a binary file and store it in /user/bin Drawbacks:\nNot friendly to starter Although we can use yay to help us do all these stuff.\nTrivias Newton\u0026rsquo;s method is quadratic convergence when we want to calculate the root of a number Catalan number is a group of sequence that appear widely in combinatorics. e.g. ways to arrange n brackets, number of triangles in an n+2 convex polygon. The common property of these applications is that they are recursive and have a constrained structure. Toom cook: It divide a d-digit number into n parts and doing arithmatic calculations. Schonhage-strassen scheme: It multiplies two integers of length ğ‘› in O (ğ‘› logğ‘› log logğ‘›) steps on a multitape Turing machine A Naive algorithm is usually the most obvious solution when one is asked a problem. It may not be a smart algorithm but will probably get the job done The taste of red wine is determined by acidity, sweetness, alcohol content, tannins, and body. Wines are categorized into New World and Old World. New World wines (from countries like the USA, Chile, Argentina, and China) are named after the grape variety, while Old World wines (mainly from Europe) are named after their place of origin.\nRed wine is made by fermenting red grapes with their skins. White wine is made from either white grapes or red grapes without their skins. RosÃ© wine is made by soaking the grape skins briefly but fermenting without them. Sparkling wine undergoes a second fermentation to produce bubbles. Gabriel\u0026rsquo;s horn is a type of geometric figure that has infinite surface area but finite volume. Resources Code question(leetcode), system design question(crack the code interview), teamwork, communication are all important in the interview. An old guideline to learn ai: https://www.captainai.net/itcoke/ A guideline to learn CS: https://csdiy.wiki/åè®°/ Useful tips to integrate by parts, åå¯¹å¹‚æŒ‡ä¸‰, to choose u. Customize your zsh: oh my zsh Xiaomi releases a concept modular camera, it looks pretty awesome and innovative. 3b1b\u0026rsquo;s taylor series explaination:https://www.youtube.com/watch?v=3d6DsjIBzJ4 3b1b\u0026rsquo;s explaination of why we have exponential e:https://www.youtube.com/watch?v=m2MIpDrF7Es This website is all about competitive writing of source code that is as short as possible: Codewolf Explaination of greedy algorithm: greedy algorithm Explaination of dynamic programming: dynamic programming Deploy perosonal VPN tools: tailscale Abstract If no one is reading blogs anymore, why should we write them? Letâ€™s make it simple: you write a blog, but nobody cares, nobody reads it. At least, the number of readers is not as many as you thought. You put your personal ideas and thoughts into the article, carefully structuring each sentence, and choose a great imageâ€”then, no response, no likes, no shares, no activity. So, what is the meaning of writing a blog? First, there are two misconceptions about blogging. One is that if I write a good article, readers will come naturally. No, they wonâ€™t come. There are billions of blogs on the internet, like a massive hurricane, and yours is just a single leaf in the wind. Who would notice? Another misconception is that if nobody reads it, writing is a waste of time. Blogs have their own hidden value. You write blogs not for the applause of others, but for your own needs. Blogs help clear your mind. They help you organize your thoughts and sharpen your perspective. When you think better, you will achieve better results. The target audience of a blog is actually not the people on the internet, but your future self. Your article will help you see the evolution of your own thoughts. Additionally, one day in the future, someone who truly needs your article will find it. A deep, thoughtful article has a longer-lasting impact than a viral article. Writing a blog is quite like street photography. You take your camera and walk through the city. You see a sceneâ€”a moment filled with light, shadow, and humanityâ€”and then you capture it. Nobody cares about what you actually captured. But thatâ€™s not the reason you photograph; you photograph because you see something interesting. Writing a blog is the same. You write a blog because you are thinking, observing new things, and hope to store them somewhere. If someone reads it, that\u0026rsquo;s great. If not, youâ€™ve still completed your work\n","permalink":"http://localhost:1313/posts/techweekly/techweek1/","summary":"\u003ch2 id=\"email-sending-and-receiving-system\"\u003eEmail sending and receiving system\u003c/h2\u003e\n\u003cp\u003eThe main system is build based on three protocols: SMTP, POP3 and IMAP.\n\u003cimg alt=\"process of email system\" loading=\"lazy\" src=\"/emailsys.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails.\nUser1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\u003c/p\u003e","title":"Start | King Weekly"},{"content":"In this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O O(n) notation before, but for Big theta Î¸(n) and reccurence relations T(n), I never heard them before. Today, I hope I can finally figure them out.\nWhat T(n) represents the actual running time of an algorithm\nO(n) represents the asymptotic upper bound of the running time of an algorithm\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\nHow to convert the three of them Normally we can directly transfer T(n) to O(n) or Î¸(n). Int sum = 0 for (i = 1; i \u0026lt;= n, i ++) { sum = sum + i } This is a classical example, First of all, we initialize variable sum requires one unit of running time. There are three statements inside the for loop, statement 1 i = 1 requires one unit of the running time, statement 2 i \u0026lt;= n requires n+1 units of the running time, statement 3 i ++ requires n units of the runningn time, and sum = sum + i requires 2n units of the running time, n for addition and n for assignment. Therefore T(n) = 1+1+(n+1)+n+2n = 4n + 3. In O(n), we ignore the constant and the lower-order terms, therefore the time complexity is O(n) / Î¸(n).\nWhen the algorithm is a recursion, such as karatsuba multiplication and high precision multiplication. There are two methods to convert T(n) into O(n)\nRecursion tree method According to Recursion tree method, we derive master theorem The time complexity of multiplication is equal to the time complexity of division\n","permalink":"http://localhost:1313/posts/the-difference-between-tn--on-and-%CE%B8n/","summary":"\u003cp\u003eIn this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O \u003ccode\u003eO(n)\u003c/code\u003e notation before, but for Big theta \u003ccode\u003eÎ¸(n)\u003c/code\u003e and reccurence relations \u003ccode\u003eT(n)\u003c/code\u003e, I never heard them before. Today, I hope I can finally figure them out.\u003c/p\u003e\n\u003ch1 id=\"what\"\u003eWhat\u003c/h1\u003e\n\u003cp\u003eT(n) represents the actual running time of an algorithm\u003cbr\u003e\nO(n) represents the asymptotic upper bound of the running time of an algorithm\u003cbr\u003e\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\u003c/p\u003e","title":"Time Complexity Notations"},{"content":"This week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\nOverview The machine model is HP-Elitedesk-800-G4-SFF. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\nIf you want to learn more here is the machine datasheet:server_datasheet\nHardware Motherboard: Q370 viewer CPU: i5-8500 GPU: intel UHD 630 RAM: 16G SSD: 256G HDD: 500G * 1 The motherboard provides a high flexibility to expand more internal storage, also it has 4 PCie expansion slots which can used to expand more storage space or other components you want.\nSoftware nextcloud\nemail domain\ngitlab\nminecraft server\ndocker\njellyin\nsynthing\nproxy?router?gateway?\nvirtual machine\nI host my server with ubuntu server distro. The reason I didn\u0026rsquo;t choose proxmox is because I want to learn server step by step, proxmox is great in visualization, maybe in the future, I will try it.\nDurign the process of configuring storage, I learned LVM, which is a wonderful tool for those users that has multiple drives. User can create a storage pool called volume group. Firstly, user add their physical volumes into volumn group, and we create logical volumn based on the storage area had in volumn groups, and then we mount those LVs with the actural dirctory. It seems like Windows is not able to achieve this function. For RAID, we seperate it into 4 different categories, radi0, raid1, raid5 and raid10. This tools shows how to save files in different number of drives or in LVM.\nFor external access, I plan to use cloudflare tunnel. They provide such service, I need to buy a domain name and combine it with the cloudflare tunnel, and when I access the server, I firstly type the domain name in my browser to ask cloudflare, and they will guide me to the tunnel to my server, also in server end, I need to install cloudflared docker image as an end, then it works! It\u0026rsquo;s so convenient for those people who live in school accommodation. And it\u0026rsquo;s totally free!\nIn my network configuration. I didn\u0026rsquo;t install a router, but the best choice is to use a router for all devices in my home, and assign each of them an static IP address. To access the server, I bought a portable monitor, since the IP address of my server using DHCP, which required to check the IP address manually when the machine reboot or close. This issue will be solved easily when I have a router. Nowadays, I need to change the tunnels configuration to enable external access.\nsince I have 3 different operating systems in my three daily devices, phone for android, laptop for arachlinux and ipad for ipados, I installed nextcloud docker to try to integrate them into one ecosystem, that pretty awesome.\nUpdate Nextcloud for cloud storage Able to build game server Music and video live streaming server. Future Mining bitcoin ","permalink":"http://localhost:1313/posts/homeserver/","summary":"\u003cp\u003eThis week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThe machine model is \u003ccode\u003eHP-Elitedesk-800-G4-SFF\u003c/code\u003e. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\u003cbr\u003e\n\u003cimg loading=\"lazy\" src=\"/TechStuff/serverbill.png\"\u003e\u003c/p\u003e","title":"My Homelab"},{"content":"This semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\nIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\nHow the JVM works The JVM executes er programs in several stages:\nCompilation: Java source code (.java) is compiled into bytecode (.class) by the Java Compiler (javac).\nClass Loading: The JVM loads compiled bytecode when required.\nBytecode Verification: Ensures security and correctness before execution.\nExecution: The JVM interprets or compiles bytecode using Just-In-Time (JIT) compilation.\nGarbage Collection: The JVM automatically manages memory, reclaiming unused objects.\nClass loader One of the organizational units of JVM byte code is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.\nLet\u0026rsquo;s explore this concept with an example: Imagine you\u0026rsquo;re watching a movie on a streaming service.\nLoading:\nThe service first finds and imports the movie data you want to watch. Similarly, the class loader locates the binary data for a Java class. Linking: Verification: Before you watch the movie, the service checks that the file isn\u0026rsquo;t corrupted. In Java, the class loader verifies the correctness of the class. Preparation: The service sets up the necessary space in memory to buffer the movie. Java allocates memory for variables and sets default values. Resolution: The service ensures all necessary subtitles or audio tracks are ready to play. Java resolves references to make them direct. Initialization:\nAs you start watching, the service begins playing the movie. Similarly, Java runs code to set up variables with their starting values. Class Loader Types:\nBootstrap Class Loader: Like the service\u0026rsquo;s core library of well-known movies, it loads fundamental, trusted classes. Extension Class Loader: Similar to special add-on features, it loads additional classes outside the core library. System/Application Class Loader: Like searching for new releases or user-uploaded content, it loads classes specific to the application youâ€™re using. Virtual machine architecture Cross-Platform Compatibility and Limitations The JVM abstracts away the underlying hardware and operating system specifics, allowing Java bytecode to run on any device equipped with a compatible JVM. This cross-platform capability greatly simplified software distribution and development, as developers could write code once and deploy it across various platforms without modification.\nDespite its strong cross-platform capabilities, the JVM faced significant challenges due to competitive corporate strategies, particularly the \u0026ldquo;Embrace, Extend, and Extinguish\u0026rdquo; (EEE) approach adopted by some companies like Microsoft in the late 1990s. This strategy involved embracing a technology, extending it with proprietary features, and eventually using those extensions to undermine the original technology.\nMicrosoft initially embraced Java by integrating it into their Internet Explorer browser and Windows platforms. However, they extended Java with proprietary features that were specific to Windows, creating a version of Java that was incompatible with the standard JVM specifications set by Sun Microsystems. This move fragmented the Java platform and undermined the \u0026ldquo;Write Once, Run Anywhere\u0026rdquo; philosophy.\nWith the development and rise of programming languages like Swift, Kotlin, and JavaScript, the JVM faced significant challenges in maintaining its performance edge. Swift, designed by Apple for iOS and macOS platforms, offers high performance and safety due to its compiled nature and modern language features. Kotlin, although initially running on the JVM, introduced concise syntax and advanced features that surpassed Java in many ways, leading it to become the preferred language for Android development. JavaScript\u0026rsquo;s performance greatly improved with engines like V8, and its versatility expanded through technologies like Node.js for server-side development. These languages not only matched but often exceeded the JVM\u0026rsquo;s performance and adaptability in their respective domains, leading to a shift in developer preferences and a relative decline in the JVM\u0026rsquo;s dominance.\nOverall Nowadays, Java has become to a normal programming lauguage. And the question is obvious solved. Python has its own interpreter to transfer the source code into machine code. Haskell has its own compiler, C++/C are compiled directly into machine code. However,they can\u0026rsquo;t generate a compiled file that enable to run in every operating system. If there is no EEE strategy, Linux may have a stronger effects in today\u0026rsquo;s world.\n","permalink":"http://localhost:1313/posts/jvm/","summary":"\u003cp\u003eThis semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\u003c/p\u003e","title":"JVM's Achievements and Limitations"},{"content":" ğŸ¤— ","permalink":"http://localhost:1313/aboutme/","summary":"aboutme","title":"About me"},{"content":"Location: Auchinstarry Quarry, Scotland Date: 2025.09.12 A great first-time rock climbing experience, meet interesting people and learned some rope skills.\n","permalink":"http://localhost:1313/aboutme/climbing/","summary":"\u003ch2 id=\"location-auchinstarry-quarry-scotland\"\u003eLocation: Auchinstarry Quarry, Scotland\u003c/h2\u003e\n\u003cp\u003eDate: 2025.09.12\n\u003cimg alt=\"rock and me\" loading=\"lazy\" src=\"/Aboutme/croy_rock_and_me.jpg\"\u003e\nA great first-time rock climbing experience, meet interesting people and learned some rope skills.\u003c/p\u003e","title":"Climbing road"},{"content":" A free web application that create a two-host daily podcast of the news that matters to you. Just add your favorite RSS feeds from news sites, blogs, and social media. Our app will automatically generate conversational scripts using an LLM and then produce a customized audio episode with text-to-speech. It\u0026rsquo;s the perfect way to stay updated on your commute.\nOfficial website Code available in github\nMotivation The idea of having a J.A.R.V.I.S. from the Marvel movies, is a common dream. With today\u0026rsquo;s advancements in LLM and TTS technology, we can now offer a real-world experience that captures a part of that dream.\nFor me, I love to start my mornings by checking my favorite blogs for updates, scrolling through X for new tech news, or quickly scanning the day\u0026rsquo;s headlines. With this application, I could simply wake up, click a button to generate my podcast, and then, while having breakfast or commuting, listen to a personalized summary of everything that happened overnight. If a specific story catches my interest, I can then go back and read the full article later. I think its an efficient way to stay informed.\nTech stack Area Technology Backend FastAPI, Python, SQLAlchemy, PostgreSQL, Pydantic Frontend React, Javascript, TypeScript, Vite, Tailwind CSS AI \u0026amp; TTS Google Gemini 2.5 Pro API Deployment Self-host(personal server), Docker Compose(Backend, Frontend, background worker, PostgreSQL, Valkey) AI assistant Gemini 2.5 pro, Claude sonnet 4 Issues I met Gemini api timeout: The current Gemini API has a 60-second request timeout, which is insufficient for generating a 10-minute podcast with a single text-to-speech (TTS) request. To solve this, my initial thought was to segment the entire podcast script into smaller parts and generate each segment separately. However, a new challenge arose with the TTS service\u0026rsquo;s free plan, which limits me to just 15 requests per day. This makes the segmentation approach impractical for daily use. My temporary fix has been to adjust the script prompt to generate shorter content, but this method is unreliable and doesn\u0026rsquo;t always prevent timeouts. The long-term solution will be to upgrade to a paid TTS service with a longer API request timeout. This will allow for single, uninterrupted podcast generation, solving the issue at its root. Roadmap 2025-09: Migrate deployment from Render to my personal server âœ… - 2025.09.21 2025-10: Add more social medias RSS feeds 2025-12: Multi-format input file support Future: Use fine-tuning TTS model Improve user experience Support spotify export Summary This project takes me 2 month from learning tech stacks to final outcome. I want to say AI really helped me a lot during the building process, whereas from learning tech stacks to fix bugs. From this prject, I experience both frontend and backend project building, frontend is just like building an artwprk, it really need me to keep patient, backend is more interesting since it relative to many different fields like crud operations, building database schemas and endpoints and connection between database, background worker. I also experience the magic of containerization, its so convenient. - 2025.08.14\n","permalink":"http://localhost:1313/aboutme/projects/lets_break_the_information_gap/","summary":"aboutme","title":"Let's break the information gap"},{"content":"ğŸ“· Welcome to my photography exhibition! ","permalink":"http://localhost:1313/aboutme/gallery/","summary":"\u003ch1 id=\"heading\"\u003eğŸ“·\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"welcome-to-my-photography-exhibition\"\u003eWelcome to my photography exhibition!\u003c/h1\u003e","title":"Photography exhibition by King Jin"},{"content":" Let\u0026rsquo;s break the information gap A free web application that create a two-host daily podcast of the news that matters to you. Reflex An AI-powered, English translator that generates a unique conversational story each day based on your search history. ","permalink":"http://localhost:1313/aboutme/projects/","summary":"\u003cpicture class=\"center\"\u003e\n  \u003c!-- Dark mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects_dark.png\" media=\"(prefers-color-scheme: dark)\"\u003e\n  \u003c!-- Light mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects.png\" media=\"(prefers-color-scheme: light)\"\u003e\n  \u003c!-- Fallback image --\u003e\n  \u003cimg src=\"/Aboutme/projects.png\" alt=\"Project image\"\u003e\n\u003c/picture\u003e\n\u003cbr\u003e\n\u003cbr\u003e\n\u003c!-- Project lbtig --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003eLet\u0026rsquo;s break the information gap\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003e\u003cimg src=\"/Aboutme/lbtig_preview.png\" class=\"center\"\u003e\u003c/a\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp class=\"center-wider\"\u003e A free web application that create a two-host daily podcast of the news that matters to you.\n\u003c/p\u003e\n\u003chr\u003e\n\u003cbr\u003e\n\u003c!-- Project reflex --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003eReflex\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003e\u003cimg src=\"/Aboutme/reflex_preview.png\" class=\"center\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cbr\u003e\n\u003cp class=\"center-wider\"\u003e An AI-powered, English translator that generates a unique conversational story each day based on your search history.\n\u003c/p\u003e","title":"Project by King Jin"},{"content":" An AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\ncode available in github Motivation I dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\nTech stack Frontend: HTML, Tailwind CSS, Electron, Javascript\nRoadmap Future: support multi-language data record system\nSummary This project takes me one week to build. To be honest, Electron is convenient for cross platform, but also it increase the size of the software.\n","permalink":"http://localhost:1313/aboutme/projects/reflex/","summary":"\u003cblockquote\u003e\n\u003cp\u003eAn AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Kingjinsight/Reflex\"\u003ecode available in github\u003c/a\u003e\n\u003cimg alt=\"reflex_dashboard\" loading=\"lazy\" src=\"/Aboutme/reflex_dashboard.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\u003c/p\u003e","title":"Reflex"},{"content":"I really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\nColor Theory. Additive Color: RGB for Red, Green and Blue. Red + Green + Blue = White Subtractive Color: These are created by mixing two additive colors. Red + Green = Yellow Green + Blue = Cyan Red + Blue = Magenta Yellow + Cyan + Magenta = Black Basics Histogram: a graph that displays the brightness and color distribution of an image From left to right, each column represents the number of the white-black pixels in different level It has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites. There are two triangles at the top left and top right. the top left one is shadow clipping which will show you whether your image has region that was too dark the top right one is highlight clipping which show you whether your image has region that was too light the reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0. There are several colors in the triangle gray - all details are well preserved red - red channel overexposure/underexposure green - green channel overexposure/underexposure blue - blue channel overexposure/underexposure yellow - yellow channel overexposure/underexposure cyan - cyan channel overexposure/underexposure magenta - magenta channel overexposure/underexposure solid white - RGB channel channel overexposure/underexposure in the same time White balance Temperature Tint Tone Exposure: change the overall brightness of the photo Contrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa. Highlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc. Shadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc. Whites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white. Blacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black. Presence Texture: Focus on the surface texture, such as skin pores, surface of rocks, etc. Clarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;. Dehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation. Vibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated. Saturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree. Tone Curve The square graph looks similar with histogram, but contain a line segment from bottom left to top right. We can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve. we can also pull RGB channel separately. S curve to increase contrast Tips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more. This post will update frequently.\n","permalink":"http://localhost:1313/posts/lightroom/","summary":"\u003cp\u003eI really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\u003c/p\u003e\n\u003ch2 id=\"color-theory\"\u003eColor Theory.\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAdditive Color: RGB for Red, Green and Blue.\n\u003cul\u003e\n\u003cli\u003eRed + Green + Blue = White\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSubtractive Color: These are created by mixing two additive colors.\n\u003cul\u003e\n\u003cli\u003eRed + Green = Yellow\u003c/li\u003e\n\u003cli\u003eGreen + Blue = Cyan\u003c/li\u003e\n\u003cli\u003eRed + Blue = Magenta\u003c/li\u003e\n\u003cli\u003eYellow + Cyan + Magenta = Black\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"basics\"\u003eBasics\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHistogram: a graph that displays the brightness and color distribution of an image\n\u003col\u003e\n\u003cli\u003eFrom left to right, each column represents the number of the white-black pixels in different level\u003c/li\u003e\n\u003cli\u003eIt has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites.\u003c/li\u003e\n\u003cli\u003eThere are two triangles at the top left and top right.\n\u003col\u003e\n\u003cli\u003ethe top left one is shadow clipping which will show you whether your image has region that was too dark\u003c/li\u003e\n\u003cli\u003ethe top right one is highlight clipping which show you whether your image has region that was too light\u003c/li\u003e\n\u003cli\u003ethe reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eThere are several colors in the triangle\n\u003col\u003e\n\u003cli\u003egray - all details are well preserved\u003c/li\u003e\n\u003cli\u003ered - red channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003egreen - green channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eblue - blue channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eyellow - yellow channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003ecyan - cyan channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003emagenta - magenta channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003esolid white - RGB channel channel overexposure/underexposure in the same time\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eWhite balance\n\u003col\u003e\n\u003cli\u003eTemperature\u003c/li\u003e\n\u003cli\u003eTint\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eTone\n\u003col\u003e\n\u003cli\u003eExposure: change the overall brightness of the photo\u003c/li\u003e\n\u003cli\u003eContrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa.\u003c/li\u003e\n\u003cli\u003eHighlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc.\u003c/li\u003e\n\u003cli\u003eShadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc.\u003c/li\u003e\n\u003cli\u003eWhites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white.\u003c/li\u003e\n\u003cli\u003eBlacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePresence\n\u003col\u003e\n\u003cli\u003eTexture: Focus on the surface texture, such as skin pores, surface of rocks, etc.\u003c/li\u003e\n\u003cli\u003eClarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eDehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation.\u003c/li\u003e\n\u003cli\u003eVibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated.\u003c/li\u003e\n\u003cli\u003eSaturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"tone-curve\"\u003eTone Curve\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThe square graph looks similar with histogram, but contain a line segment from bottom left to top right.\u003c/li\u003e\n\u003cli\u003eWe can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve.\u003c/li\u003e\n\u003cli\u003ewe can also pull RGB channel separately.\u003c/li\u003e\n\u003cli\u003eS curve to increase contrast\u003c/li\u003e\n\u003cli\u003eTips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis post will update frequently.\u003c/p\u003e","title":"Lightroom Notes"},{"content":"Recently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them. Virtualization How virtualization work Virtualization is the technology that allows a single physical machine, known as the host, to run multiple virtual machines or guests.\nThe key of virtualization is hypervisor. It\u0026rsquo;s the software that create and manages the virtual machines.\nType 1 (Bare-Metal): This hypervisor is installed directly onto the host\u0026rsquo;s hardware, acting as the operating system itself. Examples include VMware vSphere, Hyper-V. This type is common in data centers due to its high performance and efficiency Type 2 (Hosted): This hypervisor runs as an application on top of a conventional operating system(Like Windows, Linux). Examples include VMware Workstaion and Oracle VirtualBox. This approach is often used for desktop virtualization and development purposes. Once the hypervisor is in place, you can create one or more VMs. This involves\nAllocating resources(CPU cores, RAM, storage) Configuring virtual hardware(virtual network adapter, virtual storage controllers, virtual BIOS) Installing a guest OS, the guest os is unaware that it\u0026rsquo;s running in a virtualized environment. How WSL work: WSL 1: It did not run a real Linux kernel. Instead, it functioned as a real-time translation layer. It tricked Linux binaries into thinking they were communicating with a Linux kernel, when in reality, they were talking to a clever interpreter connected to the Windows kernel. WSL 2: Due to the limitations of translating every single Linux syscall, WSL 2 uses a lightweight, highly optimized type 1 virtual machine. Use case Server consolidation and optimization Development and testing envionments Application isolation and legacy application support Pros and Cons Pros:\nStrong isolation Total compatibility Cons:\nHeavyweight and slow Inefficient Containerization Containerization works by virtualizing the operating system, allowing an application to run in an isolated user space with all its dependencies, code and libraries. It all runs on a single host operating system and shared host OS\u0026rsquo;s kernel, making containers incredibly lightweight and fast.\nHow containers work Containers work by creating isolated environments for applications using two key technologies built into the host OS\u0026rsquo;s kernel:\nNamespaces: This feature provides isolation. Each container gets its own isolated view of resources like the network stack, process IDs, and filesystem mounts. This prevents conbtainers from seeing or interacting with each other or the host system. Control Groups(cgroups): This feature manages resource allocation. It limits and monitors how much of the host\u0026rsquo;s physical resources, such as CPU, RAM, I/O, each container can consume. This ensures no single container can monopolize the host\u0026rsquo;s resources. How container engine work Rule: A tool to create, run and manage container. It\u0026rsquo;s the translator/project manager between user and host\u0026rsquo;s OS kernel. Workflow: Creating a Dockerfile: A developer creates a text file defining all the steps required to build the application environment. Building an Image: The container engine reads the Dockerfile and packages it into a single, read-only, standardized image. This image serves as a static template for the container. Running a Container: The container engine uses this image to launch one or more container instances. During runtime, it creates an isolated namespace cgroups, while also adding a writable layer on top of the image to make the applications runnable. Docker vs Kubernetes vs Podman These are container enginers, but they serve very different purposes.\nDocker: The industry standard, it provides an all-in-one platform that includes a daemon, a client and an image registry(Docker hub). It\u0026rsquo;s easy to get started with and has a mature ecosystem Podman: A more security, daemonless alternative. Its command line is highly compatible with Docker\u0026rsquo;s, its run as a non-root user by default, and its architecture is more streamlined. Kubernetes: When applications scale up and need to run across hundreds or thousands of containers and multiple servers, Kubernetes is needed. It is a container orchestrator and servers as the brain of a container cluster. Kubernetes is not responsible for the actual running of containers. Instead, it handles higher-level management tasks such as automated deployment, elastic scaling, service discovery, load balancing and self healing. How distrobox works Distrobox is a clever tool that uses a container negine like Podman to create tightly integrated development environments. Its main purpose is to let you run any Linux distribution inside a container on your host OS but make it feel completely native.\nUse case Breaking down large, monolithic applications into smaller, independently deployable services Creating consistent and reproducible environments for building, testing and deploying software. Pros and Cons Pros\nLightweight and fast Highly portable Cons\nWeaker isolation You can only run containers that are compatible with the host OS kernel. Sandbox A sandbox is a secure, isolated environment on a computer where you can run untrusted program without risking harm to your host system.\nHow to create a sandbox Container Virtual machine Using dedicated sandbox software(like Sandboxie-Plus) Use case To be listed on the App Store or Google play store, and application must run in a sandbox. That\u0026rsquo;s why when we open a new app, the user have t approve a list of resources that enable the app to access. Education Browser plugin Pros and Cons Pros\nExtremely lightweight and fast: A sandbox applies rules to an already running process, adding minimal overhead. It\u0026rsquo;s instantaneous Targeted security: It\u0026rsquo;s perfect for its narrow purpose: running a single untrusted application and preventing it from accessing your personal files, network or hardware. Cons\nWeakest Isolation: It shares the host OS and kernel. Limited scape: It\u0026rsquo;s purely a security feature, not a deployment or development tool. ","permalink":"http://localhost:1313/posts/container_sandbox_vm/","summary":"\u003cp\u003eRecently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them.\n\u003cimg loading=\"lazy\" src=\"/Interesting_thing/distrobox.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"virtualization\"\u003eVirtualization\u003c/h2\u003e\n\u003ch4 id=\"how-virtualization-work\"\u003eHow virtualization work\u003c/h4\u003e\n\u003cp\u003eVirtualization is the technology that allows a single physical machine, known as the \u003cstrong\u003ehost\u003c/strong\u003e, to run multiple virtual machines or \u003cstrong\u003eguests\u003c/strong\u003e.\u003c/p\u003e","title":"Virtual machine vs Container vs Sandbox"},{"content":"The Global Interpreter Lock (GIL) At its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code. It is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment. The primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\nPython\u0026rsquo;s Memory Management Python uses a system called automatic memory management. Each object in Python has a reference count, which is a number that keeps track of how many variables or other object refer to it. When you assign a variable to an object, its reference count increases by one. When a reference is removed (for instance, when a variable goes out of scope), the count decreases. Once an object\u0026rsquo;s reference count drops to zero, it means nothing is using it.\nThis is where GIL becomes important. In a multi-threaded program, multiple threads could try to increase or decrease the same object\u0026rsquo;s reference count simultaneously, which may cause memory leaks.\nPython 3.13 GIL has become a bottleneck for CPU program performance, by reconstruct python, the development team has made a groundbreaking change in Python 3.13: the ability to disable the GIL.\nWhy python slow GIL Dynamic Datatype. It\u0026rsquo;s an interpreter language Easy to read makes python very abstract Trivias Generating functions are a powerful mathematical tool that transform discrete sequences into algebraic functions, enabling us to solve complex combinatorial counting problems through function operations. GPOS vs RTOS General purpose operating systems like Windows, macOS, and Linux are designed to provide a versatile, user-friendly computing environment. They prioritize overall system efficiency, with more flexible restrictions on task response times. Real time operating systems such as VxWorks, focus on strict time limitations for individual tasks, ensuring predictable and deterministic responses. These systems are typically used in environments demanding high reliability and precise time control, featuring a smaller, more streamlined kernel that guarantees real-time performance. In python, Boolean type is essentially a subclass of the integer type, True == 1 and False == 0. This design is mainly due to historical reasons and pragmatic considerations. In early version of Python(before 2.3), there was no dedicated bool type, and people used integers 1 and 0 to represent true and false. When the bool type was introuced, in order to allow old code to continue to run seamlessly, it was the best choice to continue to design it as a subclass of int. True + 1 = 2, False * 1 = 0 Huffman coding, an algorithm to compress data losslessly. Huffman coding assigns variable-length binary codes to input symbols. More frequent symbols -\u0026gt; shorter codes Less frequent symbols -\u0026gt; longer codes Windows vs Linux Aspect Linux Windows System Usage Lightweight, minimal background processes Heavy, many preloaded services and features Bloatware No pre-installed junk, user chooses all Comes with many default apps and features Transparency Fully open, config files are plain text Many hidden processes, registry-based config Customizability Highly customizable, from kernel to GUI Limited customization without hacking Privacy \u0026amp; Security User-controlled, minimal telemetry Sends telemetry, often needs antivirus Software Support Great for dev tools, less for gaming Excellent app/game compatibility System Control Full control over system and services Some restrictions, frequent auto-updates Hardware Support Good but sometimes manual setup required Plug-and-play for most consumer hardware Resouces Why the formular of normal distribution has a pi:explain from 3b1b Talks æ¼«å£«ï¼\n","permalink":"http://localhost:1313/posts/techweekly/techweek5/","summary":"\u003ch2 id=\"the-global-interpreter-lock-gil\"\u003eThe Global Interpreter Lock (GIL)\u003c/h2\u003e\n\u003cp\u003eAt its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code.\nIt is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment.\nThe primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\u003c/p\u003e","title":"Some Python Notes | King Weekly"},{"content":"Bitcoin The legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\nThe Blockchainâ›“ï¸ The blockchain is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\nIt\u0026rsquo;s a Chain of Blocks: Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block. It\u0026rsquo;s Immutable: Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending. The Genesis BlockğŸ“œ The very first block, known as the Genesis Block, was mined on 2009.01.04, by Bitcoin\u0026rsquo;s mysterious creator, Satoshi Nakamoto. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\nMiningâ›ï¸ Mining is the process of creating new blocks. It\u0026rsquo;s a competitive race that serves two critical purposes:\nValidating Transactions: Miners group pending transactions into a new block. Creating New Bitcoin: The winner of the race is rewarded with new bitcoin. Hereâ€™s how a miner wins the race and proves their block is valid:\nThe Hashing Puzzle: Miners take the data in their block and use their computers to find a specific number called a nonce. When the block data and the nonce are combined and put through a cryptographic function (SHA-256), they produce a unique digital fingerprint called a hash. The \u0026ldquo;Lower Than\u0026rdquo; Rule: To win, a miner must find a hash that is lower than the current network \u0026ldquo;target\u0026rdquo;. This target is a very large number that the entire network agrees on. Finding a hash below this target is incredibly difficult and requires immense computational powerâ€”it\u0026rsquo;s like trying to win a global lottery every 10 minutes. The Reward and The Halving: The first miner to find a valid hash wins the block reward. Initially, the reward was 50 BTC. However, this reward is programmed to cut in half roughly every four years (or 210,000 blocks) in an event called the halving. As of the April 2024 halving, the reward is now 3.125 BTC. This mechanism controls the supply of new bitcoin, making the currency scarce and ensuring its total amount will never exceed 21 million coins. When a winning block is found, its hash is broadcast across the P2P network. All other participants quickly verify that the hash is valid. Once confirmed, they add the new block to their copy of the blockchain and immediately start competing to solve the next block.\nHow to Mine: There are a few ways to participate in Bitcoin mining, each with its own pros and cons.\nSolo Mining: This is you, on your own, trying to solve a block. If you succeed, you get the entire block reward (3.125 BTC + transaction fees). However, the odds of a single person solving a block today are astronomically low due to the immense competition. It\u0026rsquo;s like buying a single lottery ticket and hoping to win the grand prize. Mining Pool: This is the most common method. You join a \u0026ldquo;pool\u0026rdquo; with thousands of other miners, combining your computing power. The pool works together to find blocks much more frequently. When the pool wins, the reward is split among all participants based on how much computing power they contributed. This provides smaller, but much more consistent and predictable, payouts. Block ForksğŸ´ A fork happens when the blockchain temporarily or permanently splits into two different paths.\nAccidental Fork: Sometimes, two miners find a valid block at almost the exact same time. The network briefly splits as some nodes follow one miner and some follow the other. This is usually resolved within a few minutes when the next block is found and added to one of the chains, making it the \u0026ldquo;longest\u0026rdquo; and therefore the official one. The shorter chain is then abandoned. Hard Fork: This is an intentional split that happens when the network\u0026rsquo;s software rules are changed in a way that is not backward-compatible. All participants must upgrade to the new rules to continue. If a significant portion of the community refuses to upgrade, the split becomes permanent, resulting in the creation of a new, separate cryptocurrency (e.g., Bitcoin Cash was created from a hard fork of Bitcoin). Ethics Mining bitcoin always consume immense energy, which critics view as a wasteful environmental cost for a seemingly meaningless computation. Proponents argue this mechanism decentralized financial system that offers freedom from the control of banks and governments.\nTools Calculate computing performance: https://www.nicehash.com/profitability-calculator\nThis is the computating power of my personal game laptop(one dollar per day hhh) Resources: Youtube channels\nhttps://www.youtube.com/watch?v=5hgdekVZb3A\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=3 https://www.youtube.com/watch?v=a41DMDfJjsU\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=2 https://Gemini.google.com Trivias Sherrington, coined the word \u0026ldquo;synapse\u0026rdquo; to define the connection between two neurons Two different roads of AI: connectionism and Symbolism. Hidden layer was firstly been implemented in Boltzmann machine, although Rosenblatt had some idea about multilayer perceptrons, but he didn\u0026rsquo;t find any useful training algorithm. Restricted Boltzmann machine - each layer is only allowed to be fully connectted to the next layer, current layer nodes are not connectted to each other. The advantages of this machine is it allows to update bodes within the same layer in parallel The invention of hidden layer allows model to understand abstract features. It also becomes to one of the most significant component in deep learning. The main difference between the Hopfield Network and the Boltzmann nachine is the presence of hidden layers. Other differences include the fact that the Hopfield network is deterministic, whereas the Boltzmann machine is stochastic, and the defintions of their energy function also differ. The fovea has many photoreceptors, with a high density of cones(for colors) and nearly no rods(for dark). This structure allows us to see the world clearly. If you develop myopia, the image formed after light is reflected by the eye may not be focused directly on the fovea. The idea of CNN was mainly inspired by the HMAX model(hierarchical, pooling, convolution), and the HMAX model was proposed by Tomaso Poggio, to simulate primate visual system, specifically ventral stream. Pytorch for research area, Tensorflow for industry and JAX for high level usage. Ethereum and ether is not the same as bitcoin. Ethereum has a longer vision, and the number of ether is unlimited. Resource Documentary of AlphaGo: https://www.youtube.com/watch?v=WXuK6gekU1Y How to use hugging face: https://www.youtube.com/watch?v=3kRB2TXewus This is the most comprehensive guide for AI beginner I had ever seen: Guide link ","permalink":"http://localhost:1313/posts/techweekly/techweek4/","summary":"\u003ch2 id=\"bitcoin\"\u003eBitcoin\u003c/h2\u003e\n\u003cp\u003eThe legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-blockchain\"\u003eThe Blockchainâ›“ï¸\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eblockchain\u003c/strong\u003e is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s a Chain of Blocks:\u003c/strong\u003e Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s Immutable:\u003c/strong\u003e Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-genesis-block\"\u003eThe Genesis BlockğŸ“œ\u003c/h4\u003e\n\u003cp\u003eThe very first block, known as the \u003cstrong\u003eGenesis Block\u003c/strong\u003e, was mined on \u003cstrong\u003e2009.01.04\u003c/strong\u003e, by Bitcoin\u0026rsquo;s mysterious creator, \u003cstrong\u003eSatoshi Nakamoto\u003c/strong\u003e. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\u003c/p\u003e","title":"Bitcoin Basic | King Weekly"},{"content":"2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning. However, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\nRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually. And before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation. So today I will record this moment.\nHopfield Network was invented at 1982. Processor John Hopfield designed it based on ideas from statistical mechanics.\nThe graph above shows two states of a ball From the left part, we can see the energy system of the ball is at the highest, which means the ball is very unstable From the right part, we can see the ball had already fall into the bottom, which means the ball is very stable. Although it is a classical physics model, but it definitly explain the main idea of hopfield network.\nSo now we can say hopfield network is just make a system move from an unstable state to stable state.\nIf you interested how exactly hopfield network work. See the video below, it\u0026rsquo;s pretty nice.\nThis idea is also useful in today. In LLM, we can say the user\u0026rsquo;s prompt and question is the most unstable states while the answer is the stable state.\nBased on the concept of Hopfield networks, many different architectures had been invented, which makes Connectionism and Deep Learning great again.\nAfter all, I catch the reason why nobel prize gives to physics.\n","permalink":"http://localhost:1313/posts/hopfieldnetwork/","summary":"\u003cp\u003e2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning.\nHowever, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\u003c/p\u003e\n\u003cp\u003eRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually.\nAnd before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation.\nSo today I will record this moment.\u003c/p\u003e","title":"Hopfield Network"},{"content":"Book - Unix: A History and a Memoir Recently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world. During the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\nAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals. The system developed before Unix was called Multics. Since â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€ Unix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10. Tools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs. Unix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems. GNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source. MacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX. In the early days, operating systems were not portable. This changed with the invention of the C language and its compiler. MINIX was widely used because it was embedded in Intel chips. The working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation. Microsoft once had its 3own Unix-like system. Another completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows. You can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan. â€œEverything is a fileâ€ is one of the core principles of Unix. The KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy. The UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\nKeep it simple stupid Do one thing, and do it well Everything is a file Make each program a filter Fail loudly Modularity Prototyping early In today, many barriers had already been removed.\nAnd I realise.\nThe revolution of AI is just like the reenactment of Unix\u0026rsquo;s development.\nSo.\nKISS.\nTrivias tty - TeleTYpewriter, terminal in the old time, before lcd screen been invented. UNIX was developed on the PDP-7, a computer with no screen, no mouse and only 8KB of RAM. It weighted nearly 500kg. UNIX and UNIX-like system use abbreviated commands because typing on TTY terminals in the 1960s was slow and insufficient. Second-system effect: It believes that after completing a small, elegant, and successful system, people tend to have overly high expectations for the next projects, which may lead to the creation of a huge, feature-rich but monstrous system.The \u0026ldquo;second-system effect\u0026rdquo; can result in software project plans being overdesigned, with too many variables and excessive complexity, ultimately falling short of expectations and leading to failure. such as PL/I in Multics Fortran(formular translation): The purpose of this lagnauge is to proceed mathematics formular and float number in an efficient way, like integration, linear algebra. That\u0026rsquo;s why fortran is still popular in some supercomputer and scientific calculation. B lagnauge is designed in a bit-unit computer PCP-7, where C langauge is designed in a byte-unit computer. Therefore the main difference between B and C is B langauge doesn\u0026rsquo;t support types and C does. Development of Clang: PL/I -\u0026gt; BCPL -\u0026gt; B -\u0026gt; New B(C) If computers using the same cpu architecture, they will using the same assembly language. The grep command is used to find lines that match a specific pattern in a file while the sed command is used to insert, replace and delete text from a file. Finally, the awk command supports programming logic and is often used for advanced data processing tasks. The development of UNIX from 1970 until now. Talks \u0026ldquo;A new technological discovery is often discredited by older generations of professionals - especially those with high authority and prestige in the existing field - in order to protect their own status\u0026rdquo; ","permalink":"http://localhost:1313/posts/techweekly/techweek3/","summary":"\u003ch2 id=\"book---unix-a-history-and-a-memoir\"\u003eBook - Unix: A History and a Memoir\u003c/h2\u003e\n\u003cp\u003eRecently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world.\nDuring the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals.\u003c/li\u003e\n\u003cli\u003eThe system developed before Unix was called Multics.\u003c/li\u003e\n\u003cli\u003eSince â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€\u003c/li\u003e\n\u003cli\u003eUnix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10.\u003c/li\u003e\n\u003cli\u003eTools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs.\u003c/li\u003e\n\u003cli\u003eUnix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems.\u003c/li\u003e\n\u003cli\u003eGNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source.\u003c/li\u003e\n\u003cli\u003eMacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX.\u003c/li\u003e\n\u003cli\u003eIn the early days, operating systems were not portable. This changed with the invention of the C language and its compiler.\u003c/li\u003e\n\u003cli\u003eMINIX was widely used because it was embedded in Intel chips.\u003c/li\u003e\n\u003cli\u003eThe working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation.\u003c/li\u003e\n\u003cli\u003eMicrosoft once had its 3own Unix-like system.\u003c/li\u003e\n\u003cli\u003eAnother completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows.\u003c/li\u003e\n\u003cli\u003eYou can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan.\u003c/li\u003e\n\u003cli\u003eâ€œEverything is a fileâ€ is one of the core principles of Unix.\u003c/li\u003e\n\u003cli\u003eThe KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\u003c/p\u003e","title":"For UNIX Week | King Weekly"},{"content":"Model Context Protocol overview MCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\nWhy is MCP needed? In the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces). The current components of MCP servers include: How does MCP work? Example: Suppose you\u0026rsquo;re using an AI assistant to manage your work, and it wants to help you schedule today\u0026rsquo;s meeting:\nWithout MCP, developers would need to write separate integration code for Outlook, Google Calendar, and Apple Calendar.\nWith MCP, the AI only needs to call the MCP server, which will automatically interface with your calendar system. No matter which calendar service you use, the AI can work seamlessly.\nCore Functions of MCP: Reducing development costs (no need to develop separate integrations for each tool).\nEnhancing AI\u0026rsquo;s ability to access external data (allowing AI to easily query and manipulate external data).\nStandardizing communication (making communication between different AI applications and tools smoother).\nYou can think of MCP as the \u0026ldquo;USB interface for AI\u0026rdquo;â€”any AI device can plug into different tools without needing to individually adapt to each one!\nTrivias: Graph is a great data structure, it can be used to find the shortest path, solve a magic cube. Rely on this data structure, Human find the shortest steps to solve the worst case configured 3\\*3\\*3 magic cube in 20 steps (which also called god\u0026rsquo;s number) and 11 steps for 2\\*2\\*2 one. What about n*n*n, Here is an interesing paper about time compleixity of solving a n*n*n magic cube: Algorithms for Solving Rubikâ€™s Cubes Topological sort is an algorithm based on DFS and DAG, It\u0026rsquo;s not a traditional sorting algorithm, like comparing the size of each number and sort them, but sorting based on dependencies. Dynamic programming is just like recursion + memorization + guessing P vs NP problems: P is a problem that can be solved easily by a computer, NP is a problem that you can check for correctness very easily once solved. However, P != NP for example wec can\u0026rsquo;t engineer luck. Also, NP hard means it is at least as hard as any problem in NP, and NP-complete is lke if you can solve one NP-complete question, you can solve all NP question. Reduction is like to prove a known NP-complete question, and transfer it into a NP question X, then X is also a NP-complete question HTTP vs REST API HTTP is a protocol, and its core task is to define how to request and transfer data between your broswer and server\nIt focuses on the data exchange level, regardless of whether the data is an image, text, video, or API response. For example, you can use an HTTP request to access a webpage (HTML page) or use an HTTP request to retrieve API data (JSON data). It doesnâ€™t care about what data you\u0026rsquo;re transmitting. REST API is a design style, based on the HTTP protocol, with rules and best practices:\nIt views content on the server as \u0026ldquo;resources\u0026rdquo; and specifies how to operate on them using HTTP methods (GET, POST, PUT, DELETE). REST API design considers resource orientation: Resources (like users, articles, comments, etc.) have unique identifiers (URIs), and clients interact with these resources through HTTP requests. For example, in a REST API, you can:\nGET request: Retrieve a resource (e.g., get movie information). POST request: Create a new resource (e.g., submit a new comment). PUT request: Update a resource (e.g., modify user information). DELETE request: Delete a resource (e.g., remove an article). Resource: A tool to build a personal streaming music player: navidrome A great blog that introduce human visualization: human visualization Five basic algorithms explanation: Dynamic programming Greedy Algorithm Backtracking Branch and Bounding This week, Nvidia GTC 2025 brings a lot of new tech, PC in AI age, B300, new architecture, robots. After watched the coverage keynotes, I was just feel like our mankind is in an special stage with unpredictable pace. In science fiction films, at this point , it often leads to the arrival of an alien civilization. A dijkstra algorithm visualiser that helps me understand it: Dijkstra shortest path MCP explain: A blog that explain MCP crystal clear ","permalink":"http://localhost:1313/posts/techweekly/techweek2/","summary":"\u003ch2 id=\"model-context-protocol-overview\"\u003eModel Context Protocol overview\u003c/h2\u003e\n\u003cp\u003eMCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\u003c/p\u003e\n\u003ch4 id=\"why-is-mcp-needed\"\u003eWhy is MCP needed?\u003c/h4\u003e\n\u003cp\u003eIn the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\u003cbr\u003e\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces).\n\u003cimg alt=\"components mcp include\" loading=\"lazy\" src=\"/TechStuff/mcp.png\"\u003e\u003c/p\u003e","title":"Jarvis Will Coming Soon | King Weekly"},{"content":"Email sending and receiving system The main system is build based on three protocols: SMTP, POP3 and IMAP. SMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails. User1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\nPOP3 downloads emails from the email server to the email client. By default, it removes emails from the server after downloading, but some email clients allow users to keep copies on the server. IMAP keeps emails on the server and synchronizes them across multiple devices. The email client initially loads only the headers, and the full email content is fetched from the server when the user opens it.\nIf you want to customize an email domain. You need to have your own SMTP and IMAP/POP3 server and a domain, the other steps are the same as above.\nPKGBUILD in Arch Linux PKGBUILD is a bash script contain the build information required by archlinux package we use makepkg script to build the package, it will search PKGBUILD first in the current folder. Benefits:\nusing pacman to manage, user can update and uninstall easily some pkgbuild file include the commands to generate a binary file and store it in /user/bin Drawbacks:\nNot friendly to starter Although we can use yay to help us do all these stuff.\nTrivias Newton\u0026rsquo;s method is quadratic convergence when we want to calculate the root of a number Catalan number is a group of sequence that appear widely in combinatorics. e.g. ways to arrange n brackets, number of triangles in an n+2 convex polygon. The common property of these applications is that they are recursive and have a constrained structure. Toom cook: It divide a d-digit number into n parts and doing arithmatic calculations. Schonhage-strassen scheme: It multiplies two integers of length ğ‘› in O (ğ‘› logğ‘› log logğ‘›) steps on a multitape Turing machine A Naive algorithm is usually the most obvious solution when one is asked a problem. It may not be a smart algorithm but will probably get the job done The taste of red wine is determined by acidity, sweetness, alcohol content, tannins, and body. Wines are categorized into New World and Old World. New World wines (from countries like the USA, Chile, Argentina, and China) are named after the grape variety, while Old World wines (mainly from Europe) are named after their place of origin.\nRed wine is made by fermenting red grapes with their skins. White wine is made from either white grapes or red grapes without their skins. RosÃ© wine is made by soaking the grape skins briefly but fermenting without them. Sparkling wine undergoes a second fermentation to produce bubbles. Gabriel\u0026rsquo;s horn is a type of geometric figure that has infinite surface area but finite volume. Resources Code question(leetcode), system design question(crack the code interview), teamwork, communication are all important in the interview. An old guideline to learn ai: https://www.captainai.net/itcoke/ A guideline to learn CS: https://csdiy.wiki/åè®°/ Useful tips to integrate by parts, åå¯¹å¹‚æŒ‡ä¸‰, to choose u. Customize your zsh: oh my zsh Xiaomi releases a concept modular camera, it looks pretty awesome and innovative. 3b1b\u0026rsquo;s taylor series explaination:https://www.youtube.com/watch?v=3d6DsjIBzJ4 3b1b\u0026rsquo;s explaination of why we have exponential e:https://www.youtube.com/watch?v=m2MIpDrF7Es This website is all about competitive writing of source code that is as short as possible: Codewolf Explaination of greedy algorithm: greedy algorithm Explaination of dynamic programming: dynamic programming Deploy perosonal VPN tools: tailscale Abstract If no one is reading blogs anymore, why should we write them? Letâ€™s make it simple: you write a blog, but nobody cares, nobody reads it. At least, the number of readers is not as many as you thought. You put your personal ideas and thoughts into the article, carefully structuring each sentence, and choose a great imageâ€”then, no response, no likes, no shares, no activity. So, what is the meaning of writing a blog? First, there are two misconceptions about blogging. One is that if I write a good article, readers will come naturally. No, they wonâ€™t come. There are billions of blogs on the internet, like a massive hurricane, and yours is just a single leaf in the wind. Who would notice? Another misconception is that if nobody reads it, writing is a waste of time. Blogs have their own hidden value. You write blogs not for the applause of others, but for your own needs. Blogs help clear your mind. They help you organize your thoughts and sharpen your perspective. When you think better, you will achieve better results. The target audience of a blog is actually not the people on the internet, but your future self. Your article will help you see the evolution of your own thoughts. Additionally, one day in the future, someone who truly needs your article will find it. A deep, thoughtful article has a longer-lasting impact than a viral article. Writing a blog is quite like street photography. You take your camera and walk through the city. You see a sceneâ€”a moment filled with light, shadow, and humanityâ€”and then you capture it. Nobody cares about what you actually captured. But thatâ€™s not the reason you photograph; you photograph because you see something interesting. Writing a blog is the same. You write a blog because you are thinking, observing new things, and hope to store them somewhere. If someone reads it, that\u0026rsquo;s great. If not, youâ€™ve still completed your work\n","permalink":"http://localhost:1313/posts/techweekly/techweek1/","summary":"\u003ch2 id=\"email-sending-and-receiving-system\"\u003eEmail sending and receiving system\u003c/h2\u003e\n\u003cp\u003eThe main system is build based on three protocols: SMTP, POP3 and IMAP.\n\u003cimg alt=\"process of email system\" loading=\"lazy\" src=\"/emailsys.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails.\nUser1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\u003c/p\u003e","title":"Start | King Weekly"},{"content":"In this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O O(n) notation before, but for Big theta Î¸(n) and reccurence relations T(n), I never heard them before. Today, I hope I can finally figure them out.\nWhat T(n) represents the actual running time of an algorithm\nO(n) represents the asymptotic upper bound of the running time of an algorithm\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\nHow to convert the three of them Normally we can directly transfer T(n) to O(n) or Î¸(n). Int sum = 0 for (i = 1; i \u0026lt;= n, i ++) { sum = sum + i } This is a classical example, First of all, we initialize variable sum requires one unit of running time. There are three statements inside the for loop, statement 1 i = 1 requires one unit of the running time, statement 2 i \u0026lt;= n requires n+1 units of the running time, statement 3 i ++ requires n units of the runningn time, and sum = sum + i requires 2n units of the running time, n for addition and n for assignment. Therefore T(n) = 1+1+(n+1)+n+2n = 4n + 3. In O(n), we ignore the constant and the lower-order terms, therefore the time complexity is O(n) / Î¸(n).\nWhen the algorithm is a recursion, such as karatsuba multiplication and high precision multiplication. There are two methods to convert T(n) into O(n)\nRecursion tree method According to Recursion tree method, we derive master theorem The time complexity of multiplication is equal to the time complexity of division\n","permalink":"http://localhost:1313/posts/the-difference-between-tn--on-and-%CE%B8n/","summary":"\u003cp\u003eIn this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O \u003ccode\u003eO(n)\u003c/code\u003e notation before, but for Big theta \u003ccode\u003eÎ¸(n)\u003c/code\u003e and reccurence relations \u003ccode\u003eT(n)\u003c/code\u003e, I never heard them before. Today, I hope I can finally figure them out.\u003c/p\u003e\n\u003ch1 id=\"what\"\u003eWhat\u003c/h1\u003e\n\u003cp\u003eT(n) represents the actual running time of an algorithm\u003cbr\u003e\nO(n) represents the asymptotic upper bound of the running time of an algorithm\u003cbr\u003e\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\u003c/p\u003e","title":"Time Complexity Notations"},{"content":"This week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\nOverview The machine model is HP-Elitedesk-800-G4-SFF. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\nIf you want to learn more here is the machine datasheet:server_datasheet\nHardware Motherboard: Q370 viewer CPU: i5-8500 GPU: intel UHD 630 RAM: 16G SSD: 256G HDD: 500G * 1 The motherboard provides a high flexibility to expand more internal storage, also it has 4 PCie expansion slots which can used to expand more storage space or other components you want.\nSoftware nextcloud\nemail domain\ngitlab\nminecraft server\ndocker\njellyin\nsynthing\nproxy?router?gateway?\nvirtual machine\nI host my server with ubuntu server distro. The reason I didn\u0026rsquo;t choose proxmox is because I want to learn server step by step, proxmox is great in visualization, maybe in the future, I will try it.\nDurign the process of configuring storage, I learned LVM, which is a wonderful tool for those users that has multiple drives. User can create a storage pool called volume group. Firstly, user add their physical volumes into volumn group, and we create logical volumn based on the storage area had in volumn groups, and then we mount those LVs with the actural dirctory. It seems like Windows is not able to achieve this function. For RAID, we seperate it into 4 different categories, radi0, raid1, raid5 and raid10. This tools shows how to save files in different number of drives or in LVM.\nFor external access, I plan to use cloudflare tunnel. They provide such service, I need to buy a domain name and combine it with the cloudflare tunnel, and when I access the server, I firstly type the domain name in my browser to ask cloudflare, and they will guide me to the tunnel to my server, also in server end, I need to install cloudflared docker image as an end, then it works! It\u0026rsquo;s so convenient for those people who live in school accommodation. And it\u0026rsquo;s totally free!\nIn my network configuration. I didn\u0026rsquo;t install a router, but the best choice is to use a router for all devices in my home, and assign each of them an static IP address. To access the server, I bought a portable monitor, since the IP address of my server using DHCP, which required to check the IP address manually when the machine reboot or close. This issue will be solved easily when I have a router. Nowadays, I need to change the tunnels configuration to enable external access.\nsince I have 3 different operating systems in my three daily devices, phone for android, laptop for arachlinux and ipad for ipados, I installed nextcloud docker to try to integrate them into one ecosystem, that pretty awesome.\nUpdate Nextcloud for cloud storage Able to build game server Music and video live streaming server. Future Mining bitcoin ","permalink":"http://localhost:1313/posts/homeserver/","summary":"\u003cp\u003eThis week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThe machine model is \u003ccode\u003eHP-Elitedesk-800-G4-SFF\u003c/code\u003e. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\u003cbr\u003e\n\u003cimg loading=\"lazy\" src=\"/TechStuff/serverbill.png\"\u003e\u003c/p\u003e","title":"My Homelab"},{"content":"This semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\nIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\nHow the JVM works The JVM executes er programs in several stages:\nCompilation: Java source code (.java) is compiled into bytecode (.class) by the Java Compiler (javac).\nClass Loading: The JVM loads compiled bytecode when required.\nBytecode Verification: Ensures security and correctness before execution.\nExecution: The JVM interprets or compiles bytecode using Just-In-Time (JIT) compilation.\nGarbage Collection: The JVM automatically manages memory, reclaiming unused objects.\nClass loader One of the organizational units of JVM byte code is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.\nLet\u0026rsquo;s explore this concept with an example: Imagine you\u0026rsquo;re watching a movie on a streaming service.\nLoading:\nThe service first finds and imports the movie data you want to watch. Similarly, the class loader locates the binary data for a Java class. Linking: Verification: Before you watch the movie, the service checks that the file isn\u0026rsquo;t corrupted. In Java, the class loader verifies the correctness of the class. Preparation: The service sets up the necessary space in memory to buffer the movie. Java allocates memory for variables and sets default values. Resolution: The service ensures all necessary subtitles or audio tracks are ready to play. Java resolves references to make them direct. Initialization:\nAs you start watching, the service begins playing the movie. Similarly, Java runs code to set up variables with their starting values. Class Loader Types:\nBootstrap Class Loader: Like the service\u0026rsquo;s core library of well-known movies, it loads fundamental, trusted classes. Extension Class Loader: Similar to special add-on features, it loads additional classes outside the core library. System/Application Class Loader: Like searching for new releases or user-uploaded content, it loads classes specific to the application youâ€™re using. Virtual machine architecture Cross-Platform Compatibility and Limitations The JVM abstracts away the underlying hardware and operating system specifics, allowing Java bytecode to run on any device equipped with a compatible JVM. This cross-platform capability greatly simplified software distribution and development, as developers could write code once and deploy it across various platforms without modification.\nDespite its strong cross-platform capabilities, the JVM faced significant challenges due to competitive corporate strategies, particularly the \u0026ldquo;Embrace, Extend, and Extinguish\u0026rdquo; (EEE) approach adopted by some companies like Microsoft in the late 1990s. This strategy involved embracing a technology, extending it with proprietary features, and eventually using those extensions to undermine the original technology.\nMicrosoft initially embraced Java by integrating it into their Internet Explorer browser and Windows platforms. However, they extended Java with proprietary features that were specific to Windows, creating a version of Java that was incompatible with the standard JVM specifications set by Sun Microsystems. This move fragmented the Java platform and undermined the \u0026ldquo;Write Once, Run Anywhere\u0026rdquo; philosophy.\nWith the development and rise of programming languages like Swift, Kotlin, and JavaScript, the JVM faced significant challenges in maintaining its performance edge. Swift, designed by Apple for iOS and macOS platforms, offers high performance and safety due to its compiled nature and modern language features. Kotlin, although initially running on the JVM, introduced concise syntax and advanced features that surpassed Java in many ways, leading it to become the preferred language for Android development. JavaScript\u0026rsquo;s performance greatly improved with engines like V8, and its versatility expanded through technologies like Node.js for server-side development. These languages not only matched but often exceeded the JVM\u0026rsquo;s performance and adaptability in their respective domains, leading to a shift in developer preferences and a relative decline in the JVM\u0026rsquo;s dominance.\nOverall Nowadays, Java has become to a normal programming lauguage. And the question is obvious solved. Python has its own interpreter to transfer the source code into machine code. Haskell has its own compiler, C++/C are compiled directly into machine code. However,they can\u0026rsquo;t generate a compiled file that enable to run in every operating system. If there is no EEE strategy, Linux may have a stronger effects in today\u0026rsquo;s world.\n","permalink":"http://localhost:1313/posts/jvm/","summary":"\u003cp\u003eThis semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\u003c/p\u003e","title":"JVM's Achievements and Limitations"},{"content":" ğŸ¤— ","permalink":"http://localhost:1313/aboutme/","summary":"aboutme","title":"About me"},{"content":"Location: Auchinstarry Quarry, Scotland Date: 2025.09.12 A great first-time rock climbing experience, meet interesting people and learned some rope skills.\n","permalink":"http://localhost:1313/aboutme/climbing/","summary":"\u003ch2 id=\"location-auchinstarry-quarry-scotland\"\u003eLocation: Auchinstarry Quarry, Scotland\u003c/h2\u003e\n\u003cp\u003eDate: 2025.09.12\n\u003cimg alt=\"rock and me\" loading=\"lazy\" src=\"/Aboutme/croy_rock_and_me.jpg\"\u003e\nA great first-time rock climbing experience, meet interesting people and learned some rope skills.\u003c/p\u003e","title":"Climbing road"},{"content":" A free web application that create a two-host daily podcast of the news that matters to you. Just add your favorite RSS feeds from news sites, blogs, and social media. Our app will automatically generate conversational scripts using an LLM and then produce a customized audio episode with text-to-speech. It\u0026rsquo;s the perfect way to stay updated on your commute.\nOfficial website\nCode available in github\nMotivation The idea of having a J.A.R.V.I.S. from the Marvel movies, is a common dream. With today\u0026rsquo;s advancements in LLM and TTS technology, we can now offer a real-world experience that captures a part of that dream.\nFor me, I love to start my mornings by checking my favorite blogs for updates, scrolling through X for new tech news, or quickly scanning the day\u0026rsquo;s headlines. With this application, I could simply wake up, click a button to generate my podcast, and then, while having breakfast or commuting, listen to a personalized summary of everything that happened overnight. If a specific story catches my interest, I can then go back and read the full article later. I think its an efficient way to stay informed.\nTech stack Area Technology Backend FastAPI, Python, SQLAlchemy, PostgreSQL, Pydantic Frontend React, Javascript, TypeScript, Vite, Tailwind CSS AI \u0026amp; TTS Google Gemini 2.5 Pro API Deployment Self-host(personal server), Docker Compose(Backend, Frontend, background worker, PostgreSQL, Valkey) AI assistant Gemini 2.5 pro, Claude sonnet 4 Issues I met Gemini api timeout: The current Gemini API has a 60-second request timeout, which is insufficient for generating a 10-minute podcast with a single text-to-speech (TTS) request. To solve this, my initial thought was to segment the entire podcast script into smaller parts and generate each segment separately. However, a new challenge arose with the TTS service\u0026rsquo;s free plan, which limits me to just 15 requests per day. This makes the segmentation approach impractical for daily use. My temporary fix has been to adjust the script prompt to generate shorter content, but this method is unreliable and doesn\u0026rsquo;t always prevent timeouts. The long-term solution will be to upgrade to a paid TTS service with a longer API request timeout. This will allow for single, uninterrupted podcast generation, solving the issue at its root. Roadmap 2025-09: Migrate deployment from Render to my personal server âœ… - 2025.09.21 2025-10: Add more social medias RSS feeds 2025-12: Multi-format input file support Future: Use fine-tuning TTS model Improve user experience Support spotify export Summary This project takes me 2 month from learning tech stacks to final outcome. I want to say AI really helped me a lot during the building process, whereas from learning tech stacks to fix bugs. From this prject, I experience both frontend and backend project building, frontend is just like building an artwprk, it really need me to keep patient, backend is more interesting since it relative to many different fields like crud operations, building database schemas and endpoints and connection between database, background worker. I also experience the magic of containerization, its so convenient. - 2025.08.14\n","permalink":"http://localhost:1313/aboutme/projects/lets_break_the_information_gap/","summary":"aboutme","title":"Let's break the information gap"},{"content":"ğŸ“· Welcome to my photography exhibition! ","permalink":"http://localhost:1313/aboutme/gallery/","summary":"\u003ch1 id=\"heading\"\u003eğŸ“·\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"welcome-to-my-photography-exhibition\"\u003eWelcome to my photography exhibition!\u003c/h1\u003e","title":"Photography exhibition by King Jin"},{"content":" Let\u0026rsquo;s break the information gap A free web application that create a two-host daily podcast of the news that matters to you. Reflex An AI-powered, English translator that generates a unique conversational story each day based on your search history. ","permalink":"http://localhost:1313/aboutme/projects/","summary":"\u003cpicture class=\"center\"\u003e\n  \u003c!-- Dark mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects_dark.png\" media=\"(prefers-color-scheme: dark)\"\u003e\n  \u003c!-- Light mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects.png\" media=\"(prefers-color-scheme: light)\"\u003e\n  \u003c!-- Fallback image --\u003e\n  \u003cimg src=\"/Aboutme/projects.png\" alt=\"Project image\"\u003e\n\u003c/picture\u003e\n\u003cbr\u003e\n\u003cbr\u003e\n\u003c!-- Project lbtig --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003eLet\u0026rsquo;s break the information gap\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003e\u003cimg src=\"/Aboutme/lbtig_preview.png\" class=\"center\"\u003e\u003c/a\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp class=\"center-wider\"\u003e A free web application that create a two-host daily podcast of the news that matters to you.\n\u003c/p\u003e\n\u003chr\u003e\n\u003cbr\u003e\n\u003c!-- Project reflex --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003eReflex\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003e\u003cimg src=\"/Aboutme/reflex_preview.png\" class=\"center\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cbr\u003e\n\u003cp class=\"center-wider\"\u003e An AI-powered, English translator that generates a unique conversational story each day based on your search history.\n\u003c/p\u003e","title":"Project by King Jin"},{"content":" An AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\ncode available in github Motivation I dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\nTech stack Frontend: HTML, Tailwind CSS, Electron, Javascript\nRoadmap Future: support multi-language data record system\nSummary This project takes me one week to build. To be honest, Electron is convenient for cross platform, but also it increase the size of the software.\n","permalink":"http://localhost:1313/aboutme/projects/reflex/","summary":"\u003cblockquote\u003e\n\u003cp\u003eAn AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Kingjinsight/Reflex\"\u003ecode available in github\u003c/a\u003e\n\u003cimg alt=\"reflex_dashboard\" loading=\"lazy\" src=\"/Aboutme/reflex_dashboard.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\u003c/p\u003e","title":"Reflex"},{"content":"I really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\nColor Theory. Additive Color: RGB for Red, Green and Blue. Red + Green + Blue = White Subtractive Color: These are created by mixing two additive colors. Red + Green = Yellow Green + Blue = Cyan Red + Blue = Magenta Yellow + Cyan + Magenta = Black Basics Histogram: a graph that displays the brightness and color distribution of an image From left to right, each column represents the number of the white-black pixels in different level It has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites. There are two triangles at the top left and top right. the top left one is shadow clipping which will show you whether your image has region that was too dark the top right one is highlight clipping which show you whether your image has region that was too light the reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0. There are several colors in the triangle gray - all details are well preserved red - red channel overexposure/underexposure green - green channel overexposure/underexposure blue - blue channel overexposure/underexposure yellow - yellow channel overexposure/underexposure cyan - cyan channel overexposure/underexposure magenta - magenta channel overexposure/underexposure solid white - RGB channel channel overexposure/underexposure in the same time White balance Temperature Tint Tone Exposure: change the overall brightness of the photo Contrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa. Highlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc. Shadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc. Whites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white. Blacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black. Presence Texture: Focus on the surface texture, such as skin pores, surface of rocks, etc. Clarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;. Dehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation. Vibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated. Saturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree. Tone Curve The square graph looks similar with histogram, but contain a line segment from bottom left to top right. We can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve. we can also pull RGB channel separately. S curve to increase contrast Tips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more. This post will update frequently.\n","permalink":"http://localhost:1313/posts/lightroom/","summary":"\u003cp\u003eI really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\u003c/p\u003e\n\u003ch2 id=\"color-theory\"\u003eColor Theory.\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAdditive Color: RGB for Red, Green and Blue.\n\u003cul\u003e\n\u003cli\u003eRed + Green + Blue = White\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSubtractive Color: These are created by mixing two additive colors.\n\u003cul\u003e\n\u003cli\u003eRed + Green = Yellow\u003c/li\u003e\n\u003cli\u003eGreen + Blue = Cyan\u003c/li\u003e\n\u003cli\u003eRed + Blue = Magenta\u003c/li\u003e\n\u003cli\u003eYellow + Cyan + Magenta = Black\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"basics\"\u003eBasics\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHistogram: a graph that displays the brightness and color distribution of an image\n\u003col\u003e\n\u003cli\u003eFrom left to right, each column represents the number of the white-black pixels in different level\u003c/li\u003e\n\u003cli\u003eIt has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites.\u003c/li\u003e\n\u003cli\u003eThere are two triangles at the top left and top right.\n\u003col\u003e\n\u003cli\u003ethe top left one is shadow clipping which will show you whether your image has region that was too dark\u003c/li\u003e\n\u003cli\u003ethe top right one is highlight clipping which show you whether your image has region that was too light\u003c/li\u003e\n\u003cli\u003ethe reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eThere are several colors in the triangle\n\u003col\u003e\n\u003cli\u003egray - all details are well preserved\u003c/li\u003e\n\u003cli\u003ered - red channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003egreen - green channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eblue - blue channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eyellow - yellow channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003ecyan - cyan channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003emagenta - magenta channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003esolid white - RGB channel channel overexposure/underexposure in the same time\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eWhite balance\n\u003col\u003e\n\u003cli\u003eTemperature\u003c/li\u003e\n\u003cli\u003eTint\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eTone\n\u003col\u003e\n\u003cli\u003eExposure: change the overall brightness of the photo\u003c/li\u003e\n\u003cli\u003eContrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa.\u003c/li\u003e\n\u003cli\u003eHighlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc.\u003c/li\u003e\n\u003cli\u003eShadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc.\u003c/li\u003e\n\u003cli\u003eWhites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white.\u003c/li\u003e\n\u003cli\u003eBlacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePresence\n\u003col\u003e\n\u003cli\u003eTexture: Focus on the surface texture, such as skin pores, surface of rocks, etc.\u003c/li\u003e\n\u003cli\u003eClarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eDehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation.\u003c/li\u003e\n\u003cli\u003eVibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated.\u003c/li\u003e\n\u003cli\u003eSaturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"tone-curve\"\u003eTone Curve\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThe square graph looks similar with histogram, but contain a line segment from bottom left to top right.\u003c/li\u003e\n\u003cli\u003eWe can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve.\u003c/li\u003e\n\u003cli\u003ewe can also pull RGB channel separately.\u003c/li\u003e\n\u003cli\u003eS curve to increase contrast\u003c/li\u003e\n\u003cli\u003eTips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis post will update frequently.\u003c/p\u003e","title":"Lightroom Notes"},{"content":"Recently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them. Virtualization How virtualization work Virtualization is the technology that allows a single physical machine, known as the host, to run multiple virtual machines or guests.\nThe key of virtualization is hypervisor. It\u0026rsquo;s the software that create and manages the virtual machines.\nType 1 (Bare-Metal): This hypervisor is installed directly onto the host\u0026rsquo;s hardware, acting as the operating system itself. Examples include VMware vSphere, Hyper-V. This type is common in data centers due to its high performance and efficiency Type 2 (Hosted): This hypervisor runs as an application on top of a conventional operating system(Like Windows, Linux). Examples include VMware Workstaion and Oracle VirtualBox. This approach is often used for desktop virtualization and development purposes. Once the hypervisor is in place, you can create one or more VMs. This involves\nAllocating resources(CPU cores, RAM, storage) Configuring virtual hardware(virtual network adapter, virtual storage controllers, virtual BIOS) Installing a guest OS, the guest os is unaware that it\u0026rsquo;s running in a virtualized environment. How WSL work: WSL 1: It did not run a real Linux kernel. Instead, it functioned as a real-time translation layer. It tricked Linux binaries into thinking they were communicating with a Linux kernel, when in reality, they were talking to a clever interpreter connected to the Windows kernel. WSL 2: Due to the limitations of translating every single Linux syscall, WSL 2 uses a lightweight, highly optimized type 1 virtual machine. Use case Server consolidation and optimization Development and testing envionments Application isolation and legacy application support Pros and Cons Pros:\nStrong isolation Total compatibility Cons:\nHeavyweight and slow Inefficient Containerization Containerization works by virtualizing the operating system, allowing an application to run in an isolated user space with all its dependencies, code and libraries. It all runs on a single host operating system and shared host OS\u0026rsquo;s kernel, making containers incredibly lightweight and fast.\nHow containers work Containers work by creating isolated environments for applications using two key technologies built into the host OS\u0026rsquo;s kernel:\nNamespaces: This feature provides isolation. Each container gets its own isolated view of resources like the network stack, process IDs, and filesystem mounts. This prevents conbtainers from seeing or interacting with each other or the host system. Control Groups(cgroups): This feature manages resource allocation. It limits and monitors how much of the host\u0026rsquo;s physical resources, such as CPU, RAM, I/O, each container can consume. This ensures no single container can monopolize the host\u0026rsquo;s resources. How container engine work Rule: A tool to create, run and manage container. It\u0026rsquo;s the translator/project manager between user and host\u0026rsquo;s OS kernel. Workflow: Creating a Dockerfile: A developer creates a text file defining all the steps required to build the application environment. Building an Image: The container engine reads the Dockerfile and packages it into a single, read-only, standardized image. This image serves as a static template for the container. Running a Container: The container engine uses this image to launch one or more container instances. During runtime, it creates an isolated namespace cgroups, while also adding a writable layer on top of the image to make the applications runnable. Docker vs Kubernetes vs Podman These are container enginers, but they serve very different purposes.\nDocker: The industry standard, it provides an all-in-one platform that includes a daemon, a client and an image registry(Docker hub). It\u0026rsquo;s easy to get started with and has a mature ecosystem Podman: A more security, daemonless alternative. Its command line is highly compatible with Docker\u0026rsquo;s, its run as a non-root user by default, and its architecture is more streamlined. Kubernetes: When applications scale up and need to run across hundreds or thousands of containers and multiple servers, Kubernetes is needed. It is a container orchestrator and servers as the brain of a container cluster. Kubernetes is not responsible for the actual running of containers. Instead, it handles higher-level management tasks such as automated deployment, elastic scaling, service discovery, load balancing and self healing. How distrobox works Distrobox is a clever tool that uses a container negine like Podman to create tightly integrated development environments. Its main purpose is to let you run any Linux distribution inside a container on your host OS but make it feel completely native.\nUse case Breaking down large, monolithic applications into smaller, independently deployable services Creating consistent and reproducible environments for building, testing and deploying software. Pros and Cons Pros\nLightweight and fast Highly portable Cons\nWeaker isolation You can only run containers that are compatible with the host OS kernel. Sandbox A sandbox is a secure, isolated environment on a computer where you can run untrusted program without risking harm to your host system.\nHow to create a sandbox Container Virtual machine Using dedicated sandbox software(like Sandboxie-Plus) Use case To be listed on the App Store or Google play store, and application must run in a sandbox. That\u0026rsquo;s why when we open a new app, the user have t approve a list of resources that enable the app to access. Education Browser plugin Pros and Cons Pros\nExtremely lightweight and fast: A sandbox applies rules to an already running process, adding minimal overhead. It\u0026rsquo;s instantaneous Targeted security: It\u0026rsquo;s perfect for its narrow purpose: running a single untrusted application and preventing it from accessing your personal files, network or hardware. Cons\nWeakest Isolation: It shares the host OS and kernel. Limited scape: It\u0026rsquo;s purely a security feature, not a deployment or development tool. ","permalink":"http://localhost:1313/posts/container_sandbox_vm/","summary":"\u003cp\u003eRecently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them.\n\u003cimg loading=\"lazy\" src=\"/Interesting_thing/distrobox.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"virtualization\"\u003eVirtualization\u003c/h2\u003e\n\u003ch4 id=\"how-virtualization-work\"\u003eHow virtualization work\u003c/h4\u003e\n\u003cp\u003eVirtualization is the technology that allows a single physical machine, known as the \u003cstrong\u003ehost\u003c/strong\u003e, to run multiple virtual machines or \u003cstrong\u003eguests\u003c/strong\u003e.\u003c/p\u003e","title":"Virtual machine vs Container vs Sandbox"},{"content":"The Global Interpreter Lock (GIL) At its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code. It is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment. The primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\nPython\u0026rsquo;s Memory Management Python uses a system called automatic memory management. Each object in Python has a reference count, which is a number that keeps track of how many variables or other object refer to it. When you assign a variable to an object, its reference count increases by one. When a reference is removed (for instance, when a variable goes out of scope), the count decreases. Once an object\u0026rsquo;s reference count drops to zero, it means nothing is using it.\nThis is where GIL becomes important. In a multi-threaded program, multiple threads could try to increase or decrease the same object\u0026rsquo;s reference count simultaneously, which may cause memory leaks.\nPython 3.13 GIL has become a bottleneck for CPU program performance, by reconstruct python, the development team has made a groundbreaking change in Python 3.13: the ability to disable the GIL.\nWhy python slow GIL Dynamic Datatype. It\u0026rsquo;s an interpreter language Easy to read makes python very abstract Trivias Generating functions are a powerful mathematical tool that transform discrete sequences into algebraic functions, enabling us to solve complex combinatorial counting problems through function operations. GPOS vs RTOS General purpose operating systems like Windows, macOS, and Linux are designed to provide a versatile, user-friendly computing environment. They prioritize overall system efficiency, with more flexible restrictions on task response times. Real time operating systems such as VxWorks, focus on strict time limitations for individual tasks, ensuring predictable and deterministic responses. These systems are typically used in environments demanding high reliability and precise time control, featuring a smaller, more streamlined kernel that guarantees real-time performance. In python, Boolean type is essentially a subclass of the integer type, True == 1 and False == 0. This design is mainly due to historical reasons and pragmatic considerations. In early version of Python(before 2.3), there was no dedicated bool type, and people used integers 1 and 0 to represent true and false. When the bool type was introuced, in order to allow old code to continue to run seamlessly, it was the best choice to continue to design it as a subclass of int. True + 1 = 2, False * 1 = 0 Huffman coding, an algorithm to compress data losslessly. Huffman coding assigns variable-length binary codes to input symbols. More frequent symbols -\u0026gt; shorter codes Less frequent symbols -\u0026gt; longer codes Windows vs Linux Aspect Linux Windows System Usage Lightweight, minimal background processes Heavy, many preloaded services and features Bloatware No pre-installed junk, user chooses all Comes with many default apps and features Transparency Fully open, config files are plain text Many hidden processes, registry-based config Customizability Highly customizable, from kernel to GUI Limited customization without hacking Privacy \u0026amp; Security User-controlled, minimal telemetry Sends telemetry, often needs antivirus Software Support Great for dev tools, less for gaming Excellent app/game compatibility System Control Full control over system and services Some restrictions, frequent auto-updates Hardware Support Good but sometimes manual setup required Plug-and-play for most consumer hardware Resouces Why the formular of normal distribution has a pi:explain from 3b1b Talks æ¼«å£«ï¼\n","permalink":"http://localhost:1313/posts/techweekly/techweek5/","summary":"\u003ch2 id=\"the-global-interpreter-lock-gil\"\u003eThe Global Interpreter Lock (GIL)\u003c/h2\u003e\n\u003cp\u003eAt its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code.\nIt is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment.\nThe primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\u003c/p\u003e","title":"Some Python Notes | King Weekly"},{"content":"Bitcoin The legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\nThe Blockchainâ›“ï¸ The blockchain is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\nIt\u0026rsquo;s a Chain of Blocks: Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block. It\u0026rsquo;s Immutable: Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending. The Genesis BlockğŸ“œ The very first block, known as the Genesis Block, was mined on 2009.01.04, by Bitcoin\u0026rsquo;s mysterious creator, Satoshi Nakamoto. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\nMiningâ›ï¸ Mining is the process of creating new blocks. It\u0026rsquo;s a competitive race that serves two critical purposes:\nValidating Transactions: Miners group pending transactions into a new block. Creating New Bitcoin: The winner of the race is rewarded with new bitcoin. Hereâ€™s how a miner wins the race and proves their block is valid:\nThe Hashing Puzzle: Miners take the data in their block and use their computers to find a specific number called a nonce. When the block data and the nonce are combined and put through a cryptographic function (SHA-256), they produce a unique digital fingerprint called a hash. The \u0026ldquo;Lower Than\u0026rdquo; Rule: To win, a miner must find a hash that is lower than the current network \u0026ldquo;target\u0026rdquo;. This target is a very large number that the entire network agrees on. Finding a hash below this target is incredibly difficult and requires immense computational powerâ€”it\u0026rsquo;s like trying to win a global lottery every 10 minutes. The Reward and The Halving: The first miner to find a valid hash wins the block reward. Initially, the reward was 50 BTC. However, this reward is programmed to cut in half roughly every four years (or 210,000 blocks) in an event called the halving. As of the April 2024 halving, the reward is now 3.125 BTC. This mechanism controls the supply of new bitcoin, making the currency scarce and ensuring its total amount will never exceed 21 million coins. When a winning block is found, its hash is broadcast across the P2P network. All other participants quickly verify that the hash is valid. Once confirmed, they add the new block to their copy of the blockchain and immediately start competing to solve the next block.\nHow to Mine: There are a few ways to participate in Bitcoin mining, each with its own pros and cons.\nSolo Mining: This is you, on your own, trying to solve a block. If you succeed, you get the entire block reward (3.125 BTC + transaction fees). However, the odds of a single person solving a block today are astronomically low due to the immense competition. It\u0026rsquo;s like buying a single lottery ticket and hoping to win the grand prize. Mining Pool: This is the most common method. You join a \u0026ldquo;pool\u0026rdquo; with thousands of other miners, combining your computing power. The pool works together to find blocks much more frequently. When the pool wins, the reward is split among all participants based on how much computing power they contributed. This provides smaller, but much more consistent and predictable, payouts. Block ForksğŸ´ A fork happens when the blockchain temporarily or permanently splits into two different paths.\nAccidental Fork: Sometimes, two miners find a valid block at almost the exact same time. The network briefly splits as some nodes follow one miner and some follow the other. This is usually resolved within a few minutes when the next block is found and added to one of the chains, making it the \u0026ldquo;longest\u0026rdquo; and therefore the official one. The shorter chain is then abandoned. Hard Fork: This is an intentional split that happens when the network\u0026rsquo;s software rules are changed in a way that is not backward-compatible. All participants must upgrade to the new rules to continue. If a significant portion of the community refuses to upgrade, the split becomes permanent, resulting in the creation of a new, separate cryptocurrency (e.g., Bitcoin Cash was created from a hard fork of Bitcoin). Ethics Mining bitcoin always consume immense energy, which critics view as a wasteful environmental cost for a seemingly meaningless computation. Proponents argue this mechanism decentralized financial system that offers freedom from the control of banks and governments.\nTools Calculate computing performance: https://www.nicehash.com/profitability-calculator\nThis is the computating power of my personal game laptop(one dollar per day hhh) Resources: Youtube channels\nhttps://www.youtube.com/watch?v=5hgdekVZb3A\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=3 https://www.youtube.com/watch?v=a41DMDfJjsU\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=2 https://Gemini.google.com Trivias Sherrington, coined the word \u0026ldquo;synapse\u0026rdquo; to define the connection between two neurons Two different roads of AI: connectionism and Symbolism. Hidden layer was firstly been implemented in Boltzmann machine, although Rosenblatt had some idea about multilayer perceptrons, but he didn\u0026rsquo;t find any useful training algorithm. Restricted Boltzmann machine - each layer is only allowed to be fully connectted to the next layer, current layer nodes are not connectted to each other. The advantages of this machine is it allows to update bodes within the same layer in parallel The invention of hidden layer allows model to understand abstract features. It also becomes to one of the most significant component in deep learning. The main difference between the Hopfield Network and the Boltzmann nachine is the presence of hidden layers. Other differences include the fact that the Hopfield network is deterministic, whereas the Boltzmann machine is stochastic, and the defintions of their energy function also differ. The fovea has many photoreceptors, with a high density of cones(for colors) and nearly no rods(for dark). This structure allows us to see the world clearly. If you develop myopia, the image formed after light is reflected by the eye may not be focused directly on the fovea. The idea of CNN was mainly inspired by the HMAX model(hierarchical, pooling, convolution), and the HMAX model was proposed by Tomaso Poggio, to simulate primate visual system, specifically ventral stream. Pytorch for research area, Tensorflow for industry and JAX for high level usage. Ethereum and ether is not the same as bitcoin. Ethereum has a longer vision, and the number of ether is unlimited. Resource Documentary of AlphaGo: https://www.youtube.com/watch?v=WXuK6gekU1Y How to use hugging face: https://www.youtube.com/watch?v=3kRB2TXewus This is the most comprehensive guide for AI beginner I had ever seen: Guide link ","permalink":"http://localhost:1313/posts/techweekly/techweek4/","summary":"\u003ch2 id=\"bitcoin\"\u003eBitcoin\u003c/h2\u003e\n\u003cp\u003eThe legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-blockchain\"\u003eThe Blockchainâ›“ï¸\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eblockchain\u003c/strong\u003e is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s a Chain of Blocks:\u003c/strong\u003e Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s Immutable:\u003c/strong\u003e Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-genesis-block\"\u003eThe Genesis BlockğŸ“œ\u003c/h4\u003e\n\u003cp\u003eThe very first block, known as the \u003cstrong\u003eGenesis Block\u003c/strong\u003e, was mined on \u003cstrong\u003e2009.01.04\u003c/strong\u003e, by Bitcoin\u0026rsquo;s mysterious creator, \u003cstrong\u003eSatoshi Nakamoto\u003c/strong\u003e. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\u003c/p\u003e","title":"Bitcoin Basic | King Weekly"},{"content":"2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning. However, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\nRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually. And before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation. So today I will record this moment.\nHopfield Network was invented at 1982. Processor John Hopfield designed it based on ideas from statistical mechanics.\nThe graph above shows two states of a ball From the left part, we can see the energy system of the ball is at the highest, which means the ball is very unstable From the right part, we can see the ball had already fall into the bottom, which means the ball is very stable. Although it is a classical physics model, but it definitly explain the main idea of hopfield network.\nSo now we can say hopfield network is just make a system move from an unstable state to stable state.\nIf you interested how exactly hopfield network work. See the video below, it\u0026rsquo;s pretty nice.\nThis idea is also useful in today. In LLM, we can say the user\u0026rsquo;s prompt and question is the most unstable states while the answer is the stable state.\nBased on the concept of Hopfield networks, many different architectures had been invented, which makes Connectionism and Deep Learning great again.\nAfter all, I catch the reason why nobel prize gives to physics.\n","permalink":"http://localhost:1313/posts/hopfieldnetwork/","summary":"\u003cp\u003e2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning.\nHowever, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\u003c/p\u003e\n\u003cp\u003eRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually.\nAnd before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation.\nSo today I will record this moment.\u003c/p\u003e","title":"Hopfield Network"},{"content":"Book - Unix: A History and a Memoir Recently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world. During the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\nAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals. The system developed before Unix was called Multics. Since â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€ Unix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10. Tools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs. Unix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems. GNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source. MacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX. In the early days, operating systems were not portable. This changed with the invention of the C language and its compiler. MINIX was widely used because it was embedded in Intel chips. The working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation. Microsoft once had its 3own Unix-like system. Another completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows. You can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan. â€œEverything is a fileâ€ is one of the core principles of Unix. The KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy. The UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\nKeep it simple stupid Do one thing, and do it well Everything is a file Make each program a filter Fail loudly Modularity Prototyping early In today, many barriers had already been removed.\nAnd I realise.\nThe revolution of AI is just like the reenactment of Unix\u0026rsquo;s development.\nSo.\nKISS.\nTrivias tty - TeleTYpewriter, terminal in the old time, before lcd screen been invented. UNIX was developed on the PDP-7, a computer with no screen, no mouse and only 8KB of RAM. It weighted nearly 500kg. UNIX and UNIX-like system use abbreviated commands because typing on TTY terminals in the 1960s was slow and insufficient. Second-system effect: It believes that after completing a small, elegant, and successful system, people tend to have overly high expectations for the next projects, which may lead to the creation of a huge, feature-rich but monstrous system.The \u0026ldquo;second-system effect\u0026rdquo; can result in software project plans being overdesigned, with too many variables and excessive complexity, ultimately falling short of expectations and leading to failure. such as PL/I in Multics Fortran(formular translation): The purpose of this lagnauge is to proceed mathematics formular and float number in an efficient way, like integration, linear algebra. That\u0026rsquo;s why fortran is still popular in some supercomputer and scientific calculation. B lagnauge is designed in a bit-unit computer PCP-7, where C langauge is designed in a byte-unit computer. Therefore the main difference between B and C is B langauge doesn\u0026rsquo;t support types and C does. Development of Clang: PL/I -\u0026gt; BCPL -\u0026gt; B -\u0026gt; New B(C) If computers using the same cpu architecture, they will using the same assembly language. The grep command is used to find lines that match a specific pattern in a file while the sed command is used to insert, replace and delete text from a file. Finally, the awk command supports programming logic and is often used for advanced data processing tasks. The development of UNIX from 1970 until now. Talks \u0026ldquo;A new technological discovery is often discredited by older generations of professionals - especially those with high authority and prestige in the existing field - in order to protect their own status\u0026rdquo; ","permalink":"http://localhost:1313/posts/techweekly/techweek3/","summary":"\u003ch2 id=\"book---unix-a-history-and-a-memoir\"\u003eBook - Unix: A History and a Memoir\u003c/h2\u003e\n\u003cp\u003eRecently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world.\nDuring the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals.\u003c/li\u003e\n\u003cli\u003eThe system developed before Unix was called Multics.\u003c/li\u003e\n\u003cli\u003eSince â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€\u003c/li\u003e\n\u003cli\u003eUnix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10.\u003c/li\u003e\n\u003cli\u003eTools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs.\u003c/li\u003e\n\u003cli\u003eUnix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems.\u003c/li\u003e\n\u003cli\u003eGNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source.\u003c/li\u003e\n\u003cli\u003eMacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX.\u003c/li\u003e\n\u003cli\u003eIn the early days, operating systems were not portable. This changed with the invention of the C language and its compiler.\u003c/li\u003e\n\u003cli\u003eMINIX was widely used because it was embedded in Intel chips.\u003c/li\u003e\n\u003cli\u003eThe working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation.\u003c/li\u003e\n\u003cli\u003eMicrosoft once had its 3own Unix-like system.\u003c/li\u003e\n\u003cli\u003eAnother completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows.\u003c/li\u003e\n\u003cli\u003eYou can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan.\u003c/li\u003e\n\u003cli\u003eâ€œEverything is a fileâ€ is one of the core principles of Unix.\u003c/li\u003e\n\u003cli\u003eThe KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\u003c/p\u003e","title":"For UNIX Week | King Weekly"},{"content":"Model Context Protocol overview MCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\nWhy is MCP needed? In the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces). The current components of MCP servers include: How does MCP work? Example: Suppose you\u0026rsquo;re using an AI assistant to manage your work, and it wants to help you schedule today\u0026rsquo;s meeting:\nWithout MCP, developers would need to write separate integration code for Outlook, Google Calendar, and Apple Calendar.\nWith MCP, the AI only needs to call the MCP server, which will automatically interface with your calendar system. No matter which calendar service you use, the AI can work seamlessly.\nCore Functions of MCP: Reducing development costs (no need to develop separate integrations for each tool).\nEnhancing AI\u0026rsquo;s ability to access external data (allowing AI to easily query and manipulate external data).\nStandardizing communication (making communication between different AI applications and tools smoother).\nYou can think of MCP as the \u0026ldquo;USB interface for AI\u0026rdquo;â€”any AI device can plug into different tools without needing to individually adapt to each one!\nTrivias: Graph is a great data structure, it can be used to find the shortest path, solve a magic cube. Rely on this data structure, Human find the shortest steps to solve the worst case configured 3\\*3\\*3 magic cube in 20 steps (which also called god\u0026rsquo;s number) and 11 steps for 2\\*2\\*2 one. What about n*n*n, Here is an interesing paper about time compleixity of solving a n*n*n magic cube: Algorithms for Solving Rubikâ€™s Cubes Topological sort is an algorithm based on DFS and DAG, It\u0026rsquo;s not a traditional sorting algorithm, like comparing the size of each number and sort them, but sorting based on dependencies. Dynamic programming is just like recursion + memorization + guessing P vs NP problems: P is a problem that can be solved easily by a computer, NP is a problem that you can check for correctness very easily once solved. However, P != NP for example wec can\u0026rsquo;t engineer luck. Also, NP hard means it is at least as hard as any problem in NP, and NP-complete is lke if you can solve one NP-complete question, you can solve all NP question. Reduction is like to prove a known NP-complete question, and transfer it into a NP question X, then X is also a NP-complete question HTTP vs REST API HTTP is a protocol, and its core task is to define how to request and transfer data between your broswer and server\nIt focuses on the data exchange level, regardless of whether the data is an image, text, video, or API response. For example, you can use an HTTP request to access a webpage (HTML page) or use an HTTP request to retrieve API data (JSON data). It doesnâ€™t care about what data you\u0026rsquo;re transmitting. REST API is a design style, based on the HTTP protocol, with rules and best practices:\nIt views content on the server as \u0026ldquo;resources\u0026rdquo; and specifies how to operate on them using HTTP methods (GET, POST, PUT, DELETE). REST API design considers resource orientation: Resources (like users, articles, comments, etc.) have unique identifiers (URIs), and clients interact with these resources through HTTP requests. For example, in a REST API, you can:\nGET request: Retrieve a resource (e.g., get movie information). POST request: Create a new resource (e.g., submit a new comment). PUT request: Update a resource (e.g., modify user information). DELETE request: Delete a resource (e.g., remove an article). Resource: A tool to build a personal streaming music player: navidrome A great blog that introduce human visualization: human visualization Five basic algorithms explanation: Dynamic programming Greedy Algorithm Backtracking Branch and Bounding This week, Nvidia GTC 2025 brings a lot of new tech, PC in AI age, B300, new architecture, robots. After watched the coverage keynotes, I was just feel like our mankind is in an special stage with unpredictable pace. In science fiction films, at this point , it often leads to the arrival of an alien civilization. A dijkstra algorithm visualiser that helps me understand it: Dijkstra shortest path MCP explain: A blog that explain MCP crystal clear ","permalink":"http://localhost:1313/posts/techweekly/techweek2/","summary":"\u003ch2 id=\"model-context-protocol-overview\"\u003eModel Context Protocol overview\u003c/h2\u003e\n\u003cp\u003eMCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\u003c/p\u003e\n\u003ch4 id=\"why-is-mcp-needed\"\u003eWhy is MCP needed?\u003c/h4\u003e\n\u003cp\u003eIn the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\u003cbr\u003e\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces).\n\u003cimg alt=\"components mcp include\" loading=\"lazy\" src=\"/TechStuff/mcp.png\"\u003e\u003c/p\u003e","title":"Jarvis Will Coming Soon | King Weekly"},{"content":"Email sending and receiving system The main system is build based on three protocols: SMTP, POP3 and IMAP. SMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails. User1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\nPOP3 downloads emails from the email server to the email client. By default, it removes emails from the server after downloading, but some email clients allow users to keep copies on the server. IMAP keeps emails on the server and synchronizes them across multiple devices. The email client initially loads only the headers, and the full email content is fetched from the server when the user opens it.\nIf you want to customize an email domain. You need to have your own SMTP and IMAP/POP3 server and a domain, the other steps are the same as above.\nPKGBUILD in Arch Linux PKGBUILD is a bash script contain the build information required by archlinux package we use makepkg script to build the package, it will search PKGBUILD first in the current folder. Benefits:\nusing pacman to manage, user can update and uninstall easily some pkgbuild file include the commands to generate a binary file and store it in /user/bin Drawbacks:\nNot friendly to starter Although we can use yay to help us do all these stuff.\nTrivias Newton\u0026rsquo;s method is quadratic convergence when we want to calculate the root of a number Catalan number is a group of sequence that appear widely in combinatorics. e.g. ways to arrange n brackets, number of triangles in an n+2 convex polygon. The common property of these applications is that they are recursive and have a constrained structure. Toom cook: It divide a d-digit number into n parts and doing arithmatic calculations. Schonhage-strassen scheme: It multiplies two integers of length ğ‘› in O (ğ‘› logğ‘› log logğ‘›) steps on a multitape Turing machine A Naive algorithm is usually the most obvious solution when one is asked a problem. It may not be a smart algorithm but will probably get the job done The taste of red wine is determined by acidity, sweetness, alcohol content, tannins, and body. Wines are categorized into New World and Old World. New World wines (from countries like the USA, Chile, Argentina, and China) are named after the grape variety, while Old World wines (mainly from Europe) are named after their place of origin.\nRed wine is made by fermenting red grapes with their skins. White wine is made from either white grapes or red grapes without their skins. RosÃ© wine is made by soaking the grape skins briefly but fermenting without them. Sparkling wine undergoes a second fermentation to produce bubbles. Gabriel\u0026rsquo;s horn is a type of geometric figure that has infinite surface area but finite volume. Resources Code question(leetcode), system design question(crack the code interview), teamwork, communication are all important in the interview. An old guideline to learn ai: https://www.captainai.net/itcoke/ A guideline to learn CS: https://csdiy.wiki/åè®°/ Useful tips to integrate by parts, åå¯¹å¹‚æŒ‡ä¸‰, to choose u. Customize your zsh: oh my zsh Xiaomi releases a concept modular camera, it looks pretty awesome and innovative. 3b1b\u0026rsquo;s taylor series explaination:https://www.youtube.com/watch?v=3d6DsjIBzJ4 3b1b\u0026rsquo;s explaination of why we have exponential e:https://www.youtube.com/watch?v=m2MIpDrF7Es This website is all about competitive writing of source code that is as short as possible: Codewolf Explaination of greedy algorithm: greedy algorithm Explaination of dynamic programming: dynamic programming Deploy perosonal VPN tools: tailscale Abstract If no one is reading blogs anymore, why should we write them? Letâ€™s make it simple: you write a blog, but nobody cares, nobody reads it. At least, the number of readers is not as many as you thought. You put your personal ideas and thoughts into the article, carefully structuring each sentence, and choose a great imageâ€”then, no response, no likes, no shares, no activity. So, what is the meaning of writing a blog? First, there are two misconceptions about blogging. One is that if I write a good article, readers will come naturally. No, they wonâ€™t come. There are billions of blogs on the internet, like a massive hurricane, and yours is just a single leaf in the wind. Who would notice? Another misconception is that if nobody reads it, writing is a waste of time. Blogs have their own hidden value. You write blogs not for the applause of others, but for your own needs. Blogs help clear your mind. They help you organize your thoughts and sharpen your perspective. When you think better, you will achieve better results. The target audience of a blog is actually not the people on the internet, but your future self. Your article will help you see the evolution of your own thoughts. Additionally, one day in the future, someone who truly needs your article will find it. A deep, thoughtful article has a longer-lasting impact than a viral article. Writing a blog is quite like street photography. You take your camera and walk through the city. You see a sceneâ€”a moment filled with light, shadow, and humanityâ€”and then you capture it. Nobody cares about what you actually captured. But thatâ€™s not the reason you photograph; you photograph because you see something interesting. Writing a blog is the same. You write a blog because you are thinking, observing new things, and hope to store them somewhere. If someone reads it, that\u0026rsquo;s great. If not, youâ€™ve still completed your work\n","permalink":"http://localhost:1313/posts/techweekly/techweek1/","summary":"\u003ch2 id=\"email-sending-and-receiving-system\"\u003eEmail sending and receiving system\u003c/h2\u003e\n\u003cp\u003eThe main system is build based on three protocols: SMTP, POP3 and IMAP.\n\u003cimg alt=\"process of email system\" loading=\"lazy\" src=\"/emailsys.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails.\nUser1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\u003c/p\u003e","title":"Start | King Weekly"},{"content":"In this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O O(n) notation before, but for Big theta Î¸(n) and reccurence relations T(n), I never heard them before. Today, I hope I can finally figure them out.\nWhat T(n) represents the actual running time of an algorithm\nO(n) represents the asymptotic upper bound of the running time of an algorithm\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\nHow to convert the three of them Normally we can directly transfer T(n) to O(n) or Î¸(n). Int sum = 0 for (i = 1; i \u0026lt;= n, i ++) { sum = sum + i } This is a classical example, First of all, we initialize variable sum requires one unit of running time. There are three statements inside the for loop, statement 1 i = 1 requires one unit of the running time, statement 2 i \u0026lt;= n requires n+1 units of the running time, statement 3 i ++ requires n units of the runningn time, and sum = sum + i requires 2n units of the running time, n for addition and n for assignment. Therefore T(n) = 1+1+(n+1)+n+2n = 4n + 3. In O(n), we ignore the constant and the lower-order terms, therefore the time complexity is O(n) / Î¸(n).\nWhen the algorithm is a recursion, such as karatsuba multiplication and high precision multiplication. There are two methods to convert T(n) into O(n)\nRecursion tree method According to Recursion tree method, we derive master theorem The time complexity of multiplication is equal to the time complexity of division\n","permalink":"http://localhost:1313/posts/the-difference-between-tn--on-and-%CE%B8n/","summary":"\u003cp\u003eIn this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O \u003ccode\u003eO(n)\u003c/code\u003e notation before, but for Big theta \u003ccode\u003eÎ¸(n)\u003c/code\u003e and reccurence relations \u003ccode\u003eT(n)\u003c/code\u003e, I never heard them before. Today, I hope I can finally figure them out.\u003c/p\u003e\n\u003ch1 id=\"what\"\u003eWhat\u003c/h1\u003e\n\u003cp\u003eT(n) represents the actual running time of an algorithm\u003cbr\u003e\nO(n) represents the asymptotic upper bound of the running time of an algorithm\u003cbr\u003e\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\u003c/p\u003e","title":"Time Complexity Notations"},{"content":"This week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\nOverview The machine model is HP-Elitedesk-800-G4-SFF. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\nIf you want to learn more here is the machine datasheet:server_datasheet\nHardware Motherboard: Q370 viewer CPU: i5-8500 GPU: intel UHD 630 RAM: 16G SSD: 256G HDD: 500G * 1 The motherboard provides a high flexibility to expand more internal storage, also it has 4 PCie expansion slots which can used to expand more storage space or other components you want.\nSoftware nextcloud\nemail domain\ngitlab\nminecraft server\ndocker\njellyin\nsynthing\nproxy?router?gateway?\nvirtual machine\nI host my server with ubuntu server distro. The reason I didn\u0026rsquo;t choose proxmox is because I want to learn server step by step, proxmox is great in visualization, maybe in the future, I will try it.\nDurign the process of configuring storage, I learned LVM, which is a wonderful tool for those users that has multiple drives. User can create a storage pool called volume group. Firstly, user add their physical volumes into volumn group, and we create logical volumn based on the storage area had in volumn groups, and then we mount those LVs with the actural dirctory. It seems like Windows is not able to achieve this function. For RAID, we seperate it into 4 different categories, radi0, raid1, raid5 and raid10. This tools shows how to save files in different number of drives or in LVM.\nFor external access, I plan to use cloudflare tunnel. They provide such service, I need to buy a domain name and combine it with the cloudflare tunnel, and when I access the server, I firstly type the domain name in my browser to ask cloudflare, and they will guide me to the tunnel to my server, also in server end, I need to install cloudflared docker image as an end, then it works! It\u0026rsquo;s so convenient for those people who live in school accommodation. And it\u0026rsquo;s totally free!\nIn my network configuration. I didn\u0026rsquo;t install a router, but the best choice is to use a router for all devices in my home, and assign each of them an static IP address. To access the server, I bought a portable monitor, since the IP address of my server using DHCP, which required to check the IP address manually when the machine reboot or close. This issue will be solved easily when I have a router. Nowadays, I need to change the tunnels configuration to enable external access.\nsince I have 3 different operating systems in my three daily devices, phone for android, laptop for arachlinux and ipad for ipados, I installed nextcloud docker to try to integrate them into one ecosystem, that pretty awesome.\nUpdate Nextcloud for cloud storage Able to build game server Music and video live streaming server. Future Mining bitcoin ","permalink":"http://localhost:1313/posts/homeserver/","summary":"\u003cp\u003eThis week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThe machine model is \u003ccode\u003eHP-Elitedesk-800-G4-SFF\u003c/code\u003e. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\u003cbr\u003e\n\u003cimg loading=\"lazy\" src=\"/TechStuff/serverbill.png\"\u003e\u003c/p\u003e","title":"My Homelab"},{"content":"This semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\nIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\nHow the JVM works The JVM executes er programs in several stages:\nCompilation: Java source code (.java) is compiled into bytecode (.class) by the Java Compiler (javac).\nClass Loading: The JVM loads compiled bytecode when required.\nBytecode Verification: Ensures security and correctness before execution.\nExecution: The JVM interprets or compiles bytecode using Just-In-Time (JIT) compilation.\nGarbage Collection: The JVM automatically manages memory, reclaiming unused objects.\nClass loader One of the organizational units of JVM byte code is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.\nLet\u0026rsquo;s explore this concept with an example: Imagine you\u0026rsquo;re watching a movie on a streaming service.\nLoading:\nThe service first finds and imports the movie data you want to watch. Similarly, the class loader locates the binary data for a Java class. Linking: Verification: Before you watch the movie, the service checks that the file isn\u0026rsquo;t corrupted. In Java, the class loader verifies the correctness of the class. Preparation: The service sets up the necessary space in memory to buffer the movie. Java allocates memory for variables and sets default values. Resolution: The service ensures all necessary subtitles or audio tracks are ready to play. Java resolves references to make them direct. Initialization:\nAs you start watching, the service begins playing the movie. Similarly, Java runs code to set up variables with their starting values. Class Loader Types:\nBootstrap Class Loader: Like the service\u0026rsquo;s core library of well-known movies, it loads fundamental, trusted classes. Extension Class Loader: Similar to special add-on features, it loads additional classes outside the core library. System/Application Class Loader: Like searching for new releases or user-uploaded content, it loads classes specific to the application youâ€™re using. Virtual machine architecture Cross-Platform Compatibility and Limitations The JVM abstracts away the underlying hardware and operating system specifics, allowing Java bytecode to run on any device equipped with a compatible JVM. This cross-platform capability greatly simplified software distribution and development, as developers could write code once and deploy it across various platforms without modification.\nDespite its strong cross-platform capabilities, the JVM faced significant challenges due to competitive corporate strategies, particularly the \u0026ldquo;Embrace, Extend, and Extinguish\u0026rdquo; (EEE) approach adopted by some companies like Microsoft in the late 1990s. This strategy involved embracing a technology, extending it with proprietary features, and eventually using those extensions to undermine the original technology.\nMicrosoft initially embraced Java by integrating it into their Internet Explorer browser and Windows platforms. However, they extended Java with proprietary features that were specific to Windows, creating a version of Java that was incompatible with the standard JVM specifications set by Sun Microsystems. This move fragmented the Java platform and undermined the \u0026ldquo;Write Once, Run Anywhere\u0026rdquo; philosophy.\nWith the development and rise of programming languages like Swift, Kotlin, and JavaScript, the JVM faced significant challenges in maintaining its performance edge. Swift, designed by Apple for iOS and macOS platforms, offers high performance and safety due to its compiled nature and modern language features. Kotlin, although initially running on the JVM, introduced concise syntax and advanced features that surpassed Java in many ways, leading it to become the preferred language for Android development. JavaScript\u0026rsquo;s performance greatly improved with engines like V8, and its versatility expanded through technologies like Node.js for server-side development. These languages not only matched but often exceeded the JVM\u0026rsquo;s performance and adaptability in their respective domains, leading to a shift in developer preferences and a relative decline in the JVM\u0026rsquo;s dominance.\nOverall Nowadays, Java has become to a normal programming lauguage. And the question is obvious solved. Python has its own interpreter to transfer the source code into machine code. Haskell has its own compiler, C++/C are compiled directly into machine code. However,they can\u0026rsquo;t generate a compiled file that enable to run in every operating system. If there is no EEE strategy, Linux may have a stronger effects in today\u0026rsquo;s world.\n","permalink":"http://localhost:1313/posts/jvm/","summary":"\u003cp\u003eThis semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\u003c/p\u003e","title":"JVM's Achievements and Limitations"},{"content":" ğŸ¤— ","permalink":"http://localhost:1313/aboutme/","summary":"aboutme","title":"About me"},{"content":"Location: Auchinstarry Quarry, Scotland Date: 2025.09.12 A great first-time rock climbing experience, meet interesting people and learned some rope skills.\n","permalink":"http://localhost:1313/aboutme/climbing/","summary":"\u003ch2 id=\"location-auchinstarry-quarry-scotland\"\u003eLocation: Auchinstarry Quarry, Scotland\u003c/h2\u003e\n\u003cp\u003eDate: 2025.09.12\n\u003cimg alt=\"rock and me\" loading=\"lazy\" src=\"/Aboutme/croy_rock_and_me.jpg\"\u003e\nA great first-time rock climbing experience, meet interesting people and learned some rope skills.\u003c/p\u003e","title":"Climbing road"},{"content":" A free web application that create a two-host daily podcast of the news that matters to you. Just add your favorite RSS feeds from news sites, blogs, and social media. Our app will automatically generate conversational scripts using an LLM and then produce a customized audio episode with text-to-speech. It\u0026rsquo;s the perfect way to stay updated on your commute.\nOfficial website\nCode available in github\nMotivation The idea of having a J.A.R.V.I.S. from the Marvel movies, is a common dream. With today\u0026rsquo;s advancements in LLM and TTS technology, we can now offer a real-world experience that captures a part of that dream.\nFor me, I love to start my mornings by checking my favorite blogs for updates, scrolling through X for new tech news, or quickly scanning the day\u0026rsquo;s headlines. With this application, I could simply wake up, click a button to generate my podcast, and then, while having breakfast or commuting, listen to a personalized summary of everything that happened overnight. If a specific story catches my interest, I can then go back and read the full article later. I think its an efficient way to stay informed.\nTech stack Area Technology Backend FastAPI, Python, SQLAlchemy, PostgreSQL, Pydantic Frontend React, Javascript, TypeScript, Vite, Tailwind CSS AI \u0026amp; TTS Google Gemini 2.5 Pro API Deployment Self-host(personal server), Docker Compose(Backend, Frontend, background worker, PostgreSQL, Valkey) AI assistant Gemini 2.5 pro, Claude sonnet 4 Issues I met Gemini api timeout: The current Gemini API has a 60-second request timeout, which is insufficient for generating a 10-minute podcast with a single text-to-speech (TTS) request. To solve this, my initial thought was to segment the entire podcast script into smaller parts and generate each segment separately. However, a new challenge arose with the TTS service\u0026rsquo;s free plan, which limits me to just 15 requests per day. This makes the segmentation approach impractical for daily use. My temporary fix has been to adjust the script prompt to generate shorter content, but this method is unreliable and doesn\u0026rsquo;t always prevent timeouts. The long-term solution will be to upgrade to a paid TTS service with a longer API request timeout. This will allow for single, uninterrupted podcast generation, solving the issue at its root. Roadmap 2025-09: Migrate deployment from Render to my personal server âœ… - 2025.09.21 2025-10: Add more social medias RSS feeds 2025-12: Multi-format input file support Future: Use fine-tuning TTS model Improve user experience Support spotify export Summary This project takes me 2 month from learning tech stacks to final outcome. I want to say AI really helped me a lot during the building process, whereas from learning tech stacks to fix bugs. From this prject, I experience both frontend and backend project building, frontend is just like building an artwprk, it really need me to keep patient, backend is more interesting since it relative to many different fields like crud operations, building database schemas and endpoints and connection between database, background worker. I also experience the magic of containerization, its so convenient. - 2025.08.14\n","permalink":"http://localhost:1313/aboutme/projects/lets_break_the_information_gap/","summary":"aboutme","title":"Let's break the information gap"},{"content":"ğŸ“· Welcome to my photography exhibition! ","permalink":"http://localhost:1313/aboutme/gallery/","summary":"\u003ch1 id=\"heading\"\u003eğŸ“·\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"welcome-to-my-photography-exhibition\"\u003eWelcome to my photography exhibition!\u003c/h1\u003e","title":"Photography exhibition by King Jin"},{"content":" Let\u0026rsquo;s break the information gap A free web application that create a two-host daily podcast of the news that matters to you. Reflex An AI-powered, English translator that generates a unique conversational story each day based on your search history. ","permalink":"http://localhost:1313/aboutme/projects/","summary":"\u003cpicture class=\"center\"\u003e\n  \u003c!-- Dark mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects_dark.png\" media=\"(prefers-color-scheme: dark)\"\u003e\n  \u003c!-- Light mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects.png\" media=\"(prefers-color-scheme: light)\"\u003e\n  \u003c!-- Fallback image --\u003e\n  \u003cimg src=\"/Aboutme/projects.png\" alt=\"Project image\"\u003e\n\u003c/picture\u003e\n\u003cbr\u003e\n\u003cbr\u003e\n\u003c!-- Project lbtig --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003eLet\u0026rsquo;s break the information gap\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003e\u003cimg src=\"/Aboutme/lbtig_preview.png\" class=\"center\"\u003e\u003c/a\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp class=\"center-wider\"\u003e A free web application that create a two-host daily podcast of the news that matters to you.\n\u003c/p\u003e\n\u003chr\u003e\n\u003cbr\u003e\n\u003c!-- Project reflex --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003eReflex\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003e\u003cimg src=\"/Aboutme/reflex_preview.png\" class=\"center\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cbr\u003e\n\u003cp class=\"center-wider\"\u003e An AI-powered, English translator that generates a unique conversational story each day based on your search history.\n\u003c/p\u003e","title":"Project by King Jin"},{"content":" An AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\ncode available in github Motivation I dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\nTech stack Frontend: HTML, Tailwind CSS, Electron, Javascript\nRoadmap Future: support multi-language data record system\nSummary This project takes me one week to build. To be honest, Electron is convenient for cross platform, but also it increase the size of the software.\n","permalink":"http://localhost:1313/aboutme/projects/reflex/","summary":"\u003cblockquote\u003e\n\u003cp\u003eAn AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Kingjinsight/Reflex\"\u003ecode available in github\u003c/a\u003e\n\u003cimg alt=\"reflex_dashboard\" loading=\"lazy\" src=\"/Aboutme/reflex_dashboard.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\u003c/p\u003e","title":"Reflex"},{"content":"I really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\nColor Theory. Additive Color: RGB for Red, Green and Blue. Red + Green + Blue = White Subtractive Color: These are created by mixing two additive colors. Red + Green = Yellow Green + Blue = Cyan Red + Blue = Magenta Yellow + Cyan + Magenta = Black Basics Histogram: a graph that displays the brightness and color distribution of an image From left to right, each column represents the number of the white-black pixels in different level It has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites. There are two triangles at the top left and top right. the top left one is shadow clipping which will show you whether your image has region that was too dark the top right one is highlight clipping which show you whether your image has region that was too light the reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0. There are several colors in the triangle gray - all details are well preserved red - red channel overexposure/underexposure green - green channel overexposure/underexposure blue - blue channel overexposure/underexposure yellow - yellow channel overexposure/underexposure cyan - cyan channel overexposure/underexposure magenta - magenta channel overexposure/underexposure solid white - RGB channel channel overexposure/underexposure in the same time White balance Temperature Tint Tone Exposure: change the overall brightness of the photo Contrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa. Highlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc. Shadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc. Whites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white. Blacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black. Presence Texture: Focus on the surface texture, such as skin pores, surface of rocks, etc. Clarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;. Dehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation. Vibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated. Saturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree. Tone Curve The square graph looks similar with histogram, but contain a line segment from bottom left to top right. We can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve. we can also pull RGB channel separately. S curve to increase contrast Tips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more. This post will update frequently.\n","permalink":"http://localhost:1313/posts/lightroom/","summary":"\u003cp\u003eI really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\u003c/p\u003e\n\u003ch2 id=\"color-theory\"\u003eColor Theory.\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAdditive Color: RGB for Red, Green and Blue.\n\u003cul\u003e\n\u003cli\u003eRed + Green + Blue = White\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSubtractive Color: These are created by mixing two additive colors.\n\u003cul\u003e\n\u003cli\u003eRed + Green = Yellow\u003c/li\u003e\n\u003cli\u003eGreen + Blue = Cyan\u003c/li\u003e\n\u003cli\u003eRed + Blue = Magenta\u003c/li\u003e\n\u003cli\u003eYellow + Cyan + Magenta = Black\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"basics\"\u003eBasics\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHistogram: a graph that displays the brightness and color distribution of an image\n\u003col\u003e\n\u003cli\u003eFrom left to right, each column represents the number of the white-black pixels in different level\u003c/li\u003e\n\u003cli\u003eIt has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites.\u003c/li\u003e\n\u003cli\u003eThere are two triangles at the top left and top right.\n\u003col\u003e\n\u003cli\u003ethe top left one is shadow clipping which will show you whether your image has region that was too dark\u003c/li\u003e\n\u003cli\u003ethe top right one is highlight clipping which show you whether your image has region that was too light\u003c/li\u003e\n\u003cli\u003ethe reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eThere are several colors in the triangle\n\u003col\u003e\n\u003cli\u003egray - all details are well preserved\u003c/li\u003e\n\u003cli\u003ered - red channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003egreen - green channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eblue - blue channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eyellow - yellow channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003ecyan - cyan channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003emagenta - magenta channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003esolid white - RGB channel channel overexposure/underexposure in the same time\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eWhite balance\n\u003col\u003e\n\u003cli\u003eTemperature\u003c/li\u003e\n\u003cli\u003eTint\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eTone\n\u003col\u003e\n\u003cli\u003eExposure: change the overall brightness of the photo\u003c/li\u003e\n\u003cli\u003eContrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa.\u003c/li\u003e\n\u003cli\u003eHighlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc.\u003c/li\u003e\n\u003cli\u003eShadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc.\u003c/li\u003e\n\u003cli\u003eWhites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white.\u003c/li\u003e\n\u003cli\u003eBlacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePresence\n\u003col\u003e\n\u003cli\u003eTexture: Focus on the surface texture, such as skin pores, surface of rocks, etc.\u003c/li\u003e\n\u003cli\u003eClarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eDehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation.\u003c/li\u003e\n\u003cli\u003eVibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated.\u003c/li\u003e\n\u003cli\u003eSaturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"tone-curve\"\u003eTone Curve\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThe square graph looks similar with histogram, but contain a line segment from bottom left to top right.\u003c/li\u003e\n\u003cli\u003eWe can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve.\u003c/li\u003e\n\u003cli\u003ewe can also pull RGB channel separately.\u003c/li\u003e\n\u003cli\u003eS curve to increase contrast\u003c/li\u003e\n\u003cli\u003eTips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis post will update frequently.\u003c/p\u003e","title":"Lightroom Notes"},{"content":"Recently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them. Virtualization How virtualization work Virtualization is the technology that allows a single physical machine, known as the host, to run multiple virtual machines or guests.\nThe key of virtualization is hypervisor. It\u0026rsquo;s the software that create and manages the virtual machines.\nType 1 (Bare-Metal): This hypervisor is installed directly onto the host\u0026rsquo;s hardware, acting as the operating system itself. Examples include VMware vSphere, Hyper-V. This type is common in data centers due to its high performance and efficiency Type 2 (Hosted): This hypervisor runs as an application on top of a conventional operating system(Like Windows, Linux). Examples include VMware Workstaion and Oracle VirtualBox. This approach is often used for desktop virtualization and development purposes. Once the hypervisor is in place, you can create one or more VMs. This involves\nAllocating resources(CPU cores, RAM, storage) Configuring virtual hardware(virtual network adapter, virtual storage controllers, virtual BIOS) Installing a guest OS, the guest os is unaware that it\u0026rsquo;s running in a virtualized environment. How WSL work: WSL 1: It did not run a real Linux kernel. Instead, it functioned as a real-time translation layer. It tricked Linux binaries into thinking they were communicating with a Linux kernel, when in reality, they were talking to a clever interpreter connected to the Windows kernel. WSL 2: Due to the limitations of translating every single Linux syscall, WSL 2 uses a lightweight, highly optimized type 1 virtual machine. Use case Server consolidation and optimization Development and testing envionments Application isolation and legacy application support Pros and Cons Pros:\nStrong isolation Total compatibility Cons:\nHeavyweight and slow Inefficient Containerization Containerization works by virtualizing the operating system, allowing an application to run in an isolated user space with all its dependencies, code and libraries. It all runs on a single host operating system and shared host OS\u0026rsquo;s kernel, making containers incredibly lightweight and fast.\nHow containers work Containers work by creating isolated environments for applications using two key technologies built into the host OS\u0026rsquo;s kernel:\nNamespaces: This feature provides isolation. Each container gets its own isolated view of resources like the network stack, process IDs, and filesystem mounts. This prevents conbtainers from seeing or interacting with each other or the host system. Control Groups(cgroups): This feature manages resource allocation. It limits and monitors how much of the host\u0026rsquo;s physical resources, such as CPU, RAM, I/O, each container can consume. This ensures no single container can monopolize the host\u0026rsquo;s resources. How container engine work Rule: A tool to create, run and manage container. It\u0026rsquo;s the translator/project manager between user and host\u0026rsquo;s OS kernel. Workflow: Creating a Dockerfile: A developer creates a text file defining all the steps required to build the application environment. Building an Image: The container engine reads the Dockerfile and packages it into a single, read-only, standardized image. This image serves as a static template for the container. Running a Container: The container engine uses this image to launch one or more container instances. During runtime, it creates an isolated namespace cgroups, while also adding a writable layer on top of the image to make the applications runnable. Docker vs Kubernetes vs Podman These are container enginers, but they serve very different purposes.\nDocker: The industry standard, it provides an all-in-one platform that includes a daemon, a client and an image registry(Docker hub). It\u0026rsquo;s easy to get started with and has a mature ecosystem Podman: A more security, daemonless alternative. Its command line is highly compatible with Docker\u0026rsquo;s, its run as a non-root user by default, and its architecture is more streamlined. Kubernetes: When applications scale up and need to run across hundreds or thousands of containers and multiple servers, Kubernetes is needed. It is a container orchestrator and servers as the brain of a container cluster. Kubernetes is not responsible for the actual running of containers. Instead, it handles higher-level management tasks such as automated deployment, elastic scaling, service discovery, load balancing and self healing. How distrobox works Distrobox is a clever tool that uses a container negine like Podman to create tightly integrated development environments. Its main purpose is to let you run any Linux distribution inside a container on your host OS but make it feel completely native.\nUse case Breaking down large, monolithic applications into smaller, independently deployable services Creating consistent and reproducible environments for building, testing and deploying software. Pros and Cons Pros\nLightweight and fast Highly portable Cons\nWeaker isolation You can only run containers that are compatible with the host OS kernel. Sandbox A sandbox is a secure, isolated environment on a computer where you can run untrusted program without risking harm to your host system.\nHow to create a sandbox Container Virtual machine Using dedicated sandbox software(like Sandboxie-Plus) Use case To be listed on the App Store or Google play store, and application must run in a sandbox. That\u0026rsquo;s why when we open a new app, the user have t approve a list of resources that enable the app to access. Education Browser plugin Pros and Cons Pros\nExtremely lightweight and fast: A sandbox applies rules to an already running process, adding minimal overhead. It\u0026rsquo;s instantaneous Targeted security: It\u0026rsquo;s perfect for its narrow purpose: running a single untrusted application and preventing it from accessing your personal files, network or hardware. Cons\nWeakest Isolation: It shares the host OS and kernel. Limited scape: It\u0026rsquo;s purely a security feature, not a deployment or development tool. ","permalink":"http://localhost:1313/posts/container_sandbox_vm/","summary":"\u003cp\u003eRecently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them.\n\u003cimg loading=\"lazy\" src=\"/Interesting_thing/distrobox.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"virtualization\"\u003eVirtualization\u003c/h2\u003e\n\u003ch4 id=\"how-virtualization-work\"\u003eHow virtualization work\u003c/h4\u003e\n\u003cp\u003eVirtualization is the technology that allows a single physical machine, known as the \u003cstrong\u003ehost\u003c/strong\u003e, to run multiple virtual machines or \u003cstrong\u003eguests\u003c/strong\u003e.\u003c/p\u003e","title":"Virtual machine vs Container vs Sandbox"},{"content":"The Global Interpreter Lock (GIL) At its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code. It is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment. The primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\nPython\u0026rsquo;s Memory Management Python uses a system called automatic memory management. Each object in Python has a reference count, which is a number that keeps track of how many variables or other object refer to it. When you assign a variable to an object, its reference count increases by one. When a reference is removed (for instance, when a variable goes out of scope), the count decreases. Once an object\u0026rsquo;s reference count drops to zero, it means nothing is using it.\nThis is where GIL becomes important. In a multi-threaded program, multiple threads could try to increase or decrease the same object\u0026rsquo;s reference count simultaneously, which may cause memory leaks.\nPython 3.13 GIL has become a bottleneck for CPU program performance, by reconstruct python, the development team has made a groundbreaking change in Python 3.13: the ability to disable the GIL.\nWhy python slow GIL Dynamic Datatype. It\u0026rsquo;s an interpreter language Easy to read makes python very abstract Trivias Generating functions are a powerful mathematical tool that transform discrete sequences into algebraic functions, enabling us to solve complex combinatorial counting problems through function operations. GPOS vs RTOS General purpose operating systems like Windows, macOS, and Linux are designed to provide a versatile, user-friendly computing environment. They prioritize overall system efficiency, with more flexible restrictions on task response times. Real time operating systems such as VxWorks, focus on strict time limitations for individual tasks, ensuring predictable and deterministic responses. These systems are typically used in environments demanding high reliability and precise time control, featuring a smaller, more streamlined kernel that guarantees real-time performance. In python, Boolean type is essentially a subclass of the integer type, True == 1 and False == 0. This design is mainly due to historical reasons and pragmatic considerations. In early version of Python(before 2.3), there was no dedicated bool type, and people used integers 1 and 0 to represent true and false. When the bool type was introuced, in order to allow old code to continue to run seamlessly, it was the best choice to continue to design it as a subclass of int. True + 1 = 2, False * 1 = 0 Huffman coding, an algorithm to compress data losslessly. Huffman coding assigns variable-length binary codes to input symbols. More frequent symbols -\u0026gt; shorter codes Less frequent symbols -\u0026gt; longer codes Windows vs Linux Aspect Linux Windows System Usage Lightweight, minimal background processes Heavy, many preloaded services and features Bloatware No pre-installed junk, user chooses all Comes with many default apps and features Transparency Fully open, config files are plain text Many hidden processes, registry-based config Customizability Highly customizable, from kernel to GUI Limited customization without hacking Privacy \u0026amp; Security User-controlled, minimal telemetry Sends telemetry, often needs antivirus Software Support Great for dev tools, less for gaming Excellent app/game compatibility System Control Full control over system and services Some restrictions, frequent auto-updates Hardware Support Good but sometimes manual setup required Plug-and-play for most consumer hardware Resouces Why the formular of normal distribution has a pi:explain from 3b1b Talks æ¼«å£«ï¼\n","permalink":"http://localhost:1313/posts/techweekly/techweek5/","summary":"\u003ch2 id=\"the-global-interpreter-lock-gil\"\u003eThe Global Interpreter Lock (GIL)\u003c/h2\u003e\n\u003cp\u003eAt its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code.\nIt is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment.\nThe primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\u003c/p\u003e","title":"Some Python Notes | King Weekly"},{"content":"Bitcoin The legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\nThe Blockchainâ›“ï¸ The blockchain is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\nIt\u0026rsquo;s a Chain of Blocks: Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block. It\u0026rsquo;s Immutable: Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending. The Genesis BlockğŸ“œ The very first block, known as the Genesis Block, was mined on 2009.01.04, by Bitcoin\u0026rsquo;s mysterious creator, Satoshi Nakamoto. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\nMiningâ›ï¸ Mining is the process of creating new blocks. It\u0026rsquo;s a competitive race that serves two critical purposes:\nValidating Transactions: Miners group pending transactions into a new block. Creating New Bitcoin: The winner of the race is rewarded with new bitcoin. Hereâ€™s how a miner wins the race and proves their block is valid:\nThe Hashing Puzzle: Miners take the data in their block and use their computers to find a specific number called a nonce. When the block data and the nonce are combined and put through a cryptographic function (SHA-256), they produce a unique digital fingerprint called a hash. The \u0026ldquo;Lower Than\u0026rdquo; Rule: To win, a miner must find a hash that is lower than the current network \u0026ldquo;target\u0026rdquo;. This target is a very large number that the entire network agrees on. Finding a hash below this target is incredibly difficult and requires immense computational powerâ€”it\u0026rsquo;s like trying to win a global lottery every 10 minutes. The Reward and The Halving: The first miner to find a valid hash wins the block reward. Initially, the reward was 50 BTC. However, this reward is programmed to cut in half roughly every four years (or 210,000 blocks) in an event called the halving. As of the April 2024 halving, the reward is now 3.125 BTC. This mechanism controls the supply of new bitcoin, making the currency scarce and ensuring its total amount will never exceed 21 million coins. When a winning block is found, its hash is broadcast across the P2P network. All other participants quickly verify that the hash is valid. Once confirmed, they add the new block to their copy of the blockchain and immediately start competing to solve the next block.\nHow to Mine: There are a few ways to participate in Bitcoin mining, each with its own pros and cons.\nSolo Mining: This is you, on your own, trying to solve a block. If you succeed, you get the entire block reward (3.125 BTC + transaction fees). However, the odds of a single person solving a block today are astronomically low due to the immense competition. It\u0026rsquo;s like buying a single lottery ticket and hoping to win the grand prize. Mining Pool: This is the most common method. You join a \u0026ldquo;pool\u0026rdquo; with thousands of other miners, combining your computing power. The pool works together to find blocks much more frequently. When the pool wins, the reward is split among all participants based on how much computing power they contributed. This provides smaller, but much more consistent and predictable, payouts. Block ForksğŸ´ A fork happens when the blockchain temporarily or permanently splits into two different paths.\nAccidental Fork: Sometimes, two miners find a valid block at almost the exact same time. The network briefly splits as some nodes follow one miner and some follow the other. This is usually resolved within a few minutes when the next block is found and added to one of the chains, making it the \u0026ldquo;longest\u0026rdquo; and therefore the official one. The shorter chain is then abandoned. Hard Fork: This is an intentional split that happens when the network\u0026rsquo;s software rules are changed in a way that is not backward-compatible. All participants must upgrade to the new rules to continue. If a significant portion of the community refuses to upgrade, the split becomes permanent, resulting in the creation of a new, separate cryptocurrency (e.g., Bitcoin Cash was created from a hard fork of Bitcoin). Ethics Mining bitcoin always consume immense energy, which critics view as a wasteful environmental cost for a seemingly meaningless computation. Proponents argue this mechanism decentralized financial system that offers freedom from the control of banks and governments.\nTools Calculate computing performance: https://www.nicehash.com/profitability-calculator\nThis is the computating power of my personal game laptop(one dollar per day hhh) Resources: Youtube channels\nhttps://www.youtube.com/watch?v=5hgdekVZb3A\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=3 https://www.youtube.com/watch?v=a41DMDfJjsU\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=2 https://Gemini.google.com Trivias Sherrington, coined the word \u0026ldquo;synapse\u0026rdquo; to define the connection between two neurons Two different roads of AI: connectionism and Symbolism. Hidden layer was firstly been implemented in Boltzmann machine, although Rosenblatt had some idea about multilayer perceptrons, but he didn\u0026rsquo;t find any useful training algorithm. Restricted Boltzmann machine - each layer is only allowed to be fully connectted to the next layer, current layer nodes are not connectted to each other. The advantages of this machine is it allows to update bodes within the same layer in parallel The invention of hidden layer allows model to understand abstract features. It also becomes to one of the most significant component in deep learning. The main difference between the Hopfield Network and the Boltzmann nachine is the presence of hidden layers. Other differences include the fact that the Hopfield network is deterministic, whereas the Boltzmann machine is stochastic, and the defintions of their energy function also differ. The fovea has many photoreceptors, with a high density of cones(for colors) and nearly no rods(for dark). This structure allows us to see the world clearly. If you develop myopia, the image formed after light is reflected by the eye may not be focused directly on the fovea. The idea of CNN was mainly inspired by the HMAX model(hierarchical, pooling, convolution), and the HMAX model was proposed by Tomaso Poggio, to simulate primate visual system, specifically ventral stream. Pytorch for research area, Tensorflow for industry and JAX for high level usage. Ethereum and ether is not the same as bitcoin. Ethereum has a longer vision, and the number of ether is unlimited. Resource Documentary of AlphaGo: https://www.youtube.com/watch?v=WXuK6gekU1Y How to use hugging face: https://www.youtube.com/watch?v=3kRB2TXewus This is the most comprehensive guide for AI beginner I had ever seen: Guide link ","permalink":"http://localhost:1313/posts/techweekly/techweek4/","summary":"\u003ch2 id=\"bitcoin\"\u003eBitcoin\u003c/h2\u003e\n\u003cp\u003eThe legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-blockchain\"\u003eThe Blockchainâ›“ï¸\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eblockchain\u003c/strong\u003e is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s a Chain of Blocks:\u003c/strong\u003e Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s Immutable:\u003c/strong\u003e Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-genesis-block\"\u003eThe Genesis BlockğŸ“œ\u003c/h4\u003e\n\u003cp\u003eThe very first block, known as the \u003cstrong\u003eGenesis Block\u003c/strong\u003e, was mined on \u003cstrong\u003e2009.01.04\u003c/strong\u003e, by Bitcoin\u0026rsquo;s mysterious creator, \u003cstrong\u003eSatoshi Nakamoto\u003c/strong\u003e. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\u003c/p\u003e","title":"Bitcoin Basic | King Weekly"},{"content":"2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning. However, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\nRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually. And before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation. So today I will record this moment.\nHopfield Network was invented at 1982. Processor John Hopfield designed it based on ideas from statistical mechanics.\nThe graph above shows two states of a ball From the left part, we can see the energy system of the ball is at the highest, which means the ball is very unstable From the right part, we can see the ball had already fall into the bottom, which means the ball is very stable. Although it is a classical physics model, but it definitly explain the main idea of hopfield network.\nSo now we can say hopfield network is just make a system move from an unstable state to stable state.\nIf you interested how exactly hopfield network work. See the video below, it\u0026rsquo;s pretty nice.\nThis idea is also useful in today. In LLM, we can say the user\u0026rsquo;s prompt and question is the most unstable states while the answer is the stable state.\nBased on the concept of Hopfield networks, many different architectures had been invented, which makes Connectionism and Deep Learning great again.\nAfter all, I catch the reason why nobel prize gives to physics.\n","permalink":"http://localhost:1313/posts/hopfieldnetwork/","summary":"\u003cp\u003e2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning.\nHowever, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\u003c/p\u003e\n\u003cp\u003eRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually.\nAnd before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation.\nSo today I will record this moment.\u003c/p\u003e","title":"Hopfield Network"},{"content":"Book - Unix: A History and a Memoir Recently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world. During the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\nAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals. The system developed before Unix was called Multics. Since â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€ Unix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10. Tools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs. Unix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems. GNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source. MacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX. In the early days, operating systems were not portable. This changed with the invention of the C language and its compiler. MINIX was widely used because it was embedded in Intel chips. The working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation. Microsoft once had its 3own Unix-like system. Another completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows. You can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan. â€œEverything is a fileâ€ is one of the core principles of Unix. The KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy. The UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\nKeep it simple stupid Do one thing, and do it well Everything is a file Make each program a filter Fail loudly Modularity Prototyping early In today, many barriers had already been removed.\nAnd I realise.\nThe revolution of AI is just like the reenactment of Unix\u0026rsquo;s development.\nSo.\nKISS.\nTrivias tty - TeleTYpewriter, terminal in the old time, before lcd screen been invented. UNIX was developed on the PDP-7, a computer with no screen, no mouse and only 8KB of RAM. It weighted nearly 500kg. UNIX and UNIX-like system use abbreviated commands because typing on TTY terminals in the 1960s was slow and insufficient. Second-system effect: It believes that after completing a small, elegant, and successful system, people tend to have overly high expectations for the next projects, which may lead to the creation of a huge, feature-rich but monstrous system.The \u0026ldquo;second-system effect\u0026rdquo; can result in software project plans being overdesigned, with too many variables and excessive complexity, ultimately falling short of expectations and leading to failure. such as PL/I in Multics Fortran(formular translation): The purpose of this lagnauge is to proceed mathematics formular and float number in an efficient way, like integration, linear algebra. That\u0026rsquo;s why fortran is still popular in some supercomputer and scientific calculation. B lagnauge is designed in a bit-unit computer PCP-7, where C langauge is designed in a byte-unit computer. Therefore the main difference between B and C is B langauge doesn\u0026rsquo;t support types and C does. Development of Clang: PL/I -\u0026gt; BCPL -\u0026gt; B -\u0026gt; New B(C) If computers using the same cpu architecture, they will using the same assembly language. The grep command is used to find lines that match a specific pattern in a file while the sed command is used to insert, replace and delete text from a file. Finally, the awk command supports programming logic and is often used for advanced data processing tasks. The development of UNIX from 1970 until now. Talks \u0026ldquo;A new technological discovery is often discredited by older generations of professionals - especially those with high authority and prestige in the existing field - in order to protect their own status\u0026rdquo; ","permalink":"http://localhost:1313/posts/techweekly/techweek3/","summary":"\u003ch2 id=\"book---unix-a-history-and-a-memoir\"\u003eBook - Unix: A History and a Memoir\u003c/h2\u003e\n\u003cp\u003eRecently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world.\nDuring the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals.\u003c/li\u003e\n\u003cli\u003eThe system developed before Unix was called Multics.\u003c/li\u003e\n\u003cli\u003eSince â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€\u003c/li\u003e\n\u003cli\u003eUnix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10.\u003c/li\u003e\n\u003cli\u003eTools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs.\u003c/li\u003e\n\u003cli\u003eUnix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems.\u003c/li\u003e\n\u003cli\u003eGNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source.\u003c/li\u003e\n\u003cli\u003eMacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX.\u003c/li\u003e\n\u003cli\u003eIn the early days, operating systems were not portable. This changed with the invention of the C language and its compiler.\u003c/li\u003e\n\u003cli\u003eMINIX was widely used because it was embedded in Intel chips.\u003c/li\u003e\n\u003cli\u003eThe working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation.\u003c/li\u003e\n\u003cli\u003eMicrosoft once had its 3own Unix-like system.\u003c/li\u003e\n\u003cli\u003eAnother completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows.\u003c/li\u003e\n\u003cli\u003eYou can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan.\u003c/li\u003e\n\u003cli\u003eâ€œEverything is a fileâ€ is one of the core principles of Unix.\u003c/li\u003e\n\u003cli\u003eThe KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\u003c/p\u003e","title":"For UNIX Week | King Weekly"},{"content":"Model Context Protocol overview MCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\nWhy is MCP needed? In the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces). The current components of MCP servers include: How does MCP work? Example: Suppose you\u0026rsquo;re using an AI assistant to manage your work, and it wants to help you schedule today\u0026rsquo;s meeting:\nWithout MCP, developers would need to write separate integration code for Outlook, Google Calendar, and Apple Calendar.\nWith MCP, the AI only needs to call the MCP server, which will automatically interface with your calendar system. No matter which calendar service you use, the AI can work seamlessly.\nCore Functions of MCP: Reducing development costs (no need to develop separate integrations for each tool).\nEnhancing AI\u0026rsquo;s ability to access external data (allowing AI to easily query and manipulate external data).\nStandardizing communication (making communication between different AI applications and tools smoother).\nYou can think of MCP as the \u0026ldquo;USB interface for AI\u0026rdquo;â€”any AI device can plug into different tools without needing to individually adapt to each one!\nTrivias: Graph is a great data structure, it can be used to find the shortest path, solve a magic cube. Rely on this data structure, Human find the shortest steps to solve the worst case configured 3\\*3\\*3 magic cube in 20 steps (which also called god\u0026rsquo;s number) and 11 steps for 2\\*2\\*2 one. What about n*n*n, Here is an interesing paper about time compleixity of solving a n*n*n magic cube: Algorithms for Solving Rubikâ€™s Cubes Topological sort is an algorithm based on DFS and DAG, It\u0026rsquo;s not a traditional sorting algorithm, like comparing the size of each number and sort them, but sorting based on dependencies. Dynamic programming is just like recursion + memorization + guessing P vs NP problems: P is a problem that can be solved easily by a computer, NP is a problem that you can check for correctness very easily once solved. However, P != NP for example wec can\u0026rsquo;t engineer luck. Also, NP hard means it is at least as hard as any problem in NP, and NP-complete is lke if you can solve one NP-complete question, you can solve all NP question. Reduction is like to prove a known NP-complete question, and transfer it into a NP question X, then X is also a NP-complete question HTTP vs REST API HTTP is a protocol, and its core task is to define how to request and transfer data between your broswer and server\nIt focuses on the data exchange level, regardless of whether the data is an image, text, video, or API response. For example, you can use an HTTP request to access a webpage (HTML page) or use an HTTP request to retrieve API data (JSON data). It doesnâ€™t care about what data you\u0026rsquo;re transmitting. REST API is a design style, based on the HTTP protocol, with rules and best practices:\nIt views content on the server as \u0026ldquo;resources\u0026rdquo; and specifies how to operate on them using HTTP methods (GET, POST, PUT, DELETE). REST API design considers resource orientation: Resources (like users, articles, comments, etc.) have unique identifiers (URIs), and clients interact with these resources through HTTP requests. For example, in a REST API, you can:\nGET request: Retrieve a resource (e.g., get movie information). POST request: Create a new resource (e.g., submit a new comment). PUT request: Update a resource (e.g., modify user information). DELETE request: Delete a resource (e.g., remove an article). Resource: A tool to build a personal streaming music player: navidrome A great blog that introduce human visualization: human visualization Five basic algorithms explanation: Dynamic programming Greedy Algorithm Backtracking Branch and Bounding This week, Nvidia GTC 2025 brings a lot of new tech, PC in AI age, B300, new architecture, robots. After watched the coverage keynotes, I was just feel like our mankind is in an special stage with unpredictable pace. In science fiction films, at this point , it often leads to the arrival of an alien civilization. A dijkstra algorithm visualiser that helps me understand it: Dijkstra shortest path MCP explain: A blog that explain MCP crystal clear ","permalink":"http://localhost:1313/posts/techweekly/techweek2/","summary":"\u003ch2 id=\"model-context-protocol-overview\"\u003eModel Context Protocol overview\u003c/h2\u003e\n\u003cp\u003eMCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\u003c/p\u003e\n\u003ch4 id=\"why-is-mcp-needed\"\u003eWhy is MCP needed?\u003c/h4\u003e\n\u003cp\u003eIn the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\u003cbr\u003e\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces).\n\u003cimg alt=\"components mcp include\" loading=\"lazy\" src=\"/TechStuff/mcp.png\"\u003e\u003c/p\u003e","title":"Jarvis Will Coming Soon | King Weekly"},{"content":"Email sending and receiving system The main system is build based on three protocols: SMTP, POP3 and IMAP. SMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails. User1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\nPOP3 downloads emails from the email server to the email client. By default, it removes emails from the server after downloading, but some email clients allow users to keep copies on the server. IMAP keeps emails on the server and synchronizes them across multiple devices. The email client initially loads only the headers, and the full email content is fetched from the server when the user opens it.\nIf you want to customize an email domain. You need to have your own SMTP and IMAP/POP3 server and a domain, the other steps are the same as above.\nPKGBUILD in Arch Linux PKGBUILD is a bash script contain the build information required by archlinux package we use makepkg script to build the package, it will search PKGBUILD first in the current folder. Benefits:\nusing pacman to manage, user can update and uninstall easily some pkgbuild file include the commands to generate a binary file and store it in /user/bin Drawbacks:\nNot friendly to starter Although we can use yay to help us do all these stuff.\nTrivias Newton\u0026rsquo;s method is quadratic convergence when we want to calculate the root of a number Catalan number is a group of sequence that appear widely in combinatorics. e.g. ways to arrange n brackets, number of triangles in an n+2 convex polygon. The common property of these applications is that they are recursive and have a constrained structure. Toom cook: It divide a d-digit number into n parts and doing arithmatic calculations. Schonhage-strassen scheme: It multiplies two integers of length ğ‘› in O (ğ‘› logğ‘› log logğ‘›) steps on a multitape Turing machine A Naive algorithm is usually the most obvious solution when one is asked a problem. It may not be a smart algorithm but will probably get the job done The taste of red wine is determined by acidity, sweetness, alcohol content, tannins, and body. Wines are categorized into New World and Old World. New World wines (from countries like the USA, Chile, Argentina, and China) are named after the grape variety, while Old World wines (mainly from Europe) are named after their place of origin.\nRed wine is made by fermenting red grapes with their skins. White wine is made from either white grapes or red grapes without their skins. RosÃ© wine is made by soaking the grape skins briefly but fermenting without them. Sparkling wine undergoes a second fermentation to produce bubbles. Gabriel\u0026rsquo;s horn is a type of geometric figure that has infinite surface area but finite volume. Resources Code question(leetcode), system design question(crack the code interview), teamwork, communication are all important in the interview. An old guideline to learn ai: https://www.captainai.net/itcoke/ A guideline to learn CS: https://csdiy.wiki/åè®°/ Useful tips to integrate by parts, åå¯¹å¹‚æŒ‡ä¸‰, to choose u. Customize your zsh: oh my zsh Xiaomi releases a concept modular camera, it looks pretty awesome and innovative. 3b1b\u0026rsquo;s taylor series explaination:https://www.youtube.com/watch?v=3d6DsjIBzJ4 3b1b\u0026rsquo;s explaination of why we have exponential e:https://www.youtube.com/watch?v=m2MIpDrF7Es This website is all about competitive writing of source code that is as short as possible: Codewolf Explaination of greedy algorithm: greedy algorithm Explaination of dynamic programming: dynamic programming Deploy perosonal VPN tools: tailscale Abstract If no one is reading blogs anymore, why should we write them? Letâ€™s make it simple: you write a blog, but nobody cares, nobody reads it. At least, the number of readers is not as many as you thought. You put your personal ideas and thoughts into the article, carefully structuring each sentence, and choose a great imageâ€”then, no response, no likes, no shares, no activity. So, what is the meaning of writing a blog? First, there are two misconceptions about blogging. One is that if I write a good article, readers will come naturally. No, they wonâ€™t come. There are billions of blogs on the internet, like a massive hurricane, and yours is just a single leaf in the wind. Who would notice? Another misconception is that if nobody reads it, writing is a waste of time. Blogs have their own hidden value. You write blogs not for the applause of others, but for your own needs. Blogs help clear your mind. They help you organize your thoughts and sharpen your perspective. When you think better, you will achieve better results. The target audience of a blog is actually not the people on the internet, but your future self. Your article will help you see the evolution of your own thoughts. Additionally, one day in the future, someone who truly needs your article will find it. A deep, thoughtful article has a longer-lasting impact than a viral article. Writing a blog is quite like street photography. You take your camera and walk through the city. You see a sceneâ€”a moment filled with light, shadow, and humanityâ€”and then you capture it. Nobody cares about what you actually captured. But thatâ€™s not the reason you photograph; you photograph because you see something interesting. Writing a blog is the same. You write a blog because you are thinking, observing new things, and hope to store them somewhere. If someone reads it, that\u0026rsquo;s great. If not, youâ€™ve still completed your work\n","permalink":"http://localhost:1313/posts/techweekly/techweek1/","summary":"\u003ch2 id=\"email-sending-and-receiving-system\"\u003eEmail sending and receiving system\u003c/h2\u003e\n\u003cp\u003eThe main system is build based on three protocols: SMTP, POP3 and IMAP.\n\u003cimg alt=\"process of email system\" loading=\"lazy\" src=\"/emailsys.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails.\nUser1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\u003c/p\u003e","title":"Start | King Weekly"},{"content":"In this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O O(n) notation before, but for Big theta Î¸(n) and reccurence relations T(n), I never heard them before. Today, I hope I can finally figure them out.\nWhat T(n) represents the actual running time of an algorithm\nO(n) represents the asymptotic upper bound of the running time of an algorithm\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\nHow to convert the three of them Normally we can directly transfer T(n) to O(n) or Î¸(n). Int sum = 0 for (i = 1; i \u0026lt;= n, i ++) { sum = sum + i } This is a classical example, First of all, we initialize variable sum requires one unit of running time. There are three statements inside the for loop, statement 1 i = 1 requires one unit of the running time, statement 2 i \u0026lt;= n requires n+1 units of the running time, statement 3 i ++ requires n units of the runningn time, and sum = sum + i requires 2n units of the running time, n for addition and n for assignment. Therefore T(n) = 1+1+(n+1)+n+2n = 4n + 3. In O(n), we ignore the constant and the lower-order terms, therefore the time complexity is O(n) / Î¸(n).\nWhen the algorithm is a recursion, such as karatsuba multiplication and high precision multiplication. There are two methods to convert T(n) into O(n)\nRecursion tree method According to Recursion tree method, we derive master theorem The time complexity of multiplication is equal to the time complexity of division\n","permalink":"http://localhost:1313/posts/the-difference-between-tn--on-and-%CE%B8n/","summary":"\u003cp\u003eIn this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O \u003ccode\u003eO(n)\u003c/code\u003e notation before, but for Big theta \u003ccode\u003eÎ¸(n)\u003c/code\u003e and reccurence relations \u003ccode\u003eT(n)\u003c/code\u003e, I never heard them before. Today, I hope I can finally figure them out.\u003c/p\u003e\n\u003ch1 id=\"what\"\u003eWhat\u003c/h1\u003e\n\u003cp\u003eT(n) represents the actual running time of an algorithm\u003cbr\u003e\nO(n) represents the asymptotic upper bound of the running time of an algorithm\u003cbr\u003e\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\u003c/p\u003e","title":"Time Complexity Notations"},{"content":"This week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\nOverview The machine model is HP-Elitedesk-800-G4-SFF. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\nIf you want to learn more here is the machine datasheet:server_datasheet\nHardware Motherboard: Q370 viewer CPU: i5-8500 GPU: intel UHD 630 RAM: 16G SSD: 256G HDD: 500G * 1 The motherboard provides a high flexibility to expand more internal storage, also it has 4 PCie expansion slots which can used to expand more storage space or other components you want.\nSoftware nextcloud\nemail domain\ngitlab\nminecraft server\ndocker\njellyin\nsynthing\nproxy?router?gateway?\nvirtual machine\nI host my server with ubuntu server distro. The reason I didn\u0026rsquo;t choose proxmox is because I want to learn server step by step, proxmox is great in visualization, maybe in the future, I will try it.\nDurign the process of configuring storage, I learned LVM, which is a wonderful tool for those users that has multiple drives. User can create a storage pool called volume group. Firstly, user add their physical volumes into volumn group, and we create logical volumn based on the storage area had in volumn groups, and then we mount those LVs with the actural dirctory. It seems like Windows is not able to achieve this function. For RAID, we seperate it into 4 different categories, radi0, raid1, raid5 and raid10. This tools shows how to save files in different number of drives or in LVM.\nFor external access, I plan to use cloudflare tunnel. They provide such service, I need to buy a domain name and combine it with the cloudflare tunnel, and when I access the server, I firstly type the domain name in my browser to ask cloudflare, and they will guide me to the tunnel to my server, also in server end, I need to install cloudflared docker image as an end, then it works! It\u0026rsquo;s so convenient for those people who live in school accommodation. And it\u0026rsquo;s totally free!\nIn my network configuration. I didn\u0026rsquo;t install a router, but the best choice is to use a router for all devices in my home, and assign each of them an static IP address. To access the server, I bought a portable monitor, since the IP address of my server using DHCP, which required to check the IP address manually when the machine reboot or close. This issue will be solved easily when I have a router. Nowadays, I need to change the tunnels configuration to enable external access.\nsince I have 3 different operating systems in my three daily devices, phone for android, laptop for arachlinux and ipad for ipados, I installed nextcloud docker to try to integrate them into one ecosystem, that pretty awesome.\nUpdate Nextcloud for cloud storage Able to build game server Music and video live streaming server. Future Mining bitcoin ","permalink":"http://localhost:1313/posts/homeserver/","summary":"\u003cp\u003eThis week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThe machine model is \u003ccode\u003eHP-Elitedesk-800-G4-SFF\u003c/code\u003e. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\u003cbr\u003e\n\u003cimg loading=\"lazy\" src=\"/TechStuff/serverbill.png\"\u003e\u003c/p\u003e","title":"My Homelab"},{"content":"This semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\nIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\nHow the JVM works The JVM executes er programs in several stages:\nCompilation: Java source code (.java) is compiled into bytecode (.class) by the Java Compiler (javac).\nClass Loading: The JVM loads compiled bytecode when required.\nBytecode Verification: Ensures security and correctness before execution.\nExecution: The JVM interprets or compiles bytecode using Just-In-Time (JIT) compilation.\nGarbage Collection: The JVM automatically manages memory, reclaiming unused objects.\nClass loader One of the organizational units of JVM byte code is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.\nLet\u0026rsquo;s explore this concept with an example: Imagine you\u0026rsquo;re watching a movie on a streaming service.\nLoading:\nThe service first finds and imports the movie data you want to watch. Similarly, the class loader locates the binary data for a Java class. Linking: Verification: Before you watch the movie, the service checks that the file isn\u0026rsquo;t corrupted. In Java, the class loader verifies the correctness of the class. Preparation: The service sets up the necessary space in memory to buffer the movie. Java allocates memory for variables and sets default values. Resolution: The service ensures all necessary subtitles or audio tracks are ready to play. Java resolves references to make them direct. Initialization:\nAs you start watching, the service begins playing the movie. Similarly, Java runs code to set up variables with their starting values. Class Loader Types:\nBootstrap Class Loader: Like the service\u0026rsquo;s core library of well-known movies, it loads fundamental, trusted classes. Extension Class Loader: Similar to special add-on features, it loads additional classes outside the core library. System/Application Class Loader: Like searching for new releases or user-uploaded content, it loads classes specific to the application youâ€™re using. Virtual machine architecture Cross-Platform Compatibility and Limitations The JVM abstracts away the underlying hardware and operating system specifics, allowing Java bytecode to run on any device equipped with a compatible JVM. This cross-platform capability greatly simplified software distribution and development, as developers could write code once and deploy it across various platforms without modification.\nDespite its strong cross-platform capabilities, the JVM faced significant challenges due to competitive corporate strategies, particularly the \u0026ldquo;Embrace, Extend, and Extinguish\u0026rdquo; (EEE) approach adopted by some companies like Microsoft in the late 1990s. This strategy involved embracing a technology, extending it with proprietary features, and eventually using those extensions to undermine the original technology.\nMicrosoft initially embraced Java by integrating it into their Internet Explorer browser and Windows platforms. However, they extended Java with proprietary features that were specific to Windows, creating a version of Java that was incompatible with the standard JVM specifications set by Sun Microsystems. This move fragmented the Java platform and undermined the \u0026ldquo;Write Once, Run Anywhere\u0026rdquo; philosophy.\nWith the development and rise of programming languages like Swift, Kotlin, and JavaScript, the JVM faced significant challenges in maintaining its performance edge. Swift, designed by Apple for iOS and macOS platforms, offers high performance and safety due to its compiled nature and modern language features. Kotlin, although initially running on the JVM, introduced concise syntax and advanced features that surpassed Java in many ways, leading it to become the preferred language for Android development. JavaScript\u0026rsquo;s performance greatly improved with engines like V8, and its versatility expanded through technologies like Node.js for server-side development. These languages not only matched but often exceeded the JVM\u0026rsquo;s performance and adaptability in their respective domains, leading to a shift in developer preferences and a relative decline in the JVM\u0026rsquo;s dominance.\nOverall Nowadays, Java has become to a normal programming lauguage. And the question is obvious solved. Python has its own interpreter to transfer the source code into machine code. Haskell has its own compiler, C++/C are compiled directly into machine code. However,they can\u0026rsquo;t generate a compiled file that enable to run in every operating system. If there is no EEE strategy, Linux may have a stronger effects in today\u0026rsquo;s world.\n","permalink":"http://localhost:1313/posts/jvm/","summary":"\u003cp\u003eThis semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\u003c/p\u003e","title":"JVM's Achievements and Limitations"},{"content":" ğŸ¤— ","permalink":"http://localhost:1313/aboutme/","summary":"aboutme","title":"About me"},{"content":"Location: Auchinstarry Quarry, Scotland Date: 2025.09.12 A great first-time rock climbing experience, meet interesting people and learned some rope skills.\n","permalink":"http://localhost:1313/aboutme/climbing/","summary":"\u003ch2 id=\"location-auchinstarry-quarry-scotland\"\u003eLocation: Auchinstarry Quarry, Scotland\u003c/h2\u003e\n\u003cp\u003eDate: 2025.09.12\n\u003cimg alt=\"rock and me\" loading=\"lazy\" src=\"/Aboutme/croy_rock_and_me.jpg\"\u003e\nA great first-time rock climbing experience, meet interesting people and learned some rope skills.\u003c/p\u003e","title":"Climbing road"},{"content":" A free web application that create a two-host daily podcast of the news that matters to you. Just add your favorite RSS feeds from news sites, blogs, and social media. Our app will automatically generate conversational scripts using an LLM and then produce a customized audio episode with text-to-speech. It\u0026rsquo;s the perfect way to stay updated on your commute.\nOfficial website\nCode available in github\nMotivation The idea of having a J.A.R.V.I.S. from the Marvel movies, is a common dream. With today\u0026rsquo;s advancements in LLM and TTS technology, we can now offer a real-world experience that captures a part of that dream.\nFor me, I love to start my mornings by checking my favorite blogs for updates, scrolling through X for new tech news, or quickly scanning the day\u0026rsquo;s headlines. With this application, I could simply wake up, click a button to generate my podcast, and then, while having breakfast or commuting, listen to a personalized summary of everything that happened overnight. If a specific story catches my interest, I can then go back and read the full article later. I think its an efficient way to stay informed.\nTech stack Area Technology Backend FastAPI, Python, SQLAlchemy, PostgreSQL, Pydantic Frontend React, Javascript, TypeScript, Vite, Tailwind CSS AI \u0026amp; TTS Google Gemini 2.5 Pro API Deployment Self-host(personal server), Docker Compose(Backend, Frontend, background worker, PostgreSQL, Valkey) AI assistant Gemini 2.5 pro, Claude sonnet 4 Issues I met Gemini api timeout: The current Gemini API has a 60-second request timeout, which is insufficient for generating a 10-minute podcast with a single text-to-speech (TTS) request. To solve this, my initial thought was to segment the entire podcast script into smaller parts and generate each segment separately. However, a new challenge arose with the TTS service\u0026rsquo;s free plan, which limits me to just 15 requests per day. This makes the segmentation approach impractical for daily use. My temporary fix has been to adjust the script prompt to generate shorter content, but this method is unreliable and doesn\u0026rsquo;t always prevent timeouts. The long-term solution will be to upgrade to a paid TTS service with a longer API request timeout. This will allow for single, uninterrupted podcast generation, solving the issue at its root. Roadmap 2025-09: Migrate deployment from Render to my personal server âœ… - 2025.09.21 2025-10: Add more social medias RSS feeds 2025-12: Multi-format input file support Future: Use fine-tuning TTS model Improve user experience Support spotify export Summary This project takes me 2 month from learning tech stacks to final outcome. I want to say AI really helped me a lot during the building process, whereas from learning tech stacks to fix bugs. From this prject, I experience both frontend and backend project building, frontend is just like building an artwprk, it really need me to keep patient, backend is more interesting since it relative to many different fields like crud operations, building database schemas and endpoints and connection between database, background worker. I also experience the magic of containerization, its so convenient. - 2025.08.14\n","permalink":"http://localhost:1313/aboutme/projects/lets_break_the_information_gap/","summary":"aboutme","title":"Let's break the information gap"},{"content":"ğŸ“· Welcome to my photography exhibition! ","permalink":"http://localhost:1313/aboutme/gallery/","summary":"\u003ch1 id=\"heading\"\u003eğŸ“·\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"welcome-to-my-photography-exhibition\"\u003eWelcome to my photography exhibition!\u003c/h1\u003e","title":"Photography exhibition by King Jin"},{"content":" Let\u0026rsquo;s break the information gap A free web application that create a two-host daily podcast of the news that matters to you. Reflex An AI-powered, English translator that generates a unique conversational story each day based on your search history. ","permalink":"http://localhost:1313/aboutme/projects/","summary":"\u003cpicture class=\"center\"\u003e\n  \u003c!-- Dark mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects_dark.png\" media=\"(prefers-color-scheme: dark)\"\u003e\n  \u003c!-- Light mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects.png\" media=\"(prefers-color-scheme: light)\"\u003e\n  \u003c!-- Fallback image --\u003e\n  \u003cimg src=\"/Aboutme/projects.png\" alt=\"Project image\"\u003e\n\u003c/picture\u003e\n\u003cbr\u003e\n\u003cbr\u003e\n\u003c!-- Project lbtig --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003eLet\u0026rsquo;s break the information gap\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003e\u003cimg src=\"/Aboutme/lbtig_preview.png\" class=\"center\"\u003e\u003c/a\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp class=\"center-wider\"\u003e A free web application that create a two-host daily podcast of the news that matters to you.\n\u003c/p\u003e\n\u003chr\u003e\n\u003cbr\u003e\n\u003c!-- Project reflex --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003eReflex\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003e\u003cimg src=\"/Aboutme/reflex_preview.png\" class=\"center\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cbr\u003e\n\u003cp class=\"center-wider\"\u003e An AI-powered, English translator that generates a unique conversational story each day based on your search history.\n\u003c/p\u003e","title":"Project by King Jin"},{"content":" An AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\ncode available in github Motivation I dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\nTech stack Frontend: HTML, Tailwind CSS, Electron, Javascript\nRoadmap Future: support multi-language data record system\nSummary This project takes me one week to build. To be honest, Electron is convenient for cross platform, but also it increase the size of the software.\n","permalink":"http://localhost:1313/aboutme/projects/reflex/","summary":"\u003cblockquote\u003e\n\u003cp\u003eAn AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Kingjinsight/Reflex\"\u003ecode available in github\u003c/a\u003e\n\u003cimg alt=\"reflex_dashboard\" loading=\"lazy\" src=\"/Aboutme/reflex_dashboard.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\u003c/p\u003e","title":"Reflex"},{"content":"I really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\nColor Theory. Additive Color: RGB for Red, Green and Blue. Red + Green + Blue = White Subtractive Color: These are created by mixing two additive colors. Red + Green = Yellow Green + Blue = Cyan Red + Blue = Magenta Yellow + Cyan + Magenta = Black Basics Histogram: a graph that displays the brightness and color distribution of an image From left to right, each column represents the number of the white-black pixels in different level It has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites. There are two triangles at the top left and top right. the top left one is shadow clipping which will show you whether your image has region that was too dark the top right one is highlight clipping which show you whether your image has region that was too light the reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0. There are several colors in the triangle gray - all details are well preserved red - red channel overexposure/underexposure green - green channel overexposure/underexposure blue - blue channel overexposure/underexposure yellow - yellow channel overexposure/underexposure cyan - cyan channel overexposure/underexposure magenta - magenta channel overexposure/underexposure solid white - RGB channel channel overexposure/underexposure in the same time White balance Temperature Tint Tone Exposure: change the overall brightness of the photo Contrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa. Highlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc. Shadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc. Whites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white. Blacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black. Presence Texture: Focus on the surface texture, such as skin pores, surface of rocks, etc. Clarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;. Dehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation. Vibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated. Saturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree. Tone Curve The square graph looks similar with histogram, but contain a line segment from bottom left to top right. We can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve. we can also pull RGB channel separately. S curve to increase contrast Tips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more. This post will update frequently.\n","permalink":"http://localhost:1313/posts/lightroom/","summary":"\u003cp\u003eI really love the feeling about using photo to represent some themes. Recently, I tried lightroom, which is a post-process software for photos. I learned many color theorys and tools behind it. Let\u0026rsquo;s have a look.\u003c/p\u003e\n\u003ch2 id=\"color-theory\"\u003eColor Theory.\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eAdditive Color: RGB for Red, Green and Blue.\n\u003cul\u003e\n\u003cli\u003eRed + Green + Blue = White\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSubtractive Color: These are created by mixing two additive colors.\n\u003cul\u003e\n\u003cli\u003eRed + Green = Yellow\u003c/li\u003e\n\u003cli\u003eGreen + Blue = Cyan\u003c/li\u003e\n\u003cli\u003eRed + Blue = Magenta\u003c/li\u003e\n\u003cli\u003eYellow + Cyan + Magenta = Black\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"basics\"\u003eBasics\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHistogram: a graph that displays the brightness and color distribution of an image\n\u003col\u003e\n\u003cli\u003eFrom left to right, each column represents the number of the white-black pixels in different level\u003c/li\u003e\n\u003cli\u003eIt has five different region, from left to right, they are blacks, shadows, exposure, highlights and whites.\u003c/li\u003e\n\u003cli\u003eThere are two triangles at the top left and top right.\n\u003col\u003e\n\u003cli\u003ethe top left one is shadow clipping which will show you whether your image has region that was too dark\u003c/li\u003e\n\u003cli\u003ethe top right one is highlight clipping which show you whether your image has region that was too light\u003c/li\u003e\n\u003cli\u003ethe reason using clipping is because we can\u0026rsquo;t represent a value of a pixel by the number of bits. For example, the most common bit to represent image is 8 bits(0-255), such as (0ï¼Œ128ï¼Œ 254). When the value over 255 or less than 0, the pixel will be break, and the information will be clipped to 255 and 0.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eThere are several colors in the triangle\n\u003col\u003e\n\u003cli\u003egray - all details are well preserved\u003c/li\u003e\n\u003cli\u003ered - red channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003egreen - green channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eblue - blue channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003eyellow - yellow channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003ecyan - cyan channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003emagenta - magenta channel overexposure/underexposure\u003c/li\u003e\n\u003cli\u003esolid white - RGB channel channel overexposure/underexposure in the same time\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eWhite balance\n\u003col\u003e\n\u003cli\u003eTemperature\u003c/li\u003e\n\u003cli\u003eTint\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eTone\n\u003col\u003e\n\u003cli\u003eExposure: change the overall brightness of the photo\u003c/li\u003e\n\u003cli\u003eContrast: Increase or decrease the difference between highlights and shadows in a photo. It can make highlights brighter and shadows darker or vice versa.\u003c/li\u003e\n\u003cli\u003eHighlights: Only the brighter areas in the photo are controlled, excluding the brightest pure white areas, such as cloud details in the sky, hightlights on the skins, etc.\u003c/li\u003e\n\u003cli\u003eShadows: Only the darker areas in the photo, but does not include the darkest pure black parts, such as the details of people in the shadows, the dark parts of buildings, etc.\u003c/li\u003e\n\u003cli\u003eWhites: Define the brightest point in photo, it controls the rightmost end of the brightness range and determines which parts of your image become pure white.\u003c/li\u003e\n\u003cli\u003eBlacks: Define the \u0026ldquo;darkest point\u0026rdquo; or \u0026ldquo;black point\u0026rdquo; of your photo. It controls the leftmost end of the brightness range and determines which parts of the image will become pure black.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePresence\n\u003col\u003e\n\u003cli\u003eTexture: Focus on the surface texture, such as skin pores, surface of rocks, etc.\u003c/li\u003e\n\u003cli\u003eClarity: å®ƒä¸åƒâ€œçº¹ç†â€é‚£ä¹ˆç²¾ç»†ï¼Œè€Œæ˜¯è®©ç‰©ä½“çš„è½®å»“å’Œç»“æ„æ˜¾å¾—æ›´â€œç¡¬æœ—â€æˆ–æ›´â€œæŸ”å’Œâ€ã€‚It isn\u0026rsquo;t as fine-detailed as \u0026ldquo;Texture\u0026rdquo;. Instead, it makes the outlines and structures of objects appear \u0026ldquo;harder\u0026rdquo; or \u0026ldquo;softer\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eDehaze: ä¸»è¦ç”¨äºæ¶ˆé™¤ç…§ç‰‡ä¸­çš„å¤§æ°”è–„é›¾ã€é›¾éœ¾æˆ–æœ¦èƒ§æ„Ÿï¼ŒåŒæ—¶å¢åŠ è‰²å½©çš„é¥±å’Œåº¦ã€‚It is primarily used to eliminate atmospheric haze, smog, or mist in a photo, while simultaneously increasing color saturation.\u003c/li\u003e\n\u003cli\u003eVibrance: å®ƒä¼šä¼˜å…ˆæå‡ç”»é¢ä¸­æœ¬èº«ä¸å¤ªé¥±å’Œçš„é¢œè‰²ï¼ˆæ¯”å¦‚å¤©ç©ºçš„è“è‰²ã€æ¤ç‰©çš„ç»¿è‰²ï¼‰ï¼Œè€Œå¯¹äºå·²ç»å¾ˆé¥±å’Œçš„é¢œè‰²åˆ™å½±å“è¾ƒå°. It selectively boosts the less saturated colors in an image (like the blue in the sky or the green in plants), while having a smaller impact on colors that are already highly saturated.\u003c/li\u003e\n\u003cli\u003eSaturation: ä¸€ä¸ªâ€œç®€å•ç²—æš´çš„â€è‰²å½©å¢å¼ºå·¥å…·ã€‚å®ƒä¼šå¯¹ç”»é¢ä¸­çš„æ‰€æœ‰é¢œè‰²è¿›è¡Œæ— å·®åˆ«çš„ã€åŒç­‰ç¨‹åº¦çš„æå‡ã€‚A \u0026ldquo;simple and heavy-handed\u0026rdquo; color enhancement tool. It boosts all colors in the image indiscriminately and to the same degree.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"tone-curve\"\u003eTone Curve\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eThe square graph looks similar with histogram, but contain a line segment from bottom left to top right.\u003c/li\u003e\n\u003cli\u003eWe can pull the curve to change tone in specific region, pull the curve left and up will make region lighter(add more RGB) and right down make region darker(add more CMYK) for point curve.\u003c/li\u003e\n\u003cli\u003ewe can also pull RGB channel separately.\u003c/li\u003e\n\u003cli\u003eS curve to increase contrast\u003c/li\u003e\n\u003cli\u003eTips: e.g. to increase Yellow, we can pull the blue-yellow curve to yellow more, or we can pull red-cyan to red more and green-magenta to green more.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis post will update frequently.\u003c/p\u003e","title":"Lightroom Notes"},{"content":"Recently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them. Virtualization How virtualization work Virtualization is the technology that allows a single physical machine, known as the host, to run multiple virtual machines or guests.\nThe key of virtualization is hypervisor. It\u0026rsquo;s the software that create and manages the virtual machines.\nType 1 (Bare-Metal): This hypervisor is installed directly onto the host\u0026rsquo;s hardware, acting as the operating system itself. Examples include VMware vSphere, Hyper-V. This type is common in data centers due to its high performance and efficiency Type 2 (Hosted): This hypervisor runs as an application on top of a conventional operating system(Like Windows, Linux). Examples include VMware Workstaion and Oracle VirtualBox. This approach is often used for desktop virtualization and development purposes. Once the hypervisor is in place, you can create one or more VMs. This involves\nAllocating resources(CPU cores, RAM, storage) Configuring virtual hardware(virtual network adapter, virtual storage controllers, virtual BIOS) Installing a guest OS, the guest os is unaware that it\u0026rsquo;s running in a virtualized environment. How WSL work: WSL 1: It did not run a real Linux kernel. Instead, it functioned as a real-time translation layer. It tricked Linux binaries into thinking they were communicating with a Linux kernel, when in reality, they were talking to a clever interpreter connected to the Windows kernel. WSL 2: Due to the limitations of translating every single Linux syscall, WSL 2 uses a lightweight, highly optimized type 1 virtual machine. Use case Server consolidation and optimization Development and testing envionments Application isolation and legacy application support Pros and Cons Pros:\nStrong isolation Total compatibility Cons:\nHeavyweight and slow Inefficient Containerization Containerization works by virtualizing the operating system, allowing an application to run in an isolated user space with all its dependencies, code and libraries. It all runs on a single host operating system and shared host OS\u0026rsquo;s kernel, making containers incredibly lightweight and fast.\nHow containers work Containers work by creating isolated environments for applications using two key technologies built into the host OS\u0026rsquo;s kernel:\nNamespaces: This feature provides isolation. Each container gets its own isolated view of resources like the network stack, process IDs, and filesystem mounts. This prevents conbtainers from seeing or interacting with each other or the host system. Control Groups(cgroups): This feature manages resource allocation. It limits and monitors how much of the host\u0026rsquo;s physical resources, such as CPU, RAM, I/O, each container can consume. This ensures no single container can monopolize the host\u0026rsquo;s resources. How container engine work Rule: A tool to create, run and manage container. It\u0026rsquo;s the translator/project manager between user and host\u0026rsquo;s OS kernel. Workflow: Creating a Dockerfile: A developer creates a text file defining all the steps required to build the application environment. Building an Image: The container engine reads the Dockerfile and packages it into a single, read-only, standardized image. This image serves as a static template for the container. Running a Container: The container engine uses this image to launch one or more container instances. During runtime, it creates an isolated namespace cgroups, while also adding a writable layer on top of the image to make the applications runnable. Docker vs Kubernetes vs Podman These are container enginers, but they serve very different purposes.\nDocker: The industry standard, it provides an all-in-one platform that includes a daemon, a client and an image registry(Docker hub). It\u0026rsquo;s easy to get started with and has a mature ecosystem Podman: A more security, daemonless alternative. Its command line is highly compatible with Docker\u0026rsquo;s, its run as a non-root user by default, and its architecture is more streamlined. Kubernetes: When applications scale up and need to run across hundreds or thousands of containers and multiple servers, Kubernetes is needed. It is a container orchestrator and servers as the brain of a container cluster. Kubernetes is not responsible for the actual running of containers. Instead, it handles higher-level management tasks such as automated deployment, elastic scaling, service discovery, load balancing and self healing. How distrobox works Distrobox is a clever tool that uses a container negine like Podman to create tightly integrated development environments. Its main purpose is to let you run any Linux distribution inside a container on your host OS but make it feel completely native.\nUse case Breaking down large, monolithic applications into smaller, independently deployable services Creating consistent and reproducible environments for building, testing and deploying software. Pros and Cons Pros\nLightweight and fast Highly portable Cons\nWeaker isolation You can only run containers that are compatible with the host OS kernel. Sandbox A sandbox is a secure, isolated environment on a computer where you can run untrusted program without risking harm to your host system.\nHow to create a sandbox Container Virtual machine Using dedicated sandbox software(like Sandboxie-Plus) Use case To be listed on the App Store or Google play store, and application must run in a sandbox. That\u0026rsquo;s why when we open a new app, the user have t approve a list of resources that enable the app to access. Education Browser plugin Pros and Cons Pros\nExtremely lightweight and fast: A sandbox applies rules to an already running process, adding minimal overhead. It\u0026rsquo;s instantaneous Targeted security: It\u0026rsquo;s perfect for its narrow purpose: running a single untrusted application and preventing it from accessing your personal files, network or hardware. Cons\nWeakest Isolation: It shares the host OS and kernel. Limited scape: It\u0026rsquo;s purely a security feature, not a deployment or development tool. ","permalink":"http://localhost:1313/posts/container_sandbox_vm/","summary":"\u003cp\u003eRecently, I tried Distrobox, a tool that enables us to run different operating systems within the host OS. I\u0026rsquo;ve noticed it\u0026rsquo;s different from a virtual machine, as it uses containerization instead. Over the past year, I\u0026rsquo;ve heard a lot about containers (like Docker), sandboxes, and virtual machines, and I\u0026rsquo;d like to understand the distinctions between them.\n\u003cimg loading=\"lazy\" src=\"/Interesting_thing/distrobox.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"virtualization\"\u003eVirtualization\u003c/h2\u003e\n\u003ch4 id=\"how-virtualization-work\"\u003eHow virtualization work\u003c/h4\u003e\n\u003cp\u003eVirtualization is the technology that allows a single physical machine, known as the \u003cstrong\u003ehost\u003c/strong\u003e, to run multiple virtual machines or \u003cstrong\u003eguests\u003c/strong\u003e.\u003c/p\u003e","title":"Virtual machine vs Container vs Sandbox"},{"content":"The Global Interpreter Lock (GIL) At its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code. It is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment. The primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\nPython\u0026rsquo;s Memory Management Python uses a system called automatic memory management. Each object in Python has a reference count, which is a number that keeps track of how many variables or other object refer to it. When you assign a variable to an object, its reference count increases by one. When a reference is removed (for instance, when a variable goes out of scope), the count decreases. Once an object\u0026rsquo;s reference count drops to zero, it means nothing is using it.\nThis is where GIL becomes important. In a multi-threaded program, multiple threads could try to increase or decrease the same object\u0026rsquo;s reference count simultaneously, which may cause memory leaks.\nPython 3.13 GIL has become a bottleneck for CPU program performance, by reconstruct python, the development team has made a groundbreaking change in Python 3.13: the ability to disable the GIL.\nWhy python slow GIL Dynamic Datatype. It\u0026rsquo;s an interpreter language Easy to read makes python very abstract Trivias Generating functions are a powerful mathematical tool that transform discrete sequences into algebraic functions, enabling us to solve complex combinatorial counting problems through function operations. GPOS vs RTOS General purpose operating systems like Windows, macOS, and Linux are designed to provide a versatile, user-friendly computing environment. They prioritize overall system efficiency, with more flexible restrictions on task response times. Real time operating systems such as VxWorks, focus on strict time limitations for individual tasks, ensuring predictable and deterministic responses. These systems are typically used in environments demanding high reliability and precise time control, featuring a smaller, more streamlined kernel that guarantees real-time performance. In python, Boolean type is essentially a subclass of the integer type, True == 1 and False == 0. This design is mainly due to historical reasons and pragmatic considerations. In early version of Python(before 2.3), there was no dedicated bool type, and people used integers 1 and 0 to represent true and false. When the bool type was introuced, in order to allow old code to continue to run seamlessly, it was the best choice to continue to design it as a subclass of int. True + 1 = 2, False * 1 = 0 Huffman coding, an algorithm to compress data losslessly. Huffman coding assigns variable-length binary codes to input symbols. More frequent symbols -\u0026gt; shorter codes Less frequent symbols -\u0026gt; longer codes Windows vs Linux Aspect Linux Windows System Usage Lightweight, minimal background processes Heavy, many preloaded services and features Bloatware No pre-installed junk, user chooses all Comes with many default apps and features Transparency Fully open, config files are plain text Many hidden processes, registry-based config Customizability Highly customizable, from kernel to GUI Limited customization without hacking Privacy \u0026amp; Security User-controlled, minimal telemetry Sends telemetry, often needs antivirus Software Support Great for dev tools, less for gaming Excellent app/game compatibility System Control Full control over system and services Some restrictions, frequent auto-updates Hardware Support Good but sometimes manual setup required Plug-and-play for most consumer hardware Resouces Why the formular of normal distribution has a pi:explain from 3b1b Talks æ¼«å£«ï¼\n","permalink":"http://localhost:1313/posts/techweekly/techweek5/","summary":"\u003ch2 id=\"the-global-interpreter-lock-gil\"\u003eThe Global Interpreter Lock (GIL)\u003c/h2\u003e\n\u003cp\u003eAt its core, the Global Interpreter Lock, or GIL, is a lock that only allows one thread to execute Python bytecode at a time within a single process. This means that even on a multi-core processor, a standard Python program with multiple threads will only utilize a single core for executing Python code.\nIt is also for compatibility with large number extension modules written in C. These C extensions may not have built-in thread safety mechanisms, so GIL provides a safety net to ensure that they are executed in a single-threaded environment.\nThe primary reason for the GIL\u0026rsquo;s existence lies in Python\u0026rsquo;s memory management.\u003c/p\u003e","title":"Some Python Notes | King Weekly"},{"content":"Bitcoin The legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\nThe Blockchainâ›“ï¸ The blockchain is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\nIt\u0026rsquo;s a Chain of Blocks: Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block. It\u0026rsquo;s Immutable: Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending. The Genesis BlockğŸ“œ The very first block, known as the Genesis Block, was mined on 2009.01.04, by Bitcoin\u0026rsquo;s mysterious creator, Satoshi Nakamoto. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\nMiningâ›ï¸ Mining is the process of creating new blocks. It\u0026rsquo;s a competitive race that serves two critical purposes:\nValidating Transactions: Miners group pending transactions into a new block. Creating New Bitcoin: The winner of the race is rewarded with new bitcoin. Hereâ€™s how a miner wins the race and proves their block is valid:\nThe Hashing Puzzle: Miners take the data in their block and use their computers to find a specific number called a nonce. When the block data and the nonce are combined and put through a cryptographic function (SHA-256), they produce a unique digital fingerprint called a hash. The \u0026ldquo;Lower Than\u0026rdquo; Rule: To win, a miner must find a hash that is lower than the current network \u0026ldquo;target\u0026rdquo;. This target is a very large number that the entire network agrees on. Finding a hash below this target is incredibly difficult and requires immense computational powerâ€”it\u0026rsquo;s like trying to win a global lottery every 10 minutes. The Reward and The Halving: The first miner to find a valid hash wins the block reward. Initially, the reward was 50 BTC. However, this reward is programmed to cut in half roughly every four years (or 210,000 blocks) in an event called the halving. As of the April 2024 halving, the reward is now 3.125 BTC. This mechanism controls the supply of new bitcoin, making the currency scarce and ensuring its total amount will never exceed 21 million coins. When a winning block is found, its hash is broadcast across the P2P network. All other participants quickly verify that the hash is valid. Once confirmed, they add the new block to their copy of the blockchain and immediately start competing to solve the next block.\nHow to Mine: There are a few ways to participate in Bitcoin mining, each with its own pros and cons.\nSolo Mining: This is you, on your own, trying to solve a block. If you succeed, you get the entire block reward (3.125 BTC + transaction fees). However, the odds of a single person solving a block today are astronomically low due to the immense competition. It\u0026rsquo;s like buying a single lottery ticket and hoping to win the grand prize. Mining Pool: This is the most common method. You join a \u0026ldquo;pool\u0026rdquo; with thousands of other miners, combining your computing power. The pool works together to find blocks much more frequently. When the pool wins, the reward is split among all participants based on how much computing power they contributed. This provides smaller, but much more consistent and predictable, payouts. Block ForksğŸ´ A fork happens when the blockchain temporarily or permanently splits into two different paths.\nAccidental Fork: Sometimes, two miners find a valid block at almost the exact same time. The network briefly splits as some nodes follow one miner and some follow the other. This is usually resolved within a few minutes when the next block is found and added to one of the chains, making it the \u0026ldquo;longest\u0026rdquo; and therefore the official one. The shorter chain is then abandoned. Hard Fork: This is an intentional split that happens when the network\u0026rsquo;s software rules are changed in a way that is not backward-compatible. All participants must upgrade to the new rules to continue. If a significant portion of the community refuses to upgrade, the split becomes permanent, resulting in the creation of a new, separate cryptocurrency (e.g., Bitcoin Cash was created from a hard fork of Bitcoin). Ethics Mining bitcoin always consume immense energy, which critics view as a wasteful environmental cost for a seemingly meaningless computation. Proponents argue this mechanism decentralized financial system that offers freedom from the control of banks and governments.\nTools Calculate computing performance: https://www.nicehash.com/profitability-calculator\nThis is the computating power of my personal game laptop(one dollar per day hhh) Resources: Youtube channels\nhttps://www.youtube.com/watch?v=5hgdekVZb3A\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=3 https://www.youtube.com/watch?v=a41DMDfJjsU\u0026amp;list=PL5TbbtexT8T0JbaWR0Zbf-aVm2onpSjHT\u0026amp;index=2 https://Gemini.google.com Trivias Sherrington, coined the word \u0026ldquo;synapse\u0026rdquo; to define the connection between two neurons Two different roads of AI: connectionism and Symbolism. Hidden layer was firstly been implemented in Boltzmann machine, although Rosenblatt had some idea about multilayer perceptrons, but he didn\u0026rsquo;t find any useful training algorithm. Restricted Boltzmann machine - each layer is only allowed to be fully connectted to the next layer, current layer nodes are not connectted to each other. The advantages of this machine is it allows to update bodes within the same layer in parallel The invention of hidden layer allows model to understand abstract features. It also becomes to one of the most significant component in deep learning. The main difference between the Hopfield Network and the Boltzmann nachine is the presence of hidden layers. Other differences include the fact that the Hopfield network is deterministic, whereas the Boltzmann machine is stochastic, and the defintions of their energy function also differ. The fovea has many photoreceptors, with a high density of cones(for colors) and nearly no rods(for dark). This structure allows us to see the world clearly. If you develop myopia, the image formed after light is reflected by the eye may not be focused directly on the fovea. The idea of CNN was mainly inspired by the HMAX model(hierarchical, pooling, convolution), and the HMAX model was proposed by Tomaso Poggio, to simulate primate visual system, specifically ventral stream. Pytorch for research area, Tensorflow for industry and JAX for high level usage. Ethereum and ether is not the same as bitcoin. Ethereum has a longer vision, and the number of ether is unlimited. Resource Documentary of AlphaGo: https://www.youtube.com/watch?v=WXuK6gekU1Y How to use hugging face: https://www.youtube.com/watch?v=3kRB2TXewus This is the most comprehensive guide for AI beginner I had ever seen: Guide link ","permalink":"http://localhost:1313/posts/techweekly/techweek4/","summary":"\u003ch2 id=\"bitcoin\"\u003eBitcoin\u003c/h2\u003e\n\u003cp\u003eThe legend of Bitcoin has shown its magic for a long time. Recently, I have started to explore this field, and this is a record of my learning.\u003c/p\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-blockchain\"\u003eThe Blockchainâ›“ï¸\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eblockchain\u003c/strong\u003e is the foundational technology of Bitcoin. Think of it as a public, digital ledger or receipt book that is shared across thousands of computers worldwide.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s a Chain of Blocks:\u003c/strong\u003e Each \u0026ldquo;block\u0026rdquo; contains a list of recent transactions. When a new block is created, it is cryptographically linked to the previous one, forming an unbroken chain leading all the way back to the very first block.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIt\u0026rsquo;s Immutable:\u003c/strong\u003e Because each block is linked to the one before it, changing a transaction in an old block would require re-doing all the work for every single block that came after it. This makes the ledger permanent and tamper-proof. This structure is what proves each coin\u0026rsquo;s history and prevents fraud like double-spending.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch4 id=\"the-genesis-block\"\u003eThe Genesis BlockğŸ“œ\u003c/h4\u003e\n\u003cp\u003eThe very first block, known as the \u003cstrong\u003eGenesis Block\u003c/strong\u003e, was mined on \u003cstrong\u003e2009.01.04\u003c/strong\u003e, by Bitcoin\u0026rsquo;s mysterious creator, \u003cstrong\u003eSatoshi Nakamoto\u003c/strong\u003e. This single block was the start of the entire Bitcoin network. Once it was created, the race to mine the second block began, and the chain has been growing continuously ever since.\u003c/p\u003e","title":"Bitcoin Basic | King Weekly"},{"content":"2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning. However, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\nRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually. And before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation. So today I will record this moment.\nHopfield Network was invented at 1982. Processor John Hopfield designed it based on ideas from statistical mechanics.\nThe graph above shows two states of a ball From the left part, we can see the energy system of the ball is at the highest, which means the ball is very unstable From the right part, we can see the ball had already fall into the bottom, which means the ball is very stable. Although it is a classical physics model, but it definitly explain the main idea of hopfield network.\nSo now we can say hopfield network is just make a system move from an unstable state to stable state.\nIf you interested how exactly hopfield network work. See the video below, it\u0026rsquo;s pretty nice.\nThis idea is also useful in today. In LLM, we can say the user\u0026rsquo;s prompt and question is the most unstable states while the answer is the stable state.\nBased on the concept of Hopfield networks, many different architectures had been invented, which makes Connectionism and Deep Learning great again.\nAfter all, I catch the reason why nobel prize gives to physics.\n","permalink":"http://localhost:1313/posts/hopfieldnetwork/","summary":"\u003cp\u003e2024 Nobel Physics prize was earned by Professor John Hopfield and Professor Geoffrey Hinton, to thanks their distribution on machine learning.\nHowever, I felt very suprised that why it gives to machine learning? Anyway, I hadn\u0026rsquo;t deeply find the answer in that time.\u003c/p\u003e\n\u003cp\u003eRecently, Professor Geoffrey Hinton gived us a short lecture about Boltzmann machine virtually.\nAnd before the lecture, I learned Hopfield Network(the predecessor of Boltzimann machine) at my accommodation.\nSo today I will record this moment.\u003c/p\u003e","title":"Hopfield Network"},{"content":"Book - Unix: A History and a Memoir Recently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world. During the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\nAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals. The system developed before Unix was called Multics. Since â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€ Unix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10. Tools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs. Unix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems. GNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source. MacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX. In the early days, operating systems were not portable. This changed with the invention of the C language and its compiler. MINIX was widely used because it was embedded in Intel chips. The working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation. Microsoft once had its 3own Unix-like system. Another completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows. You can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan. â€œEverything is a fileâ€ is one of the core principles of Unix. The KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy. The UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\nKeep it simple stupid Do one thing, and do it well Everything is a file Make each program a filter Fail loudly Modularity Prototyping early In today, many barriers had already been removed.\nAnd I realise.\nThe revolution of AI is just like the reenactment of Unix\u0026rsquo;s development.\nSo.\nKISS.\nTrivias tty - TeleTYpewriter, terminal in the old time, before lcd screen been invented. UNIX was developed on the PDP-7, a computer with no screen, no mouse and only 8KB of RAM. It weighted nearly 500kg. UNIX and UNIX-like system use abbreviated commands because typing on TTY terminals in the 1960s was slow and insufficient. Second-system effect: It believes that after completing a small, elegant, and successful system, people tend to have overly high expectations for the next projects, which may lead to the creation of a huge, feature-rich but monstrous system.The \u0026ldquo;second-system effect\u0026rdquo; can result in software project plans being overdesigned, with too many variables and excessive complexity, ultimately falling short of expectations and leading to failure. such as PL/I in Multics Fortran(formular translation): The purpose of this lagnauge is to proceed mathematics formular and float number in an efficient way, like integration, linear algebra. That\u0026rsquo;s why fortran is still popular in some supercomputer and scientific calculation. B lagnauge is designed in a bit-unit computer PCP-7, where C langauge is designed in a byte-unit computer. Therefore the main difference between B and C is B langauge doesn\u0026rsquo;t support types and C does. Development of Clang: PL/I -\u0026gt; BCPL -\u0026gt; B -\u0026gt; New B(C) If computers using the same cpu architecture, they will using the same assembly language. The grep command is used to find lines that match a specific pattern in a file while the sed command is used to insert, replace and delete text from a file. Finally, the awk command supports programming logic and is often used for advanced data processing tasks. The development of UNIX from 1970 until now. Talks \u0026ldquo;A new technological discovery is often discredited by older generations of professionals - especially those with high authority and prestige in the existing field - in order to protect their own status\u0026rdquo; ","permalink":"http://localhost:1313/posts/techweekly/techweek3/","summary":"\u003ch2 id=\"book---unix-a-history-and-a-memoir\"\u003eBook - Unix: A History and a Memoir\u003c/h2\u003e\n\u003cp\u003eRecently, I read this fantastic book. It bring me back to that 1960s - a period without modern computer and how the most clever minds in this world changed the world.\nDuring the reading, I found many answers to the \u0026ldquo;why\u0026rdquo; questions I had when I learning linux system.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAT\u0026amp;T built Bell Labs and invited some of the most brilliant people in the world to do the most advanced scientific work. There was no limit on funding and no fixed goals for individuals.\u003c/li\u003e\n\u003cli\u003eThe system developed before Unix was called Multics.\u003c/li\u003e\n\u003cli\u003eSince â€œMulticsâ€ already used â€œmulti,â€ the early name of Unix was â€œUnics.â€\u003c/li\u003e\n\u003cli\u003eUnix was first written on the PDP-7. The next version, written in C, was developed on the PDP-11. Fortunately, it wasnâ€™t written for the PDP-10.\u003c/li\u003e\n\u003cli\u003eTools like the shell, grep, regular expressions, the C language, the C compiler, yacc, lex, make, sed, awk, and troff were all invented at Bell Labs.\u003c/li\u003e\n\u003cli\u003eUnix eventually declined due to copyright issues. AT\u0026amp;T sold it as a product and made it proprietary, which gave rise to open-source Unix-like systems.\u003c/li\u003e\n\u003cli\u003eGNU is a Unix-like project that provides free and open-source alternatives. Under the GNU license, if you modify the source code of a project, the modified version must also remain open-source.\u003c/li\u003e\n\u003cli\u003eMacOS is based on BSD, which is a Unix-like system. The Linux kernel combined with GNU forms GNU/Linux. They both follow POSIX.\u003c/li\u003e\n\u003cli\u003eIn the early days, operating systems were not portable. This changed with the invention of the C language and its compiler.\u003c/li\u003e\n\u003cli\u003eMINIX was widely used because it was embedded in Intel chips.\u003c/li\u003e\n\u003cli\u003eThe working environment at Bell Labs in the 1970s are of hard problems, brilliant colleagues with shared dreams, and a unique management style that encouraged innovation.\u003c/li\u003e\n\u003cli\u003eMicrosoft once had its 3own Unix-like system.\u003c/li\u003e\n\u003cli\u003eAnother completely different path from Unix was MS-DOS, which eventually evolved into todayâ€™s Windows.\u003c/li\u003e\n\u003cli\u003eYou can also get to know the geniuses of that era, like Ken Thompson, Richard Stallman, and Brian Kernighan.\u003c/li\u003e\n\u003cli\u003eâ€œEverything is a fileâ€ is one of the core principles of Unix.\u003c/li\u003e\n\u003cli\u003eThe KISS principle (â€œKeep It Simple, Stupidâ€) is a fundamental part of Unix philosophy.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe UNIX philosophy is very similar to some programming concepts I\u0026rsquo;ve recently learned at university. That\u0026rsquo;s why, Its impact not only on system desisgn but also software deveopemnt and beyond.\u003c/p\u003e","title":"For UNIX Week | King Weekly"},{"content":"Model Context Protocol overview MCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\nWhy is MCP needed? In the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces). The current components of MCP servers include: How does MCP work? Example: Suppose you\u0026rsquo;re using an AI assistant to manage your work, and it wants to help you schedule today\u0026rsquo;s meeting:\nWithout MCP, developers would need to write separate integration code for Outlook, Google Calendar, and Apple Calendar.\nWith MCP, the AI only needs to call the MCP server, which will automatically interface with your calendar system. No matter which calendar service you use, the AI can work seamlessly.\nCore Functions of MCP: Reducing development costs (no need to develop separate integrations for each tool).\nEnhancing AI\u0026rsquo;s ability to access external data (allowing AI to easily query and manipulate external data).\nStandardizing communication (making communication between different AI applications and tools smoother).\nYou can think of MCP as the \u0026ldquo;USB interface for AI\u0026rdquo;â€”any AI device can plug into different tools without needing to individually adapt to each one!\nTrivias: Graph is a great data structure, it can be used to find the shortest path, solve a magic cube. Rely on this data structure, Human find the shortest steps to solve the worst case configured 3\\*3\\*3 magic cube in 20 steps (which also called god\u0026rsquo;s number) and 11 steps for 2\\*2\\*2 one. What about n*n*n, Here is an interesing paper about time compleixity of solving a n*n*n magic cube: Algorithms for Solving Rubikâ€™s Cubes Topological sort is an algorithm based on DFS and DAG, It\u0026rsquo;s not a traditional sorting algorithm, like comparing the size of each number and sort them, but sorting based on dependencies. Dynamic programming is just like recursion + memorization + guessing P vs NP problems: P is a problem that can be solved easily by a computer, NP is a problem that you can check for correctness very easily once solved. However, P != NP for example wec can\u0026rsquo;t engineer luck. Also, NP hard means it is at least as hard as any problem in NP, and NP-complete is lke if you can solve one NP-complete question, you can solve all NP question. Reduction is like to prove a known NP-complete question, and transfer it into a NP question X, then X is also a NP-complete question HTTP vs REST API HTTP is a protocol, and its core task is to define how to request and transfer data between your broswer and server\nIt focuses on the data exchange level, regardless of whether the data is an image, text, video, or API response. For example, you can use an HTTP request to access a webpage (HTML page) or use an HTTP request to retrieve API data (JSON data). It doesnâ€™t care about what data you\u0026rsquo;re transmitting. REST API is a design style, based on the HTTP protocol, with rules and best practices:\nIt views content on the server as \u0026ldquo;resources\u0026rdquo; and specifies how to operate on them using HTTP methods (GET, POST, PUT, DELETE). REST API design considers resource orientation: Resources (like users, articles, comments, etc.) have unique identifiers (URIs), and clients interact with these resources through HTTP requests. For example, in a REST API, you can:\nGET request: Retrieve a resource (e.g., get movie information). POST request: Create a new resource (e.g., submit a new comment). PUT request: Update a resource (e.g., modify user information). DELETE request: Delete a resource (e.g., remove an article). Resource: A tool to build a personal streaming music player: navidrome A great blog that introduce human visualization: human visualization Five basic algorithms explanation: Dynamic programming Greedy Algorithm Backtracking Branch and Bounding This week, Nvidia GTC 2025 brings a lot of new tech, PC in AI age, B300, new architecture, robots. After watched the coverage keynotes, I was just feel like our mankind is in an special stage with unpredictable pace. In science fiction films, at this point , it often leads to the arrival of an alien civilization. A dijkstra algorithm visualiser that helps me understand it: Dijkstra shortest path MCP explain: A blog that explain MCP crystal clear ","permalink":"http://localhost:1313/posts/techweekly/techweek2/","summary":"\u003ch2 id=\"model-context-protocol-overview\"\u003eModel Context Protocol overview\u003c/h2\u003e\n\u003cp\u003eMCP (Model Context Protocol) can be understood as a \u0026ldquo;universal language\u0026rdquo; for communication between AI and external tools. It\u0026rsquo;s like a translator, allowing different AI applications (such as chatbots, code assistants) and different tools (like databases, GitHub, calendars) to easily communicate without needing to develop a new interface every time.\u003c/p\u003e\n\u003ch4 id=\"why-is-mcp-needed\"\u003eWhy is MCP needed?\u003c/h4\u003e\n\u003cp\u003eIn the past, if you wanted an AI assistant to access different tools, like a calendar, email, or task manager, you would need to develop a separate interface for each tool (function calling), which resulted in a huge amount of work (N AI applications Ã— M tools = NÃ—M interfaces).\u003cbr\u003e\nMCP simplifies everything: all AI applications only need to support MCP, and all tools only need to support MCP. This way, they can communicate with each other, reducing development costs (N+M interfaces).\n\u003cimg alt=\"components mcp include\" loading=\"lazy\" src=\"/TechStuff/mcp.png\"\u003e\u003c/p\u003e","title":"Jarvis Will Coming Soon | King Weekly"},{"content":"Email sending and receiving system The main system is build based on three protocols: SMTP, POP3 and IMAP. SMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails. User1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\nPOP3 downloads emails from the email server to the email client. By default, it removes emails from the server after downloading, but some email clients allow users to keep copies on the server. IMAP keeps emails on the server and synchronizes them across multiple devices. The email client initially loads only the headers, and the full email content is fetched from the server when the user opens it.\nIf you want to customize an email domain. You need to have your own SMTP and IMAP/POP3 server and a domain, the other steps are the same as above.\nPKGBUILD in Arch Linux PKGBUILD is a bash script contain the build information required by archlinux package we use makepkg script to build the package, it will search PKGBUILD first in the current folder. Benefits:\nusing pacman to manage, user can update and uninstall easily some pkgbuild file include the commands to generate a binary file and store it in /user/bin Drawbacks:\nNot friendly to starter Although we can use yay to help us do all these stuff.\nTrivias Newton\u0026rsquo;s method is quadratic convergence when we want to calculate the root of a number Catalan number is a group of sequence that appear widely in combinatorics. e.g. ways to arrange n brackets, number of triangles in an n+2 convex polygon. The common property of these applications is that they are recursive and have a constrained structure. Toom cook: It divide a d-digit number into n parts and doing arithmatic calculations. Schonhage-strassen scheme: It multiplies two integers of length ğ‘› in O (ğ‘› logğ‘› log logğ‘›) steps on a multitape Turing machine A Naive algorithm is usually the most obvious solution when one is asked a problem. It may not be a smart algorithm but will probably get the job done The taste of red wine is determined by acidity, sweetness, alcohol content, tannins, and body. Wines are categorized into New World and Old World. New World wines (from countries like the USA, Chile, Argentina, and China) are named after the grape variety, while Old World wines (mainly from Europe) are named after their place of origin.\nRed wine is made by fermenting red grapes with their skins. White wine is made from either white grapes or red grapes without their skins. RosÃ© wine is made by soaking the grape skins briefly but fermenting without them. Sparkling wine undergoes a second fermentation to produce bubbles. Gabriel\u0026rsquo;s horn is a type of geometric figure that has infinite surface area but finite volume. Resources Code question(leetcode), system design question(crack the code interview), teamwork, communication are all important in the interview. An old guideline to learn ai: https://www.captainai.net/itcoke/ A guideline to learn CS: https://csdiy.wiki/åè®°/ Useful tips to integrate by parts, åå¯¹å¹‚æŒ‡ä¸‰, to choose u. Customize your zsh: oh my zsh Xiaomi releases a concept modular camera, it looks pretty awesome and innovative. 3b1b\u0026rsquo;s taylor series explaination:https://www.youtube.com/watch?v=3d6DsjIBzJ4 3b1b\u0026rsquo;s explaination of why we have exponential e:https://www.youtube.com/watch?v=m2MIpDrF7Es This website is all about competitive writing of source code that is as short as possible: Codewolf Explaination of greedy algorithm: greedy algorithm Explaination of dynamic programming: dynamic programming Deploy perosonal VPN tools: tailscale Abstract If no one is reading blogs anymore, why should we write them? Letâ€™s make it simple: you write a blog, but nobody cares, nobody reads it. At least, the number of readers is not as many as you thought. You put your personal ideas and thoughts into the article, carefully structuring each sentence, and choose a great imageâ€”then, no response, no likes, no shares, no activity. So, what is the meaning of writing a blog? First, there are two misconceptions about blogging. One is that if I write a good article, readers will come naturally. No, they wonâ€™t come. There are billions of blogs on the internet, like a massive hurricane, and yours is just a single leaf in the wind. Who would notice? Another misconception is that if nobody reads it, writing is a waste of time. Blogs have their own hidden value. You write blogs not for the applause of others, but for your own needs. Blogs help clear your mind. They help you organize your thoughts and sharpen your perspective. When you think better, you will achieve better results. The target audience of a blog is actually not the people on the internet, but your future self. Your article will help you see the evolution of your own thoughts. Additionally, one day in the future, someone who truly needs your article will find it. A deep, thoughtful article has a longer-lasting impact than a viral article. Writing a blog is quite like street photography. You take your camera and walk through the city. You see a sceneâ€”a moment filled with light, shadow, and humanityâ€”and then you capture it. Nobody cares about what you actually captured. But thatâ€™s not the reason you photograph; you photograph because you see something interesting. Writing a blog is the same. You write a blog because you are thinking, observing new things, and hope to store them somewhere. If someone reads it, that\u0026rsquo;s great. If not, youâ€™ve still completed your work\n","permalink":"http://localhost:1313/posts/techweekly/techweek1/","summary":"\u003ch2 id=\"email-sending-and-receiving-system\"\u003eEmail sending and receiving system\u003c/h2\u003e\n\u003cp\u003eThe main system is build based on three protocols: SMTP, POP3 and IMAP.\n\u003cimg alt=\"process of email system\" loading=\"lazy\" src=\"/emailsys.png\"\u003e\u003c/p\u003e\n\u003cp\u003eSMTP is used for sending emails to the recipientâ€™s email server, but it does not handle receiving emails.\nUser1 sends an email via an email client, and the email is first sent to User1\u0026rsquo;s email server using SMTP. Then, the email server forwards it to the recipient\u0026rsquo;s email server using SMTP as well.\u003c/p\u003e","title":"Start | King Weekly"},{"content":"In this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O O(n) notation before, but for Big theta Î¸(n) and reccurence relations T(n), I never heard them before. Today, I hope I can finally figure them out.\nWhat T(n) represents the actual running time of an algorithm\nO(n) represents the asymptotic upper bound of the running time of an algorithm\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\nHow to convert the three of them Normally we can directly transfer T(n) to O(n) or Î¸(n). Int sum = 0 for (i = 1; i \u0026lt;= n, i ++) { sum = sum + i } This is a classical example, First of all, we initialize variable sum requires one unit of running time. There are three statements inside the for loop, statement 1 i = 1 requires one unit of the running time, statement 2 i \u0026lt;= n requires n+1 units of the running time, statement 3 i ++ requires n units of the runningn time, and sum = sum + i requires 2n units of the running time, n for addition and n for assignment. Therefore T(n) = 1+1+(n+1)+n+2n = 4n + 3. In O(n), we ignore the constant and the lower-order terms, therefore the time complexity is O(n) / Î¸(n).\nWhen the algorithm is a recursion, such as karatsuba multiplication and high precision multiplication. There are two methods to convert T(n) into O(n)\nRecursion tree method According to Recursion tree method, we derive master theorem The time complexity of multiplication is equal to the time complexity of division\n","permalink":"http://localhost:1313/posts/the-difference-between-tn--on-and-%CE%B8n/","summary":"\u003cp\u003eIn this semester, I listened the course mit 6.006 in youtube channel. Duirng the course, the professor used different notations to represents time complexity of an algorithm. I learned Big O \u003ccode\u003eO(n)\u003c/code\u003e notation before, but for Big theta \u003ccode\u003eÎ¸(n)\u003c/code\u003e and reccurence relations \u003ccode\u003eT(n)\u003c/code\u003e, I never heard them before. Today, I hope I can finally figure them out.\u003c/p\u003e\n\u003ch1 id=\"what\"\u003eWhat\u003c/h1\u003e\n\u003cp\u003eT(n) represents the actual running time of an algorithm\u003cbr\u003e\nO(n) represents the asymptotic upper bound of the running time of an algorithm\u003cbr\u003e\nÎ¸(n) represents the running time when asymptotic upper bound and lower bound of an algorithm ares the same.\u003c/p\u003e","title":"Time Complexity Notations"},{"content":"This week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\nOverview The machine model is HP-Elitedesk-800-G4-SFF. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\nIf you want to learn more here is the machine datasheet:server_datasheet\nHardware Motherboard: Q370 viewer CPU: i5-8500 GPU: intel UHD 630 RAM: 16G SSD: 256G HDD: 500G * 1 The motherboard provides a high flexibility to expand more internal storage, also it has 4 PCie expansion slots which can used to expand more storage space or other components you want.\nSoftware nextcloud\nemail domain\ngitlab\nminecraft server\ndocker\njellyin\nsynthing\nproxy?router?gateway?\nvirtual machine\nI host my server with ubuntu server distro. The reason I didn\u0026rsquo;t choose proxmox is because I want to learn server step by step, proxmox is great in visualization, maybe in the future, I will try it.\nDurign the process of configuring storage, I learned LVM, which is a wonderful tool for those users that has multiple drives. User can create a storage pool called volume group. Firstly, user add their physical volumes into volumn group, and we create logical volumn based on the storage area had in volumn groups, and then we mount those LVs with the actural dirctory. It seems like Windows is not able to achieve this function. For RAID, we seperate it into 4 different categories, radi0, raid1, raid5 and raid10. This tools shows how to save files in different number of drives or in LVM.\nFor external access, I plan to use cloudflare tunnel. They provide such service, I need to buy a domain name and combine it with the cloudflare tunnel, and when I access the server, I firstly type the domain name in my browser to ask cloudflare, and they will guide me to the tunnel to my server, also in server end, I need to install cloudflared docker image as an end, then it works! It\u0026rsquo;s so convenient for those people who live in school accommodation. And it\u0026rsquo;s totally free!\nIn my network configuration. I didn\u0026rsquo;t install a router, but the best choice is to use a router for all devices in my home, and assign each of them an static IP address. To access the server, I bought a portable monitor, since the IP address of my server using DHCP, which required to check the IP address manually when the machine reboot or close. This issue will be solved easily when I have a router. Nowadays, I need to change the tunnels configuration to enable external access.\nsince I have 3 different operating systems in my three daily devices, phone for android, laptop for arachlinux and ipad for ipados, I installed nextcloud docker to try to integrate them into one ecosystem, that pretty awesome.\nUpdate Nextcloud for cloud storage Able to build game server Music and video live streaming server. Future Mining bitcoin ","permalink":"http://localhost:1313/posts/homeserver/","summary":"\u003cp\u003eThis week, I browsed many old machines at ebay to use for my first attempt at setting up a homelab. Initially, I planned to build a machine myself during the summer holiday, but in today I found a great performance and a high cost-effective machine that changed my mind.\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThe machine model is \u003ccode\u003eHP-Elitedesk-800-G4-SFF\u003c/code\u003e. Compared to its previous generation, the chipset supports 8th and 9th generation of intel core cpu, which offers a significant improvement(6c6t) compare with 6th/7th core cpu(4c4t). Furthermore it provides NVme express in this generation. With these components, I can build a highly cost-effective homelab. The total cost is Â£150.\u003cbr\u003e\n\u003cimg loading=\"lazy\" src=\"/TechStuff/serverbill.png\"\u003e\u003c/p\u003e","title":"My Homelab"},{"content":"This semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\nIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\nHow the JVM works The JVM executes er programs in several stages:\nCompilation: Java source code (.java) is compiled into bytecode (.class) by the Java Compiler (javac).\nClass Loading: The JVM loads compiled bytecode when required.\nBytecode Verification: Ensures security and correctness before execution.\nExecution: The JVM interprets or compiles bytecode using Just-In-Time (JIT) compilation.\nGarbage Collection: The JVM automatically manages memory, reclaiming unused objects.\nClass loader One of the organizational units of JVM byte code is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides class files, but it must recognize class files.\nLet\u0026rsquo;s explore this concept with an example: Imagine you\u0026rsquo;re watching a movie on a streaming service.\nLoading:\nThe service first finds and imports the movie data you want to watch. Similarly, the class loader locates the binary data for a Java class. Linking: Verification: Before you watch the movie, the service checks that the file isn\u0026rsquo;t corrupted. In Java, the class loader verifies the correctness of the class. Preparation: The service sets up the necessary space in memory to buffer the movie. Java allocates memory for variables and sets default values. Resolution: The service ensures all necessary subtitles or audio tracks are ready to play. Java resolves references to make them direct. Initialization:\nAs you start watching, the service begins playing the movie. Similarly, Java runs code to set up variables with their starting values. Class Loader Types:\nBootstrap Class Loader: Like the service\u0026rsquo;s core library of well-known movies, it loads fundamental, trusted classes. Extension Class Loader: Similar to special add-on features, it loads additional classes outside the core library. System/Application Class Loader: Like searching for new releases or user-uploaded content, it loads classes specific to the application youâ€™re using. Virtual machine architecture Cross-Platform Compatibility and Limitations The JVM abstracts away the underlying hardware and operating system specifics, allowing Java bytecode to run on any device equipped with a compatible JVM. This cross-platform capability greatly simplified software distribution and development, as developers could write code once and deploy it across various platforms without modification.\nDespite its strong cross-platform capabilities, the JVM faced significant challenges due to competitive corporate strategies, particularly the \u0026ldquo;Embrace, Extend, and Extinguish\u0026rdquo; (EEE) approach adopted by some companies like Microsoft in the late 1990s. This strategy involved embracing a technology, extending it with proprietary features, and eventually using those extensions to undermine the original technology.\nMicrosoft initially embraced Java by integrating it into their Internet Explorer browser and Windows platforms. However, they extended Java with proprietary features that were specific to Windows, creating a version of Java that was incompatible with the standard JVM specifications set by Sun Microsystems. This move fragmented the Java platform and undermined the \u0026ldquo;Write Once, Run Anywhere\u0026rdquo; philosophy.\nWith the development and rise of programming languages like Swift, Kotlin, and JavaScript, the JVM faced significant challenges in maintaining its performance edge. Swift, designed by Apple for iOS and macOS platforms, offers high performance and safety due to its compiled nature and modern language features. Kotlin, although initially running on the JVM, introduced concise syntax and advanced features that surpassed Java in many ways, leading it to become the preferred language for Android development. JavaScript\u0026rsquo;s performance greatly improved with engines like V8, and its versatility expanded through technologies like Node.js for server-side development. These languages not only matched but often exceeded the JVM\u0026rsquo;s performance and adaptability in their respective domains, leading to a shift in developer preferences and a relative decline in the JVM\u0026rsquo;s dominance.\nOverall Nowadays, Java has become to a normal programming lauguage. And the question is obvious solved. Python has its own interpreter to transfer the source code into machine code. Haskell has its own compiler, C++/C are compiled directly into machine code. However,they can\u0026rsquo;t generate a compiled file that enable to run in every operating system. If there is no EEE strategy, Linux may have a stronger effects in today\u0026rsquo;s world.\n","permalink":"http://localhost:1313/posts/jvm/","summary":"\u003cp\u003eThis semester, I learned OOP from Inf1B, which using java as the official teaching language. What fascinates me the most is why Java has the JVM. I learned Python, C++, js and Haskell before, but all of them doesn\u0026rsquo;t have a jargon for virtual machine. And then I went to wikipeidia to find out why.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn 1995, Sun Microsystems introduced Java and the JVM to the world with an ambitious dream: \u0026ldquo;Write Once, Run Anywhere.\u0026rdquo; This WORA philosophy became a reality through the JVM, enabling Java applications to run on any operating system with a compatible JVM. Before talking about the archivements and limitations, Let\u0026rsquo;s have a look about how JVM works.\u003c/p\u003e","title":"JVM's Achievements and Limitations"},{"content":" ğŸ¤— ","permalink":"http://localhost:1313/aboutme/","summary":"aboutme","title":"About me"},{"content":"Location: Auchinstarry Quarry, Scotland Date: 2025.09.12 A great first-time rock climbing experience, meet interesting people and learned some rope skills.\n","permalink":"http://localhost:1313/aboutme/climbing/","summary":"\u003ch2 id=\"location-auchinstarry-quarry-scotland\"\u003eLocation: Auchinstarry Quarry, Scotland\u003c/h2\u003e\n\u003cp\u003eDate: 2025.09.12\n\u003cimg alt=\"rock and me\" loading=\"lazy\" src=\"/Aboutme/croy_rock_and_me.jpg\"\u003e\nA great first-time rock climbing experience, meet interesting people and learned some rope skills.\u003c/p\u003e","title":"Climbing road"},{"content":" A free web application that create a two-host daily podcast of the news that matters to you. Just add your favorite RSS feeds from news sites, blogs, and social media. Our app will automatically generate conversational scripts using an LLM and then produce a customized audio episode with text-to-speech. It\u0026rsquo;s the perfect way to stay updated on your commute.\nOfficial website\nSource code\nMotivation The idea of having a J.A.R.V.I.S. from the Marvel movies, is a common dream. With today\u0026rsquo;s advancements in LLM and TTS technology, we can now offer a real-world experience that captures a part of that dream.\nFor me, I love to start my mornings by checking my favorite blogs for updates, scrolling through X for new tech news, or quickly scanning the day\u0026rsquo;s headlines. With this application, I could simply wake up, click a button to generate my podcast, and then, while having breakfast or commuting, listen to a personalized summary of everything that happened overnight. If a specific story catches my interest, I can then go back and read the full article later. I think its an efficient way to stay informed.\nTech stack Area Technology Backend FastAPI, Python, SQLAlchemy, PostgreSQL, Pydantic Frontend React, Javascript, TypeScript, Vite, Tailwind CSS AI \u0026amp; TTS Google Gemini 2.5 Pro API Deployment Self-host(personal server), Docker Compose(Backend, Frontend, background worker, PostgreSQL, Valkey) AI assistant Gemini 2.5 pro, Claude sonnet 4 Issues I met Gemini api timeout: The current Gemini API has a 60-second request timeout, which is insufficient for generating a 10-minute podcast with a single text-to-speech (TTS) request. To solve this, my initial thought was to segment the entire podcast script into smaller parts and generate each segment separately. However, a new challenge arose with the TTS service\u0026rsquo;s free plan, which limits me to just 15 requests per day. This makes the segmentation approach impractical for daily use. My temporary fix has been to adjust the script prompt to generate shorter content, but this method is unreliable and doesn\u0026rsquo;t always prevent timeouts. The long-term solution will be to upgrade to a paid TTS service with a longer API request timeout. This will allow for single, uninterrupted podcast generation, solving the issue at its root. Roadmap 2025-09: Migrate deployment from Render to my personal server âœ… - 2025.09.21 2025-10: Add more social medias RSS feeds 2025-12: Multi-format input file support Future: Use fine-tuning TTS model Improve user experience Support spotify export Summary This project takes me 2 month from learning tech stacks to final outcome. I want to say AI really helped me a lot during the building process, whereas from learning tech stacks to fix bugs. From this prject, I experience both frontend and backend project building, frontend is just like building an artwprk, it really need me to keep patient, backend is more interesting since it relative to many different fields like crud operations, building database schemas and endpoints and connection between database, background worker. I also experience the magic of containerization, its so convenient. - 2025.08.14\n","permalink":"http://localhost:1313/aboutme/projects/lets_break_the_information_gap/","summary":"aboutme","title":"Let's break the information gap"},{"content":"ğŸ“· Welcome to my photography exhibition! ","permalink":"http://localhost:1313/aboutme/gallery/","summary":"\u003ch1 id=\"heading\"\u003eğŸ“·\u003c/h1\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003ch1 id=\"welcome-to-my-photography-exhibition\"\u003eWelcome to my photography exhibition!\u003c/h1\u003e","title":"Photography exhibition by King Jin"},{"content":" Let\u0026rsquo;s break the information gap A free web application that create a two-host daily podcast of the news that matters to you. Reflex An AI-powered, English translator that generates a unique conversational story each day based on your search history. ","permalink":"http://localhost:1313/aboutme/projects/","summary":"\u003cpicture class=\"center\"\u003e\n  \u003c!-- Dark mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects_dark.png\" media=\"(prefers-color-scheme: dark)\"\u003e\n  \u003c!-- Light mode image --\u003e\n  \u003csource srcset=\"/Aboutme/projects.png\" media=\"(prefers-color-scheme: light)\"\u003e\n  \u003c!-- Fallback image --\u003e\n  \u003cimg src=\"/Aboutme/projects.png\" alt=\"Project image\"\u003e\n\u003c/picture\u003e\n\u003cbr\u003e\n\u003cbr\u003e\n\u003c!-- Project lbtig --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003eLet\u0026rsquo;s break the information gap\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/aboutme/projects/lets_break_the_information_gap\"\u003e\u003cimg src=\"/Aboutme/lbtig_preview.png\" class=\"center\"\u003e\u003c/a\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003cp class=\"center-wider\"\u003e A free web application that create a two-host daily podcast of the news that matters to you.\n\u003c/p\u003e\n\u003chr\u003e\n\u003cbr\u003e\n\u003c!-- Project reflex --\u003e\n\u003cp\u003e\u003cu\u003e\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003eReflex\u003c/a\u003e\u003c/h2\u003e\u003c/u\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"/aboutme/projects/reflex\"\u003e\u003cimg src=\"/Aboutme/reflex_preview.png\" class=\"center\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cbr\u003e\n\u003cp class=\"center-wider\"\u003e An AI-powered, English translator that generates a unique conversational story each day based on your search history.\n\u003c/p\u003e","title":"Project by King Jin"},{"content":" An AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\ncode available in github Motivation I dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\nTech stack Frontend: HTML, Tailwind CSS, Electron, Javascript\nRoadmap Future: support multi-language data record system\nSummary This project takes me one week to build. To be honest, Electron is convenient for cross platform, but also it increase the size of the software.\n","permalink":"http://localhost:1313/aboutme/projects/reflex/","summary":"\u003cblockquote\u003e\n\u003cp\u003eAn AI-powered English learning application built on Electron. The app\u0026rsquo;s core function is a dictionary, which records each word you searched. The search history is then used to generate a unique review story, allowing you to practice new vocabulary in conversational contexts rather than just through translation.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Kingjinsight/Reflex\"\u003ecode available in github\u003c/a\u003e\n\u003cimg alt=\"reflex_dashboard\" loading=\"lazy\" src=\"/Aboutme/reflex_dashboard.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI dislike the traditional method of using a dictionary to search a word, gettings its translation and storing it in a new word notebook. This relies on rote memorization, but using a word is the most efficient way to remember it. Therefore, a fter a day\u0026rsquo;s record, a user can generate a short conversational story with an LLM to help recall the meaning of each new word, which should make a stronger impression.\u003c/p\u003e","title":"Reflex"}]